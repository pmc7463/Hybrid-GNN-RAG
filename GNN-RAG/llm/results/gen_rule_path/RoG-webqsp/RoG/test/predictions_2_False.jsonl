{"id": "WebQTest-0", "question": "what does jamaican people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does jamaican people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.03621038421988487, -0.08329375833272934], "norm_scores": [0.511768639087677, 0.4882313311100006]}}
{"id": "WebQTest-1", "question": "what did james k polk do before he was president", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["people.person.profession", "base.onephylogeny.type_of_thing.things_of_this_type"]], "ground_paths": [["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["people.person.nationality", "government.government_office_or_title.jurisdiction"], ["people.person.profession", "base.onephylogeny.type_of_thing.things_of_this_type"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did james k polk do before he was president [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> people.person.profession <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>"], "scores": [-0.042701300233602524, -0.05536096543073654], "norm_scores": [0.5031648874282837, 0.4968351423740387]}}
{"id": "WebQTest-3", "question": "who plays ken barlow in coronation street", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays ken barlow in coronation street [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.05246363952755928, -0.075360007584095], "norm_scores": [0.5057238340377808, 0.49427613615989685]}}
{"id": "WebQTest-6", "question": "where is jamarcus russell from", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is jamarcus russell from [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.0355239175260067, -0.08568187057971954], "norm_scores": [0.512536883354187, 0.4874631464481354]}}
{"id": "WebQTest-7", "question": "where was george washington carver from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was george washington carver from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.02237563580274582, -0.17211313545703888], "norm_scores": [0.5373645424842834, 0.4626353979110718]}}
{"id": "WebQTest-8", "question": "what else did ben franklin invent", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"]], "ground_paths": [["law.inventor.inventions"], ["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what else did ben franklin invent [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>"], "scores": [-0.030575381591916084, -0.12431080639362335], "norm_scores": [0.5234166979789734, 0.4765833020210266]}}
{"id": "WebQTest-9", "question": "who was richard nixon married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["people.person.spouse_s", "people.person.spouse_s"], ["people.person.children", "people.person.parents"], ["people.person.profession", "common.topic.notable_types"], ["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_ownership.owner"], ["people.person.nationality", "people.person.nationality"], ["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_owner.pets_owned"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_ownership.owner"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was richard nixon married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.053938042372465134, -0.08261197805404663], "norm_scores": [0.5071679949760437, 0.4928320050239563]}}
{"id": "WebQTest-12", "question": "who is governor of ohio 2011", "prediction": [["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["base.locations.states_and_provences.country", "people.person.nationality"], ["government.political_district.representatives", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is governor of ohio 2011 [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.035219475626945496, -0.04525924474000931], "norm_scores": [0.5025098919868469, 0.4974900484085083]}}
{"id": "WebQTest-13", "question": "who was vice president after kennedy died", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vice president after kennedy died [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>"], "scores": [-0.041219454258680344, -0.049298856407403946], "norm_scores": [0.5020198225975037, 0.49798014760017395]}}
{"id": "WebQTest-14", "question": "where is the fukushima daiichi nuclear plant located", "prediction": [["location.location.containedby"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the fukushima daiichi nuclear plant located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.05254063755273819, -0.09356322884559631], "norm_scores": [0.5102542042732239, 0.4897457957267761]}}
{"id": "WebQTest-16", "question": "what countries are part of the uk", "prediction": [["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_area_type"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are part of the uk [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>"], "scores": [-0.06372551620006561, -0.06688480824232101], "norm_scores": [0.5007898211479187, 0.4992101490497589]}}
{"id": "WebQTest-19", "question": "what is my timezone in louisiana", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is my timezone in louisiana [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>"], "scores": [-6.266545824473724e-05, -0.5184895992279053], "norm_scores": [0.6267798542976379, 0.37322014570236206]}}
{"id": "WebQTest-20", "question": "what did st augustine do", "prediction": [["people.person.profession"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did st augustine do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.01027179416269064, -0.19140580296516418], "norm_scores": [0.5451601147651672, 0.45483988523483276]}}
{"id": "WebQTest-21", "question": "who is keyshia cole dad", "prediction": [["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is keyshia cole dad [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.036402054131031036, -0.16010257601737976], "norm_scores": [0.5308857560157776, 0.4691142439842224]}}
{"id": "WebQTest-22", "question": "what kind government does egypt have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind government does egypt have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.019037645310163498, -0.0923164114356041], "norm_scores": [0.5183115005493164, 0.4816884994506836]}}
{"id": "WebQTest-23", "question": "what town was martin luther king assassinated in", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.place_of_death", "base.biblioness.bibs_location.city"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what town was martin luther king assassinated in [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> base.biblioness.bibs_location.city </PATH>"], "scores": [-0.006112908478826284, -0.2799040377140045], "norm_scores": [0.5680233836174011, 0.4319766163825989]}}
{"id": "WebQTest-24", "question": "where did edgar allan poe died", "prediction": [["people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did edgar allan poe died [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.000755832705181092, -0.28658440709114075], "norm_scores": [0.570974588394165, 0.42902541160583496]}}
{"id": "WebQTest-26", "question": "what to do today in atlanta with kids", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do today in atlanta with kids [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.0538807176053524, -0.055156633257865906], "norm_scores": [0.5003189444541931, 0.4996809959411621]}}
{"id": "WebQTest-28", "question": "what electorate does anna bligh represent", "prediction": [["people.person.nationality", "base.biblioness.bibs_location.country"], ["government.politician.government_positions_held", "government.government_position_held.district_represented"]], "ground_paths": [["government.politician.government_positions_held", "government.political_district.representatives"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what electorate does anna bligh represent [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.district_represented </PATH>"], "scores": [-0.041764091700315475, -0.04183897376060486], "norm_scores": [0.5000187158584595, 0.4999813139438629]}}
{"id": "WebQTest-31", "question": "where did andy murray started playing tennis", "prediction": [["people.person.nationality", "location.location.containedby"], ["sports.sports_facility.teams"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did andy murray started playing tennis [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_facility.teams </PATH>"], "scores": [-0.07778599858283997, -0.13838233053684235], "norm_scores": [0.5151444673538208, 0.4848555624485016]}}
{"id": "WebQTest-32", "question": "what is the australian dollar called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the australian dollar called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.03920337185263634, -0.13145999610424042], "norm_scores": [0.5230478048324585, 0.4769521653652191]}}
{"id": "WebQTest-33", "question": "what timezone is sweden", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is sweden [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>"], "scores": [-0.0026616009417921305, -0.18826667964458466], "norm_scores": [0.5462685227394104, 0.4537314772605896]}}
{"id": "WebQTest-34", "question": "who did cam newton sign with", "prediction": [["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did cam newton sign with [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.football_player.rushing <SEP> american_football.player_rushing_statistics.team </PATH>"], "scores": [-0.05760182812809944, -0.060585785657167435], "norm_scores": [0.5007460117340088, 0.4992540180683136]}}
{"id": "WebQTest-35", "question": "what county is frederick md in", "prediction": [["location.us_county.hud_county_place"], ["location.us_county.county_seat"]], "ground_paths": [["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is frederick md in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.us_county.county_seat </PATH>"], "scores": [-0.06720505654811859, -0.08030997961759567], "norm_scores": [0.5032761693000793, 0.49672380089759827]}}
{"id": "WebQTest-36", "question": "what highschool did harper lee go to", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what highschool did harper lee go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.03297493979334831, -0.061070624738931656], "norm_scores": [0.5070234537124634, 0.49297651648521423]}}
{"id": "WebQTest-37", "question": "what timezone is utah in", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is utah in [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>"], "scores": [-6.206868420122191e-05, -0.44768866896629333], "norm_scores": [0.6100747585296631, 0.3899252414703369]}}
{"id": "WebQTest-38", "question": "who did george w. bush run against for the second term", "prediction": [["time.event.locations", "people.person.nationality"], ["government.election.campaigns", "government.election_campaign.candidate"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did george w. bush run against for the second term [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations <SEP> people.person.nationality </PATH>", "<PATH> government.election.campaigns <SEP> government.election_campaign.candidate </PATH>"], "scores": [-0.048509590327739716, -0.06350074708461761], "norm_scores": [0.5037477016448975, 0.49625226855278015]}}
{"id": "WebQTest-39", "question": "who is niall ferguson 's wife", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "ground_paths": [["people.person.spouse_s", "people.person.spouse_s"], ["people.person.profession", "people.person.profession"], ["people.person.profession", "common.topic.notable_types"], ["freebase.valuenotation.has_value", "freebase.valuenotation.is_reviewed"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is niall ferguson 's wife [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>"], "scores": [-0.06981779634952545, -0.07283566147089005], "norm_scores": [0.5007544755935669, 0.4992455542087555]}}
{"id": "WebQTest-41", "question": "where is the galapagos islands located on a world map", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the galapagos islands located on a world map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0013055220479145646, -0.6012616753578186], "norm_scores": [0.6456462740898132, 0.3543536961078644]}}
{"id": "WebQTest-42", "question": "what are the songs that justin bieber wrote", "prediction": [["music.composer.compositions"], ["music.recording.artist"]], "ground_paths": [["music.recording.artist"], ["music.composer.compositions"], ["music.album.artist"], ["music.lyricist.lyrics_written"], ["music.recording.featured_artists"], ["music.artist.album"], ["music.composition.lyricist"], ["music.composition.composer"], ["music.featured_artist.recordings"], ["music.featured_artist.albums"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the songs that justin bieber wrote [/INST]", "raw_output": {"paths": ["<PATH> music.composer.compositions </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.10128364711999893, -0.11784444004297256], "norm_scores": [0.5041400790214539, 0.49585989117622375]}}
{"id": "WebQTest-43", "question": "what did george clemenceau do", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did george clemenceau do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.020421570166945457, -0.23442651331424713], "norm_scores": [0.5532979369163513, 0.4467020034790039]}}
{"id": "WebQTest-44", "question": "what is the state flower of arizona", "prediction": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"]], "ground_paths": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the state flower of arizona [/INST]", "raw_output": {"paths": ["<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>"], "scores": [-0.026895886287093163, -0.03278713300824165], "norm_scores": [0.5014728307723999, 0.4985271990299225]}}
{"id": "WebQTest-45", "question": "where was rihanna born and raised", "prediction": [["location.location.people_born_here"], ["music.artist.origin"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was rihanna born and raised [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.03283098340034485, -0.15106754004955292], "norm_scores": [0.5295247435569763, 0.4704752266407013]}}
{"id": "WebQTest-46", "question": "what year did president william henry harrison take office", "prediction": [["government.election_campaign.candidate", "government.election_campaign.election"], ["government.election_campaign.candidate", "time.event.instance_of_recurring_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did president william henry harrison take office [/INST]", "raw_output": {"paths": ["<PATH> government.election_campaign.candidate <SEP> government.election_campaign.election </PATH>", "<PATH> government.election_campaign.candidate <SEP> time.event.instance_of_recurring_event </PATH>"], "scores": [-0.04982931911945343, -0.0907997339963913], "norm_scores": [0.5102412104606628, 0.48975884914398193]}}
{"id": "WebQTest-47", "question": "where george lopez was born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where george lopez was born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0337374247610569, -0.09057489037513733], "norm_scores": [0.5142055153846741, 0.48579445481300354]}}
{"id": "WebQTest-48", "question": "where are samsung based", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are samsung based [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>"], "scores": [-0.025121530517935753, -0.08298692852258682], "norm_scores": [0.5144622921943665, 0.48553767800331116]}}
{"id": "WebQTest-49", "question": "what did the islamic people believe in", "prediction": [["religion.belief.belief_of"], ["religion.religion.beliefs"]], "ground_paths": [["religion.religion.beliefs"], ["religion.belief.belief_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did the islamic people believe in [/INST]", "raw_output": {"paths": ["<PATH> religion.belief.belief_of </PATH>", "<PATH> religion.religion.beliefs </PATH>"], "scores": [-0.03413044288754463, -0.071196548640728], "norm_scores": [0.5092654824256897, 0.4907345473766327]}}
{"id": "WebQTest-51", "question": "who will play mr gray in the film", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who will play mr gray in the film [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.06385720521211624, -0.06534314900636673], "norm_scores": [0.500371515750885, 0.49962854385375977]}}
{"id": "WebQTest-52", "question": "what did george orwell died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did george orwell died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.005368396639823914, -0.17323414981365204], "norm_scores": [0.5418681502342224, 0.4581318199634552]}}
{"id": "WebQTest-54", "question": "what country did adolf hitler control", "prediction": [["base.culturalevent.event.entity_involved", "time.event.locations"], ["base.culturalevent.event.entity_involved", "location.location.events"]], "ground_paths": [["people.deceased_person.place_of_death", "location.country.capital"], ["organization.organization_founder.organizations_founded", "organization.organization.place_founded"], ["military.military_command.military_commander", "military.military_combatant.military_commanders"], ["organization.organization.founders", "organization.organization.place_founded"], ["government.political_appointer.appointees", "government.governmental_jurisdiction.governing_officials"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["military.military_commander.military_commands", "military.military_command.military_combatant"], ["base.crime.crime.victim_s", "time.event.locations"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["government.government_position_held.office_holder", "government.governmental_jurisdiction.governing_officials"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did adolf hitler control [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>"], "scores": [-0.07754654437303543, -0.08503182232379913], "norm_scores": [0.5018713474273682, 0.4981286823749542]}}
{"id": "WebQTest-55", "question": "who is michael buble", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is michael buble [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0017460273811593652, -0.3873370885848999], "norm_scores": [0.5952209234237671, 0.4047791063785553]}}
{"id": "WebQTest-56", "question": "what county is kansas city kansas", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"]], "ground_paths": [["location.hud_county_place.county"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is kansas city kansas [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.06805408746004105, -0.11703356355428696], "norm_scores": [0.5122424364089966, 0.4877575635910034]}}
{"id": "WebQTest-58", "question": "who did jackie robinson first play for", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did jackie robinson first play for [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>"], "scores": [-0.03952879458665848, -0.04612014815211296], "norm_scores": [0.5016478300094604, 0.49835214018821716]}}
{"id": "WebQTest-59", "question": "when did annie open", "prediction": [["theater.theater_character.plays_appears_in"], ["film.film.directed_by"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did annie open [/INST]", "raw_output": {"paths": ["<PATH> theater.theater_character.plays_appears_in </PATH>", "<PATH> film.film.directed_by </PATH>"], "scores": [-0.10097549110651016, -0.19933933019638062], "norm_scores": [0.5245711207389832, 0.47542884945869446]}}
{"id": "WebQTest-60", "question": "where did eleanor roosevelt die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did eleanor roosevelt die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>"], "scores": [-0.005382809787988663, -0.2530056834220886], "norm_scores": [0.5615913271903992, 0.43840867280960083]}}
{"id": "WebQTest-61", "question": "what are the religions practiced in indonesia", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "ground_paths": [["book.book_subject.works", "book.written_work.subjects"], ["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the religions practiced in indonesia [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>"], "scores": [-0.021471532061696053, -0.045842770487070084], "norm_scores": [0.5060925483703613, 0.49390748143196106]}}
{"id": "WebQTest-62", "question": "what was jesse james killed with", "prediction": [["people.deceased_person.cause_of_death"], ["people.deceased_person.place_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was jesse james killed with [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>"], "scores": [-0.004948365036398172, -0.2089911252260208], "norm_scores": [0.5508344173431396, 0.4491655230522156]}}
{"id": "WebQTest-63", "question": "who was vp for lincoln", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"]], "ground_paths": [["government.us_vice_president.to_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vp for lincoln [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>"], "scores": [-0.027695341035723686, -0.061065059155225754], "norm_scores": [0.5083416700363159, 0.49165835976600647]}}
{"id": "WebQTest-64", "question": "what other books did charles dickens write", "prediction": [["book.written_work.author"], ["fictional_universe.fictional_character_creator.fictional_characters_created"]], "ground_paths": [["fictional_universe.fictional_character_creator.fictional_characters_created"], ["book.written_work.author"], ["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other books did charles dickens write [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>"], "scores": [-0.031140586361289024, -0.07658229023218155], "norm_scores": [0.5113584995269775, 0.48864153027534485]}}
{"id": "WebQTest-65", "question": "who was the first dictator of the soviet union", "prediction": [["government.government_position_held.basic_title", "government.politician.government_positions_held"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the first dictator of the soviet union [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.04398320987820625, -0.073528952896595], "norm_scores": [0.5073859095573425, 0.49261409044265747]}}
{"id": "WebQTest-66", "question": "what is the currency of puerto rico called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency of puerto rico called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.0031655444763600826, -0.29424136877059937], "norm_scores": [0.5722594857215881, 0.42774051427841187]}}
{"id": "WebQTest-67", "question": "what kind of cancer did carl wilson have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of cancer did carl wilson have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.03871644660830498, -0.054603029042482376], "norm_scores": [0.5039715766906738, 0.49602845311164856]}}
{"id": "WebQTest-68", "question": "who plays the voice of kitt in knight rider", "prediction": [["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "ground_paths": [["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays the voice of kitt in knight rider [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>"], "scores": [-0.04697222635149956, -0.0652296245098114], "norm_scores": [0.5045642256736755, 0.49543577432632446]}}
{"id": "WebQTest-69", "question": "what county is brentwood tennessee in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"]], "ground_paths": [["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is brentwood tennessee in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.056208156049251556, -0.1234530434012413], "norm_scores": [0.5168049335479736, 0.48319509625434875]}}
{"id": "WebQTest-71", "question": "who did michael j fox marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["people.person.spouse_s", "people.marriage.spouse"], ["award.award_winner.awards_won", "award.award_honor.award_winner"], ["common.image.appears_in_topic_gallery", "common.topic.image"], ["people.person.children", "people.person.parents"], ["people.person.profession", "people.person.profession"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.parents"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did michael j fox marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.06133723631501198, -0.06735210865736008], "norm_scores": [0.5015037059783936, 0.49849629402160645]}}
{"id": "WebQTest-72", "question": "what battles did stonewall jackson fight in", "prediction": [["base.culturalevent.event.entity_involved"], ["military.military_person.participated_in_conflicts"]], "ground_paths": [["military.military_person.participated_in_conflicts"], ["people.person.nationality", "book.written_work.subjects"], ["common.topic.image"], ["base.culturalevent.event.entity_involved"], ["military.military_commander.military_commands", "military.military_conflict.commanders"], ["military.military_conflict.military_personnel_involved"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what battles did stonewall jackson fight in [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>"], "scores": [-0.04029622673988342, -0.08867918699979782], "norm_scores": [0.5120933651924133, 0.4879066050052643]}}
{"id": "WebQTest-73", "question": "what language do the maasai tribe speak", "prediction": [["people.ethnicity.languages_spoken"], ["people.ethnicity.languages_spoken", "language.human_language.region", "language.human_language.region"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do the maasai tribe speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> language.human_language.region <SEP> language.human_language.region </PATH>"], "scores": [-0.002650488866493106, -0.22420437633991241], "norm_scores": [0.5551629662513733, 0.4448370039463043]}}
{"id": "WebQTest-74", "question": "who did benjamin franklin get married to", "prediction": [["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.place_of_interment.interred_here", "people.place_of_interment.interred_here"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did benjamin franklin get married to [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.1113906130194664, -0.12716084718704224], "norm_scores": [0.5039424896240234, 0.49605751037597656]}}
{"id": "WebQTest-75", "question": "what disease did patrick swayze died from", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"]], "ground_paths": [["medicine.disease.notable_people_with_this_condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what disease did patrick swayze died from [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.01588841713964939, -0.09496790170669556], "norm_scores": [0.5197595357894897, 0.48024043440818787]}}
{"id": "WebQTest-76", "question": "what works of art did leonardo da vinci produce", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_art_form.artists"]], "ground_paths": [["visual_art.visual_artist.artworks"], ["visual_art.artwork.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what works of art did leonardo da vinci produce [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>"], "scores": [-0.0033142792526632547, -0.3879477381706238], "norm_scores": [0.5949901342391968, 0.4050098657608032]}}
{"id": "WebQTest-77", "question": "what capital of austria", "prediction": [["location.country.capital"], ["location.location.containedby"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what capital of austria [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.036497216671705246, -0.19049854576587677], "norm_scores": [0.5384244322776794, 0.46157556772232056]}}
{"id": "WebQTest-78", "question": "what type of cancer did eva peron have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"]], "ground_paths": [["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of cancer did eva peron have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.024997901171445847, -0.09406887739896774], "norm_scores": [0.5172609090805054, 0.482739120721817]}}
{"id": "WebQTest-79", "question": "what country did buddha come from", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did buddha come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.03514712676405907, -0.1414448767900467], "norm_scores": [0.5265494585037231, 0.47345057129859924]}}
{"id": "WebQTest-80", "question": "what county is greeley colorado in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"]], "ground_paths": [["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is greeley colorado in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0688244104385376, -0.1409073770046234], "norm_scores": [0.5180129408836365, 0.4819870591163635]}}
{"id": "WebQTest-82", "question": "what did peter tchaikovsky do", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did peter tchaikovsky do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.005168939009308815, -0.34982606768608093], "norm_scores": [0.5853213667869568, 0.4146786332130432]}}
{"id": "WebQTest-83", "question": "which countries speak german officially", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries speak german officially [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.07351595908403397, -0.08346839249134064], "norm_scores": [0.5024880766868591, 0.49751192331314087]}}
{"id": "WebQTest-84", "question": "what type of music did john lennon sing", "prediction": [["music.artist.genre"], ["music.genre.artists"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did john lennon sing [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>"], "scores": [-0.006590524222701788, -0.30277055501937866], "norm_scores": [0.5735084414482117, 0.42649155855178833]}}
{"id": "WebQTest-85", "question": "who are the colorado representatives", "prediction": [["government.political_district.representatives", "government.politician.government_positions_held"], ["book.written_work.subjects", "people.person.nationality"]], "ground_paths": [["government.election.district", "government.election.winner"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the colorado representatives [/INST]", "raw_output": {"paths": ["<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> book.written_work.subjects <SEP> people.person.nationality </PATH>"], "scores": [-0.06368502974510193, -0.07893246412277222], "norm_scores": [0.503811776638031, 0.4961881935596466]}}
{"id": "WebQTest-86", "question": "which country does greenland belong to", "prediction": [["location.location.containedby"], ["location.administrative_division.country"]], "ground_paths": [["location.country.administrative_divisions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country does greenland belong to [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-0.08935268223285675, -0.14013025164604187], "norm_scores": [0.5126916766166687, 0.4873083531856537]}}
{"id": "WebQTest-87", "question": "what the zip code for seattle washington", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the zip code for seattle washington [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0007467021350748837, -0.48644766211509705], "norm_scores": [0.619093120098114, 0.3809068202972412]}}
{"id": "WebQTest-88", "question": "who is willow smith mom name", "prediction": [["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is willow smith mom name [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.04442129284143448, -0.1417820155620575], "norm_scores": [0.5243209600448608, 0.47567906975746155]}}
{"id": "WebQTest-89", "question": "what do you call the chinese writing system", "prediction": [["language.human_language.writing_system"], ["language.language_writing_system.writing_systems"]], "ground_paths": [["language.human_language.writing_system"], ["language.language_writing_system.languages"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do you call the chinese writing system [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.writing_system </PATH>", "<PATH> language.language_writing_system.writing_systems </PATH>"], "scores": [-0.036858025938272476, -0.17161710560321808], "norm_scores": [0.5336388945579529, 0.4663611352443695]}}
{"id": "WebQTest-90", "question": "who played on the jeffersons", "prediction": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"]], "ground_paths": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["award.award_winning_work.awards_won", "award.award_honor.award_winner"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["award.award_honor.honored_for", "award.award_winner.awards_won"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["award.award_honor.honored_for", "award.award_honor.award_winner"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["tv.tv_program.regular_cast", "tv.tv_actor.starring_roles"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played on the jeffersons [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.0505376011133194, -0.053592972457408905], "norm_scores": [0.5007638335227966, 0.49923616647720337]}}
{"id": "WebQTest-91", "question": "what is the name of the san francisco newspaper", "prediction": [["book.newspaper.circulation_areas"], ["periodicals.newspaper_circulation_area.newspapers"]], "ground_paths": [["periodicals.newspaper_circulation_area.newspapers"], ["book.newspaper.circulation_areas"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the san francisco newspaper [/INST]", "raw_output": {"paths": ["<PATH> book.newspaper.circulation_areas </PATH>", "<PATH> periodicals.newspaper_circulation_area.newspapers </PATH>"], "scores": [-0.012410253286361694, -0.09777297079563141], "norm_scores": [0.5213277339935303, 0.4786722660064697]}}
{"id": "WebQTest-93", "question": "what continent does armenia belong to", "prediction": [["base.locations.countries.continent"], ["location.location.containedby"]], "ground_paths": [["base.locations.countries.continent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what continent does armenia belong to [/INST]", "raw_output": {"paths": ["<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.06342213600873947, -0.12687866389751434], "norm_scores": [0.5158588290214539, 0.48414120078086853]}}
{"id": "WebQTest-94", "question": "what did randy savage died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did randy savage died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.002946221036836505, -0.21812604367733002], "norm_scores": [0.553588330745697, 0.4464116394519806]}}
{"id": "WebQTest-95", "question": "what was the title of the book charles darwin wrote", "prediction": [["book.written_work.author"], ["book.book_edition.author_editor"]], "ground_paths": [["exhibitions.exhibition_subject.exhibitions_created_about_this_subject"], [], ["book.written_work.author"], ["base.argumentmaps.innovator.original_ideas"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the title of the book charles darwin wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.008209964260458946, -0.3415418267250061], "norm_scores": [0.5825698375701904, 0.41743016242980957]}}
{"id": "WebQTest-96", "question": "where did richard nixon die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people", "people.deceased_person.place_of_death"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did richard nixon die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.0018843712750822306, -0.19185777008533478], "norm_scores": [0.547351062297821, 0.45264899730682373]}}
{"id": "WebQTest-97", "question": "what songs does smokey robinson sing", "prediction": [["music.recording.artist"], ["music.artist.track"]], "ground_paths": [["music.recording.artist"], ["music.album.artist"], ["music.lyricist.lyrics_written"], ["music.recording.featured_artists"], ["music.artist.album"], ["music.composition.lyricist"], ["music.featured_artist.recordings"], ["music.producer.releases_produced"], ["music.recording.producer"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what songs does smokey robinson sing [/INST]", "raw_output": {"paths": ["<PATH> music.recording.artist </PATH>", "<PATH> music.artist.track </PATH>"], "scores": [-0.07758446037769318, -0.21911776065826416], "norm_scores": [0.5353243947029114, 0.4646756052970886]}}
{"id": "WebQTest-98", "question": "who was vice president when jfk was president", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vice president when jfk was president [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>"], "scores": [-0.035548679530620575, -0.05462874844670296], "norm_scores": [0.5047698616981506, 0.49523013830184937]}}
{"id": "WebQTest-99", "question": "what is serbian language called", "prediction": [["language.human_language.countries_spoken_in"], ["language.human_language.language_family"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is serbian language called [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.language_family </PATH>"], "scores": [-0.06242981180548668, -0.11188375949859619], "norm_scores": [0.5123609900474548, 0.48763903975486755]}}
{"id": "WebQTest-100", "question": "what language is spoken in haiti today", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is spoken in haiti today [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.04272085055708885, -0.07579075545072556], "norm_scores": [0.5082667469978333, 0.49173328280448914]}}
{"id": "WebQTest-102", "question": "who played barbara gordon batgirl", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played barbara gordon batgirl [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.03292945399880409, -0.09282403439283371], "norm_scores": [0.5149691700935364, 0.485030859708786]}}
{"id": "WebQTest-103", "question": "what basketball teams has shaq played for", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"]], "ground_paths": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["sports.sports_award.award_winner", "sports.sports_award.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what basketball teams has shaq played for [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.056625232100486755, -0.06926091015338898], "norm_scores": [0.5031588673591614, 0.4968411326408386]}}
{"id": "WebQTest-104", "question": "where is jay leno from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is jay leno from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.018948230892419815, -0.15108375251293182], "norm_scores": [0.5329859256744385, 0.4670141041278839]}}
{"id": "WebQTest-105", "question": "what language do people from thailand speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people from thailand speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.05700232833623886, -0.08678465336561203], "norm_scores": [0.5074450373649597, 0.4925549626350403]}}
{"id": "WebQTest-106", "question": "who did armie hammer play in the social network", "prediction": [["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.performance.character"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did armie hammer play in the social network [/INST]", "raw_output": {"paths": ["<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>"], "scores": [-0.07957001030445099, -0.11116233468055725], "norm_scores": [0.5078974366188049, 0.49210259318351746]}}
{"id": "WebQTest-107", "question": "what was thomas jefferson role in the declaration of independence", "prediction": [["book.written_work.subjects", "people.person.nationality"], ["book.author.book_editions_published"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was thomas jefferson role in the declaration of independence [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> people.person.nationality </PATH>", "<PATH> book.author.book_editions_published </PATH>"], "scores": [-0.10340231657028198, -0.1426752507686615], "norm_scores": [0.5098169445991516, 0.4901830554008484]}}
{"id": "WebQTest-108", "question": "what was the book written by charles darwin", "prediction": [["book.written_work.subjects"], ["book.written_work.author"]], "ground_paths": [["exhibitions.exhibition_subject.exhibitions_created_about_this_subject"], [], ["book.written_work.author"], ["base.argumentmaps.innovator.original_ideas"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the book written by charles darwin [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.06758089363574982, -0.09620501101016998], "norm_scores": [0.5071555376052856, 0.49284443259239197]}}
{"id": "WebQTest-109", "question": "who did tim tebow play college football for", "prediction": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did tim tebow play college football for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.036254264414310455, -0.05698297172784805], "norm_scores": [0.5051819682121277, 0.4948180019855499]}}
{"id": "WebQTest-110", "question": "who does peyton manning play football for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does peyton manning play football for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.024839092046022415, -0.05695115029811859], "norm_scores": [0.5080273151397705, 0.4919726848602295]}}
{"id": "WebQTest-111", "question": "where is the carpathian mountain range located", "prediction": [["location.location.partially_containedby"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the carpathian mountain range located [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0724923387169838, -0.08567787706851959], "norm_scores": [0.5032963156700134, 0.4967036843299866]}}
{"id": "WebQTest-112", "question": "what was robert burns", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was robert burns [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.020615743473172188, -0.20278581976890564], "norm_scores": [0.5454169511795044, 0.45458298921585083]}}
{"id": "WebQTest-114", "question": "who did the voice of darth vader in episode 3", "prediction": [["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did the voice of darth vader in episode 3 [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.08066537231206894, -0.08973652124404907], "norm_scores": [0.5022677779197693, 0.4977322220802307]}}
{"id": "WebQTest-115", "question": "what super bowl did peyton manning win", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "ground_paths": [["tv.tv_actor.guest_roles", "tv.tv_guest_role.episodes_appeared_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what super bowl did peyton manning win [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>"], "scores": [-0.02035539783537388, -0.15924309194087982], "norm_scores": [0.5346661806106567, 0.4653337895870209]}}
{"id": "WebQTest-116", "question": "which country was justin bieber born in", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country was justin bieber born in [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.031360652297735214, -0.12503637373447418], "norm_scores": [0.5234017968177795, 0.47659817337989807]}}
{"id": "WebQTest-118", "question": "who fought in the gulf war 1991", "prediction": [["military.military_combatant_group.conflict", "military.military_combatant.military_conflicts"], ["military.military_combatant_group.conflict", "military.military_combatant_group.combatants"]], "ground_paths": [["military.military_person.participated_in_conflicts", "people.person.nationality"], ["film.film.subjects", "film.film.country"], ["film.film_subject.films", "film.film.country"], ["book.book_subject.works", "media_common.netflix_title.netflix_genres"], ["military.casualties.military_conflict", "military.military_combatant.casualties"], ["time.event.includes_event", "base.culturalevent.event.entity_involved"], ["time.event.included_in_event", "base.culturalevent.event.entity_involved"], ["film.film.genre", "film.film.country"], ["film.film_genre.films_in_this_genre", "film.film.country"], ["military.military_conflict.casualties", "military.military_combatant.casualties"], ["time.event.locations"], ["base.culturalevent.event.entity_involved", "people.person.place_of_birth"], ["military.military_conflict.combatants", "military.military_combatant_group.combatants"], ["base.culturalevent.event.entity_involved", "people.person.nationality"], ["film.film_subject.films", "tv.tv_program.country_of_origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who fought in the gulf war 1991 [/INST]", "raw_output": {"paths": ["<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant.military_conflicts </PATH>", "<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant_group.combatants </PATH>"], "scores": [-0.04427634924650192, -0.06563381105661392], "norm_scores": [0.5053391456604004, 0.4946608245372772]}}
{"id": "WebQTest-119", "question": "who plays london tipton in suite life on deck", "prediction": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "ground_paths": [["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["tv.tv_program.genre", "music.artist.genre"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays london tipton in suite life on deck [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.03957035765051842, -0.07834052294492722], "norm_scores": [0.5096913576126099, 0.4903086721897125]}}
{"id": "WebQTest-121", "question": "what team did david beckham play for in 2011", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"]], "ground_paths": [["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["soccer.football_player_loan.player", "soccer.football_player_loan.lending_team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["book.written_work.subjects", "book.written_work.subjects"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did david beckham play for in 2011 [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>"], "scores": [-0.07302001118659973, -0.08872762322425842], "norm_scores": [0.5039268136024475, 0.4960731863975525]}}
{"id": "WebQTest-122", "question": "where did francisco coronado come from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did francisco coronado come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.03294028341770172, -0.1179160624742508], "norm_scores": [0.5212311744689941, 0.47876885533332825]}}
{"id": "WebQTest-123", "question": "what all does google now do", "prediction": [["organization.organization.sectors"], ["organization.organization.founders"]], "ground_paths": [["computer.software.developer"], ["projects.project_participation.participant", "projects.project_participation.project"], ["computer.computer_manufacturer_brand.computer_models"], ["business.consumer_company.products", "business.consumer_product.company"], ["business.company_product_relationship.company", "business.consumer_product.company"], ["computer.software_developer.software"], ["internet.website.owner"], ["business.consumer_company.products", "business.company_product_relationship.consumer_product"], ["business.company_product_relationship.company", "business.company_product_relationship.consumer_product"], ["business.consumer_company.products"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what all does google now do [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.sectors </PATH>", "<PATH> organization.organization.founders </PATH>"], "scores": [-0.1981913447380066, -0.21271850168704987], "norm_scores": [0.5036317110061646, 0.49636825919151306]}}
{"id": "WebQTest-124", "question": "what language does egyptian people speak", "prediction": [["people.ethnicity.people", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.ethnicity.people", "people.person.nationality", "language.human_language.main_country"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does egyptian people speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> language.human_language.main_country </PATH>"], "scores": [-0.09362676739692688, -0.11369849741458893], "norm_scores": [0.5050177574157715, 0.4949822425842285]}}
{"id": "WebQTest-125", "question": "what are abraham sons names", "prediction": [["people.person.children"], ["people.person.parents"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are abraham sons names [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.03514734283089638, -0.16496039927005768], "norm_scores": [0.5324077606201172, 0.4675922393798828]}}
{"id": "WebQTest-126", "question": "who wrote the jana gana mana", "prediction": [["music.composition.composer"], ["music.composition.lyricist"]], "ground_paths": [["music.lyricist.lyrics_written"], ["music.composition.composer"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who wrote the jana gana mana [/INST]", "raw_output": {"paths": ["<PATH> music.composition.composer </PATH>", "<PATH> music.composition.lyricist </PATH>"], "scores": [-0.043896231800317764, -0.14077915251255035], "norm_scores": [0.5242018103599548, 0.47579818964004517]}}
{"id": "WebQTest-127", "question": "what type of cancer did gilda radner die of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of cancer did gilda radner die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.028791159391403198, -0.1115616112947464], "norm_scores": [0.5206807851791382, 0.47931918501853943]}}
{"id": "WebQTest-128", "question": "who plays juni cortez", "prediction": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays juni cortez [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.06856682896614075, -0.09045596420764923], "norm_scores": [0.5054720640182495, 0.4945279061794281]}}
{"id": "WebQTest-129", "question": "what was lucille ball", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was lucille ball [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.008066859096288681, -0.28878501057624817], "norm_scores": [0.5697222948074341, 0.4302777647972107]}}
{"id": "WebQTest-131", "question": "who inspired obama", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "ground_paths": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who inspired obama [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.03419092297554016, -0.049514029175043106], "norm_scores": [0.5038307309150696, 0.4961692988872528]}}
{"id": "WebQTest-132", "question": "who is michael j fox wife", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["people.person.spouse_s", "people.marriage.spouse"], ["award.award_winner.awards_won", "award.award_honor.award_winner"], ["common.image.appears_in_topic_gallery", "common.topic.image"], ["people.person.children", "people.person.parents"], ["people.person.profession", "people.person.profession"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.parents"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is michael j fox wife [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.05345578119158745, -0.0889948382973671], "norm_scores": [0.5088838338851929, 0.4911161959171295]}}
{"id": "WebQTest-133", "question": "where did dolly parton grow up", "prediction": [["music.artist.origin"], ["people.person.place_of_birth"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did dolly parton grow up [/INST]", "raw_output": {"paths": ["<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.0491507388651371, -0.13340222835540771], "norm_scores": [0.5210503935813904, 0.47894957661628723]}}
{"id": "WebQTest-134", "question": "what type of books did agatha christie wrote", "prediction": [["book.book_edition.author_editor"], ["book.written_work.author"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of books did agatha christie wrote [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.04800114780664444, -0.11987678706645966], "norm_scores": [0.5179611444473267, 0.48203879594802856]}}
{"id": "WebQTest-138", "question": "what are the four main languages spoken in spain", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the four main languages spoken in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.08083898574113846, -0.0924484059214592], "norm_scores": [0.5029023289680481, 0.4970976710319519]}}
{"id": "WebQTest-139", "question": "where is the nra headquarters located", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the nra headquarters located [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>"], "scores": [-0.010652827098965645, -0.10084856301546097], "norm_scores": [0.522533655166626, 0.4774663746356964]}}
{"id": "WebQTest-141", "question": "who is gimli 's father in the hobbit", "prediction": [["fictional_universe.fictional_character.parents"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["fictional_universe.fictional_character.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is gimli 's father in the hobbit [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.parents </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.10992401838302612, -0.12117230147123337], "norm_scores": [0.5028120279312134, 0.49718794226646423]}}
{"id": "WebQTest-142", "question": "who developed the tcp ip reference model", "prediction": [["law.invention.inventor"], ["internet.website.owner", "organization.organization_founder.organizations_founded"]], "ground_paths": [["base.argumentmaps.original_idea.innovator"], ["base.argumentmaps.innovator.original_ideas"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who developed the tcp ip reference model [/INST]", "raw_output": {"paths": ["<PATH> law.invention.inventor </PATH>", "<PATH> internet.website.owner <SEP> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.04988975450396538, -0.184320867061615], "norm_scores": [0.5335572361946106, 0.466442734003067]}}
{"id": "WebQTest-145", "question": "what college did albert einstein go to", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.employment_history", "business.employer.employees"], ["business.employment_tenure.person", "business.employer.employees"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.nationality", "organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did albert einstein go to [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.06013975292444229, -0.07568208873271942], "norm_scores": [0.5038855075836182, 0.49611449241638184]}}
{"id": "WebQTest-146", "question": "what team does jordan own", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team_owner.teams_owned"]], "ground_paths": [["sports.sports_team.location"], ["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does jordan own [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team_owner.teams_owned </PATH>"], "scores": [-0.013217853382229805, -0.15041588246822357], "norm_scores": [0.5342457890510559, 0.4657541811466217]}}
{"id": "WebQTest-147", "question": "who is the state governor of tennessee", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["location.administrative_division.first_level_division_of", "people.person.nationality"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["base.locations.countries.states_provinces_within", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the state governor of tennessee [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> location.administrative_division.first_level_division_of <SEP> people.person.nationality </PATH>"], "scores": [-0.05115439370274544, -0.05191502720117569], "norm_scores": [0.5001901388168335, 0.4998098313808441]}}
{"id": "WebQTest-149", "question": "what do portuguese people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do portuguese people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.05580843612551689, -0.07883096486330032], "norm_scores": [0.5057553648948669, 0.49424460530281067]}}
{"id": "WebQTest-150", "question": "what language does cuba speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does cuba speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.034058794379234314, -0.07636409997940063], "norm_scores": [0.5105747580528259, 0.4894252419471741]}}
{"id": "WebQTest-153", "question": "what are the sights to see in madrid", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"], ["base.arthist.helynevek.intezmeny"], ["common.topic.image"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the sights to see in madrid [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.05687659606337547, -0.10814569890499115], "norm_scores": [0.512814462184906, 0.487185537815094]}}
{"id": "WebQTest-154", "question": "what instruments did louis armstrong play", "prediction": [["music.group_member.instruments_played"], ["music.artist.track_contributions", "music.track_contribution.role"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what instruments did louis armstrong play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>"], "scores": [-0.0010278162080794573, -0.24876351654529572], "norm_scores": [0.5616191029548645, 0.4383808970451355]}}
{"id": "WebQTest-155", "question": "what year was george w bush elected", "prediction": [["time.recurring_event.instances"], ["time.event.locations", "location.location.events"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was george w bush elected [/INST]", "raw_output": {"paths": ["<PATH> time.recurring_event.instances </PATH>", "<PATH> time.event.locations <SEP> location.location.events </PATH>"], "scores": [-0.062468547374010086, -0.17664121091365814], "norm_scores": [0.528512179851532, 0.47148779034614563]}}
{"id": "WebQTest-156", "question": "what time zone am i in california", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone am i in california [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>"], "scores": [-8.417910430580378e-05, -0.4444323480129242], "norm_scores": [0.609294593334198, 0.3907053768634796]}}
{"id": "WebQTest-157", "question": "where is mission san buenaventura located", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mission san buenaventura located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0046868775971233845, -0.40624096989631653], "norm_scores": [0.5990610122680664, 0.40093904733657837]}}
{"id": "WebQTest-159", "question": "what time in hilo hawaii", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time in hilo hawaii [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>"], "scores": [-0.0003762721025850624, -0.30775341391563416], "norm_scores": [0.5762448906898499, 0.42375504970550537]}}
{"id": "WebQTest-160", "question": "where did rudolf virchow conduct his research", "prediction": [["people.deceased_person.place_of_death", "location.location.containedby"], ["business.employment_tenure.person", "business.employer.employees"]], "ground_paths": [["people.deceased_person.place_of_death", "location.location.containedby"], ["business.employment_tenure.person", "business.employer.employees"], ["people.person.employment_history", "business.employment_tenure.company"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did rudolf virchow conduct his research [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employer.employees </PATH>"], "scores": [-0.03389710932970047, -0.0704331025481224], "norm_scores": [0.509132981300354, 0.490867018699646]}}
{"id": "WebQTest-161", "question": "what university did gordon brown attend", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what university did gordon brown attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.04164597764611244, -0.054627615958452225], "norm_scores": [0.5032453536987305, 0.49675461649894714]}}
{"id": "WebQTest-162", "question": "what countries does greece share borders with", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"]], "ground_paths": [["organization.organization.founders", "organization.organization.founders"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.location.containedby", "location.location.containedby"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["location.location.containedby", "base.locations.countries.continent"], ["location.location.partially_contains", "geography.river.basin_countries"], ["location.country.form_of_government", "location.country.form_of_government"], ["media_common.netflix_title.netflix_genres", "film.film.country"], ["location.location.time_zones", "location.location.time_zones"], ["military.military_combatant.military_conflicts", "military.military_combatant_group.combatants"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["location.location.containedby", "base.locations.continents.countries_within"], ["location.location.containedby", "location.location.partially_containedby"], ["location.location.partially_containedby", "location.location.partially_contains"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.statistical_region.places_imported_from", "location.statistical_region.places_exported_to"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["location.location.events", "time.event.locations"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["location.location.events", "location.location.events"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.partially_contains", "location.location.partially_contains"], ["media_common.netflix_genre.titles", "film.film.country"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.country.official_language", "location.country.languages_spoken"], ["organization.organization_founder.organizations_founded", "organization.organization_founder.organizations_founded"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["time.event.locations", "time.event.locations"], ["location.location.partially_contains", "location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does greece share borders with [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>"], "scores": [-0.05179154500365257, -0.081446073949337], "norm_scores": [0.5074130892753601, 0.4925869107246399]}}
{"id": "WebQTest-163", "question": "what year did the seahawks win the superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the seahawks win the superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.04903117194771767, -0.053888943046331406], "norm_scores": [0.5012144446372986, 0.4987855553627014]}}
{"id": "WebQTest-164", "question": "what did stephen hawking become famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did stephen hawking become famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.012557312846183777, -0.2500661313533783], "norm_scores": [0.5590996146202087, 0.44090035557746887]}}
{"id": "WebQTest-165", "question": "who plays donna noble", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_guest_role.character", "tv.tv_guest_role.actor"], ["fictional_universe.fictional_character.ethnicity", "people.person.ethnicity"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_guest_role.character", "tv.tv_actor.guest_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays donna noble [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.07428454607725143, -0.09136581420898438], "norm_scores": [0.5042701959609985, 0.49572980403900146]}}
{"id": "WebQTest-166", "question": "what was dr seuss education", "prediction": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was dr seuss education [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.03418944031000137, -0.03934214636683464], "norm_scores": [0.5012881755828857, 0.49871182441711426]}}
{"id": "WebQTest-167", "question": "where did queensland get its name from", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"]], "ground_paths": [["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did queensland get its name from [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.030517516657710075, -0.12439980357885361], "norm_scores": [0.5234533548355103, 0.47654664516448975]}}
{"id": "WebQTest-168", "question": "what document did james madison write", "prediction": [["book.written_work.author"], ["book.author.works_written"]], "ground_paths": [["book.book_subject.works"], ["book.written_work.author"], ["book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what document did james madison write [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.06765832006931305, -0.08776245266199112], "norm_scores": [0.5050258636474609, 0.49497413635253906]}}
{"id": "WebQTest-169", "question": "what are the three official languages of belgium", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the three official languages of belgium [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.04478523135185242, -0.0781254842877388], "norm_scores": [0.5083343386650085, 0.49166572093963623]}}
{"id": "WebQTest-170", "question": "who played bilbo in lord of the rings", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["people.person.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played bilbo in lord of the rings [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.08961447328329086, -0.10150644183158875], "norm_scores": [0.5029729604721069, 0.49702703952789307]}}
{"id": "WebQTest-171", "question": "what airport do you fly into to get to destin fl", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"]], "ground_paths": [["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what airport do you fly into to get to destin fl [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>"], "scores": [-0.052866920828819275, -0.09318497031927109], "norm_scores": [0.5100781321525574, 0.4899218678474426]}}
{"id": "WebQTest-172", "question": "what do you call members of the senate", "prediction": [["government.government_office_or_title.governmental_body_if_any", "common.topic.notable_types", "common.topic.notable_types"], ["government.government_position_held.governmental_body", "government.government_position_held.office_position_or_title"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do you call members of the senate [/INST]", "raw_output": {"paths": ["<PATH> government.government_office_or_title.governmental_body_if_any <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.02400275692343712, -0.041660815477371216], "norm_scores": [0.5044143795967102, 0.4955855906009674]}}
{"id": "WebQTest-173", "question": "what countries are part of the baltic", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are part of the baltic [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.059630680829286575, -0.1233706921339035], "norm_scores": [0.515929639339447, 0.48407039046287537]}}
{"id": "WebQTest-174", "question": "what team does colin kaepernick play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does colin kaepernick play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.03409845754504204, -0.06258117407560349], "norm_scores": [0.5071201920509338, 0.49287980794906616]}}
{"id": "WebQTest-175", "question": "where did joe flacco attend college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did joe flacco attend college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.054520197212696075, -0.06331662088632584], "norm_scores": [0.502199113368988, 0.49780091643333435]}}
{"id": "WebQTest-176", "question": "who is emma stone father", "prediction": [["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is emma stone father [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.0428687185049057, -0.14588534832000732], "norm_scores": [0.5257313847541809, 0.4742686152458191]}}
{"id": "WebQTest-177", "question": "where was theodore roosevelt buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"]], "ground_paths": [["people.place_of_interment.interred_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was theodore roosevelt buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>"], "scores": [-0.036141905933618546, -0.0872771292924881], "norm_scores": [0.512781023979187, 0.487218976020813]}}
{"id": "WebQTest-178", "question": "who was the leader of the us during wwii", "prediction": [["event.speech_or_presentation.speech_topic", "event.speech_or_presentation.speaker_s"], ["government.government_office_or_title.office_holders", "government.government_position_held.office_holder"]], "ground_paths": [["event.speech_topic.speeches_or_presentations_on_this_topic", "event.speech_or_presentation.speaker_s"], ["military.military_command.military_conflict", "military.military_commander.military_commands"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the leader of the us during wwii [/INST]", "raw_output": {"paths": ["<PATH> event.speech_or_presentation.speech_topic <SEP> event.speech_or_presentation.speaker_s </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.03957418352365494, -0.06026579812169075], "norm_scores": [0.5051727294921875, 0.4948273003101349]}}
{"id": "WebQTest-179", "question": "what artistic movement did henri matisse belong to", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what artistic movement did henri matisse belong to [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.004059110302478075, -0.1394175887107849], "norm_scores": [0.5337880253791809, 0.4662119448184967]}}
{"id": "WebQTest-180", "question": "where the missouri river ends", "prediction": [["geography.river.mouth"], ["location.location.partially_containedby"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where the missouri river ends [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.019399888813495636, -0.2339981645345688], "norm_scores": [0.5534446239471436, 0.44655540585517883]}}
{"id": "WebQTest-181", "question": "what type of currency do they use in thailand", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of currency do they use in thailand [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.04571885988116264, -0.08775238692760468], "norm_scores": [0.510506808757782, 0.48949316143989563]}}
{"id": "WebQTest-182", "question": "who is khloe kardashian 's husband", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["people.marriage.spouse", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is khloe kardashian 's husband [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.049632810056209564, -0.05338872969150543], "norm_scores": [0.5009390115737915, 0.49906104803085327]}}
{"id": "WebQTest-184", "question": "what state does romney live in", "prediction": [["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.administrative_division.country"]], "ground_paths": [["people.person.nationality", "location.administrative_division.country"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"], ["people.person.places_lived", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state does romney live in [/INST]", "raw_output": {"paths": ["<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.country </PATH>"], "scores": [-0.08572391420602798, -0.11248165369033813], "norm_scores": [0.5066890716552734, 0.49331095814704895]}}
{"id": "WebQTest-185", "question": "what type of government does france use", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does france use [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>"], "scores": [-0.04378243535757065, -0.046163953840732574], "norm_scores": [0.5005953907966614, 0.4994046092033386]}}
{"id": "WebQTest-186", "question": "who rules denmark right now", "prediction": [["royalty.monarch.kingdom"], ["people.person.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who rules denmark right now [/INST]", "raw_output": {"paths": ["<PATH> royalty.monarch.kingdom </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.046427447348833084, -0.09717945754528046], "norm_scores": [0.5126852989196777, 0.48731473088264465]}}
{"id": "WebQTest-187", "question": "what did stephen hawking study", "prediction": [["education.education.student", "education.education.major_field_of_study"], ["people.person.profession"]], "ground_paths": [["education.education.student", "education.education.major_field_of_study"], ["book.book_edition.author_editor", "book.written_work.subjects"], ["people.person.profession", "people.professional_field.professions_in_this_field"], ["book.author.works_written", "book.written_work.subjects"], ["book.written_work.author", "book.written_work.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did stephen hawking study [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.04992557689547539, -0.10820560902357101], "norm_scores": [0.5145658850669861, 0.4854341149330139]}}
{"id": "WebQTest-188", "question": "where did bristol palin go to school", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did bristol palin go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.022401008754968643, -0.057463131844997406], "norm_scores": [0.5087646245956421, 0.4912354052066803]}}
{"id": "WebQTest-189", "question": "what region of the world is egypt associated with", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what region of the world is egypt associated with [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.09984980523586273, -0.12311407178640366], "norm_scores": [0.5058158040046692, 0.4941841959953308]}}
{"id": "WebQTest-190", "question": "what did dr. seuss do", "prediction": [["people.person.profession"], ["book.book.interior_illustrations_by"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did dr. seuss do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> book.book.interior_illustrations_by </PATH>"], "scores": [-0.04556983336806297, -0.11201252788305283], "norm_scores": [0.5166045427322388, 0.48339542746543884]}}
{"id": "WebQTest-191", "question": "where did drew brees go to college wikianswers", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did drew brees go to college wikianswers [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.052554260939359665, -0.06525681912899017], "norm_scores": [0.5031755566596985, 0.4968244135379791]}}
{"id": "WebQTest-193", "question": "what school did ben roethlisberger go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did ben roethlisberger go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.06395226716995239, -0.06887299567461014], "norm_scores": [0.5012301802635193, 0.4987698197364807]}}
{"id": "WebQTest-194", "question": "where did the iroquois indians come from", "prediction": [["people.ethnicity.geographic_distribution"], ["people.ethnicity.geographic_distribution", "base.locations.states_and_provences.country"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the iroquois indians come from [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> base.locations.states_and_provences.country </PATH>"], "scores": [-0.0009086983045563102, -0.2612690031528473], "norm_scores": [0.564724862575531, 0.435275137424469]}}
{"id": "WebQTest-195", "question": "what kind of guitar did george harrison use", "prediction": [["music.group_member.instruments_played"], ["music.guitar.guitarists"]], "ground_paths": [["music.guitar.guitarists"], ["music.guitarist.guitars_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of guitar did george harrison use [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.guitar.guitarists </PATH>"], "scores": [-0.03616516292095184, -0.18347714841365814], "norm_scores": [0.5367615222930908, 0.4632384777069092]}}
{"id": "WebQTest-196", "question": "what language do navajo people speak", "prediction": [["people.ethnicity.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do navajo people speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.0024811159819364548, -0.3922085464000702], "norm_scores": [0.5962170958518982, 0.4037829041481018]}}
{"id": "WebQTest-197", "question": "what places make up new england", "prediction": [["location.location.contains"], ["location.location.containedby"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what places make up new england [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.05660735443234444, -0.10309672355651855], "norm_scores": [0.5116202235221863, 0.48837974667549133]}}
{"id": "WebQTest-199", "question": "what type of government does iraq have now", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does iraq have now [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.020319271832704544, -0.08831365406513214], "norm_scores": [0.5169920325279236, 0.4830079674720764]}}
{"id": "WebQTest-200", "question": "where was the vietnam war location", "prediction": [["time.event.locations"], ["location.location.events"]], "ground_paths": [["book.written_work.subjects"], ["book.book_subject.works"], ["time.event.locations"], ["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the vietnam war location [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.045668505132198334, -0.18614162504673004], "norm_scores": [0.5350606441497803, 0.4649393558502197]}}
{"id": "WebQTest-201", "question": "who did gerald ford select as his vice president when he became president", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did gerald ford select as his vice president when he became president [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>"], "scores": [-0.023542121052742004, -0.07300733774900436], "norm_scores": [0.5123637318611145, 0.4876362085342407]}}
{"id": "WebQTest-202", "question": "what is the currency used in italy", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency used in italy [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.026752417907118797, -0.12131602317094803], "norm_scores": [0.5236232876777649, 0.4763767123222351]}}
{"id": "WebQTest-204", "question": "what is the capital of modern egypt", "prediction": [["location.country.capital"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of modern egypt [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.05742093548178673, -0.12663254141807556], "norm_scores": [0.5172960162162781, 0.4827040135860443]}}
{"id": "WebQTest-205", "question": "what school did sir ernest rutherford go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.contains"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did sir ernest rutherford go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.047605618834495544, -0.05471084266901016], "norm_scores": [0.5017762780189514, 0.4982237219810486]}}
{"id": "WebQTest-206", "question": "what position does brandon roy play", "prediction": [["basketball.basketball_player.position_s"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.position"]], "ground_paths": [["basketball.basketball_player.position_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what position does brandon roy play [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.position_s </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.position </PATH>"], "scores": [-0.037100158631801605, -0.08185013383626938], "norm_scores": [0.5111856460571289, 0.4888143539428711]}}
{"id": "WebQTest-207", "question": "where is harvard university", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is harvard university [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.016824906691908836, -0.2847895920276642], "norm_scores": [0.5665931701660156, 0.43340685963630676]}}
{"id": "WebQTest-209", "question": "where is laos in world map", "prediction": [["location.location.containedby"], ["base.locations.countries.continent"]], "ground_paths": [["book.book_subject.works"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is laos in world map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.0777195394039154, -0.11439124494791031], "norm_scores": [0.5091668963432312, 0.4908330738544464]}}
{"id": "WebQTest-210", "question": "where is tyrese gibson from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is tyrese gibson from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.02580724097788334, -0.11286380887031555], "norm_scores": [0.5217503905296326, 0.47824957966804504]}}
{"id": "WebQTest-211", "question": "what countries are near italy", "prediction": [["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.partially_contains", "location.location.partially_contains"]], "ground_paths": [["sports.sport_country.multi_event_tournaments_participated_in", "sports.sport_country.multi_event_tournaments_participated_in"], ["location.location.containedby", "location.location.containedby"], ["location.location.containedby", "base.locations.countries.continent"], ["fictional_universe.fictional_setting.works_set_here", "fictional_universe.work_of_fiction.setting"], ["travel.guidebook.travel_destinations_covered", "travel.travel_destination.guidebooks"], ["location.location.time_zones", "location.location.time_zones"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.containedby", "geography.river.basin_countries"], ["military.military_combatant.military_conflicts", "military.military_combatant_group.combatants"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.country.currency_used", "finance.currency.countries_used"], ["military.military_combatant.includes_allies", "military.military_combatant.includes_allies"], ["location.country.official_language", "language.human_language.main_country"], ["geography.lake.basin_countries", "geography.lake.basin_countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["fictional_universe.fictional_setting.universe", "fictional_universe.fictional_setting.universe"], ["location.location.partially_contains", "geography.river.origin"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["location.country.official_language", "location.country.official_language"], ["government.form_of_government.countries", "location.country.form_of_government"], ["base.locations.countries.continent", "location.location.contains_major_portion_of"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.multi_event_tournament.participating_countries"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.partially_contains", "location.location.partially_contains"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["geography.river.basin_countries", "location.location.partially_contains"], ["organization.organization_founder.organizations_founded", "organization.organization_founder.organizations_founded"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["organization.organization.founders", "base.nations.supranational_body.member"], ["base.locations.countries.continent", "location.location.containedby"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["base.locations.countries.continent", "base.locations.countries.continent"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["location.country.form_of_government", "government.form_of_government.countries"], ["biology.breed_origin.breeds_originating_here", "biology.animal_breed.place_of_origin"], ["location.country.currency_used", "location.country.currency_used"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["organization.organization.founders", "organization.organization.founders"], ["location.location.partially_contains", "location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are near italy [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>"], "scores": [-0.1004679948091507, -0.11223510652780533], "norm_scores": [0.5029417276382446, 0.49705827236175537]}}
{"id": "WebQTest-212", "question": "which states does the colorado river run through", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "ground_paths": [["location.location.partially_contains"], ["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which states does the colorado river run through [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.05300974100828171, -0.09390581399202347], "norm_scores": [0.5102226138114929, 0.48977741599082947]}}
{"id": "WebQTest-213", "question": "who did scarlett johansson date", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["film.film.executive_produced_by", "film.producer.films_executive_produced"], ["people.person.languages", "people.person.languages"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["base.popstra.celebrity.breakup", "base.popstra.breakup.participant"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["people.person.profession", "people.person.profession"], ["people.person.children", "people.person.children"], ["people.person.spouse_s", "people.marriage.spouse"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["people.marriage.spouse", "people.person.spouse_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.celebrity.breakup", "base.popstra.celebrity.breakup"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did scarlett johansson date [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.04146731644868851, -0.08071164041757584], "norm_scores": [0.5098097920417786, 0.49019017815589905]}}
{"id": "WebQTest-214", "question": "which continents were part of the roman empire", "prediction": [["fictional_universe.fictional_setting.languages", "language.human_language.region"], ["location.location.partially_contained_by", "location.partial_containment_relationship.partially_contained_by"]], "ground_paths": [["location.country.official_language", "language.human_language.region"], ["fictional_universe.fictional_setting.languages", "language.human_language.region"], ["location.location.partially_contained_by", "location.partial_containment_relationship.partially_contained_by"], ["location.location.partially_contained_by", "location.location.partiallycontains"], ["book.book_subject.works", "book.written_work.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which continents were part of the roman empire [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_setting.languages <SEP> language.human_language.region </PATH>", "<PATH> location.location.partially_contained_by <SEP> location.partial_containment_relationship.partially_contained_by </PATH>"], "scores": [-0.047817740589380264, -0.07032594829797745], "norm_scores": [0.5056268572807312, 0.4943732023239136]}}
{"id": "WebQTest-215", "question": "where was elvis costello born", "prediction": [["location.location.people_born_here"], ["music.artist.origin"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was elvis costello born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.07255364209413528, -0.10056684911251068], "norm_scores": [0.5070028305053711, 0.4929971396923065]}}
{"id": "WebQTest-216", "question": "where are the gobi desert located on a map", "prediction": [["location.location.containedby"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the gobi desert located on a map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.008212966844439507, -0.25908952951431274], "norm_scores": [0.5623922348022461, 0.4376077950000763]}}
{"id": "WebQTest-217", "question": "what type of government does the us follow", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does the us follow [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.022595131769776344, -0.08212834596633911], "norm_scores": [0.5148789286613464, 0.48512113094329834]}}
{"id": "WebQTest-218", "question": "in which continent is germany", "prediction": [["location.location.containedby"], ["base.locations.countries.continent"]], "ground_paths": [["base.locations.countries.continent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: in which continent is germany [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.03508623316884041, -0.13293655216693878], "norm_scores": [0.5244430899620056, 0.4755569398403168]}}
{"id": "WebQTest-220", "question": "what language turkey people speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language turkey people speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.024839505553245544, -0.13205952942371368], "norm_scores": [0.5267793536186218, 0.4732206463813782]}}
{"id": "WebQTest-221", "question": "who does jeremy lin play for", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["people.person.place_of_birth", "sports.sports_team.location"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does jeremy lin play for [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.031712211668491364, -0.0641448125243187], "norm_scores": [0.5081074237823486, 0.49189257621765137]}}
{"id": "WebQTest-222", "question": "where is the seychelles on world map", "prediction": [["location.location.containedby"], ["base.locations.continents.countries_within"]], "ground_paths": [["location.location.contains"], ["base.locations.continents.countries_within"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the seychelles on world map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.020641645416617393, -0.16875040531158447], "norm_scores": [0.5369596481323242, 0.4630403220653534]}}
{"id": "WebQTest-224", "question": "where did william morris go to college", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did william morris go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.04771154746413231, -0.051874443888664246], "norm_scores": [0.5010406970977783, 0.4989592730998993]}}
{"id": "WebQTest-225", "question": "what state is mount st. helens in", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state is mount st. helens in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.021463776007294655, -0.20847733318805695], "norm_scores": [0.5466176271438599, 0.4533824324607849]}}
{"id": "WebQTest-226", "question": "who is jimmy savile", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is jimmy savile [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.004173768684267998, -0.3467584252357483], "norm_scores": [0.5848182439804077, 0.4151817858219147]}}
{"id": "WebQTest-227", "question": "where were the seneca indians located", "prediction": [["people.ethnicity.geographic_distribution"], ["location.location.containedby"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where were the seneca indians located [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.003064631950110197, -0.4382076859474182], "norm_scores": [0.6071010828018188, 0.39289888739585876]}}
{"id": "WebQTest-231", "question": "when did florida marlins join mlb", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.championships"]], "ground_paths": [["baseball.baseball_team.team_stats", "baseball.baseball_team_stats.season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did florida marlins join mlb [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.championships </PATH>"], "scores": [-0.06175148859620094, -0.08050396293401718], "norm_scores": [0.504688024520874, 0.49531200528144836]}}
{"id": "WebQTest-232", "question": "what countries have spanish as the national language", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries have spanish as the national language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.0549461804330349, -0.1050528883934021], "norm_scores": [0.5125240683555603, 0.4874759316444397]}}
{"id": "WebQTest-233", "question": "what to see in phoenix arizona", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "ground_paths": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to see in phoenix arizona [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>"], "scores": [-0.04524035379290581, -0.04775054752826691], "norm_scores": [0.5006275177001953, 0.4993724524974823]}}
{"id": "WebQTest-234", "question": "what countries does the panama canal go through", "prediction": [["location.location.containedby"], ["geography.body_of_water.bridges", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does the panama canal go through [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> geography.body_of_water.bridges <SEP> location.location.containedby </PATH>"], "scores": [-0.005533725488930941, -0.21052850782871246], "norm_scores": [0.551069974899292, 0.448930025100708]}}
{"id": "WebQTest-235", "question": "what is charlie chaplin famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is charlie chaplin famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.02187279611825943, -0.1628999412059784], "norm_scores": [0.535198450088501, 0.464801549911499]}}
{"id": "WebQTest-237", "question": "what did fred durst do", "prediction": [["people.person.profession"], ["music.artist.genre", "music.artist.genre"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did fred durst do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>"], "scores": [-0.01950756087899208, -0.22491008043289185], "norm_scores": [0.551170825958252, 0.44882914423942566]}}
{"id": "WebQTest-239", "question": "where do american bulldogs originate from", "prediction": [["biology.animal_breed.breed_of"], ["biology.breed_origin.breeds_originating_here"]], "ground_paths": [["biology.animal_breed.place_of_origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do american bulldogs originate from [/INST]", "raw_output": {"paths": ["<PATH> biology.animal_breed.breed_of </PATH>", "<PATH> biology.breed_origin.breeds_originating_here </PATH>"], "scores": [-0.030761443078517914, -0.05673414468765259], "norm_scores": [0.5064927935600281, 0.4935072064399719]}}
{"id": "WebQTest-240", "question": "what colleges did albert einstein teach at", "prediction": [["people.person.employment_history", "business.employer.employees"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.employment_history", "business.employment_tenure.company"], ["people.person.employment_history", "business.employer.employees"], ["business.employment_tenure.person", "business.employer.employees"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.department.academics_old", "education.university.departments"], ["people.person.nationality", "organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what colleges did albert einstein teach at [/INST]", "raw_output": {"paths": ["<PATH> people.person.employment_history <SEP> business.employer.employees </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.05204642191529274, -0.10941150039434433], "norm_scores": [0.5143373012542725, 0.48566266894340515]}}
{"id": "WebQTest-241", "question": "who plays the voice of brian on family guy", "prediction": [["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays the voice of brian on family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.06351063400506973, -0.06546758860349655], "norm_scores": [0.5004892349243164, 0.4995107650756836]}}
{"id": "WebQTest-243", "question": "where did george w bush live as a child", "prediction": [["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did george w bush live as a child [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.0483890138566494, -0.08062973618507385], "norm_scores": [0.5080595016479492, 0.49194052815437317]}}
{"id": "WebQTest-245", "question": "what year was the first miss america pageant held", "prediction": [["time.event.instance_of_recurring_event"], ["time.recurring_event.instances"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was the first miss america pageant held [/INST]", "raw_output": {"paths": ["<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.recurring_event.instances </PATH>"], "scores": [-0.030095485970377922, -0.10635046660900116], "norm_scores": [0.5190545320510864, 0.48094549775123596]}}
{"id": "WebQTest-246", "question": "what was nikola tesla inventions", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"]], "ground_paths": [["symbols.name_source.namesakes"], ["law.inventor.inventions"], ["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was nikola tesla inventions [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>"], "scores": [-0.028162486851215363, -0.19485755264759064], "norm_scores": [0.5415775179862976, 0.45842245221138]}}
{"id": "WebQTest-247", "question": "where did flemish people come from", "prediction": [["people.ethnicity.geographic_distribution"], ["people.ethnicity.included_in_group"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did flemish people come from [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> people.ethnicity.included_in_group </PATH>"], "scores": [-0.0003288559091743082, -0.49618786573410034], "norm_scores": [0.621485710144043, 0.3785143196582794]}}
{"id": "WebQTest-248", "question": "what state new york city belong to", "prediction": [["location.location.containedby"], ["location.administrative_division.country"]], "ground_paths": [["base.biblioness.bibs_location.state"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state new york city belong to [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-0.06023351103067398, -0.20873168110847473], "norm_scores": [0.5370564460754395, 0.46294352412223816]}}
{"id": "WebQTest-249", "question": "what was the cause of death for bruce lee", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the cause of death for bruce lee [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.0037755598314106464, -0.20133689045906067], "norm_scores": [0.5492302775382996, 0.45076969265937805]}}
{"id": "WebQTest-250", "question": "what type of government does usa follow", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does usa follow [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.027338378131389618, -0.07114439457654953], "norm_scores": [0.5109497308731079, 0.4890502393245697]}}
{"id": "WebQTest-251", "question": "what type of currency do they use in england", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of currency do they use in england [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.010784902609884739, -0.21297264099121094], "norm_scores": [0.5503754615783691, 0.44962456822395325]}}
{"id": "WebQTest-252", "question": "where was benjamin franklin educated", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was benjamin franklin educated [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.038067277520895004, -0.0718606635928154], "norm_scores": [0.508447527885437, 0.491552472114563]}}
{"id": "WebQTest-253", "question": "what countries included in the uk", "prediction": [["base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries included in the uk [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.061148207634687424, -0.0716727152466774], "norm_scores": [0.5026311278343201, 0.4973689019680023]}}
{"id": "WebQTest-254", "question": "what countries in the world speak chinese", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries in the world speak chinese [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.07737293839454651, -0.08471143990755081], "norm_scores": [0.5018346309661865, 0.4981653690338135]}}
{"id": "WebQTest-255", "question": "where obama went to school", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where obama went to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.032471515238285065, -0.07851877063512802], "norm_scores": [0.5115097761154175, 0.4884902536869049]}}
{"id": "WebQTest-256", "question": "what is the primary language of china", "prediction": [["language.human_language.main_country"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the primary language of china [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.03234922140836716, -0.16137942671775818], "norm_scores": [0.5322128534317017, 0.46778711676597595]}}
{"id": "WebQTest-257", "question": "who was the soviet leader during world war ii", "prediction": [["military.military_person.participated_in_conflicts"], ["people.person.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the soviet leader during world war ii [/INST]", "raw_output": {"paths": ["<PATH> military.military_person.participated_in_conflicts </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.032114673405885696, -0.07236069440841675], "norm_scores": [0.5100601315498352, 0.4899398386478424]}}
{"id": "WebQTest-258", "question": "which of the following does australia export the most", "prediction": [["location.statistical_region.major_exports", "location.imports_exports_by_industry.industry"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"]], "ground_paths": [["location.statistical_region.major_exports", "location.imports_exports_by_industry.industry"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which of the following does australia export the most [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.major_exports <SEP> location.imports_exports_by_industry.industry </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>"], "scores": [-0.022406671196222305, -0.05866219848394394], "norm_scores": [0.5090628862380981, 0.49093711376190186]}}
{"id": "WebQTest-260", "question": "where was the city of david", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the city of david [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.029558977112174034, -0.21183522045612335], "norm_scores": [0.5454432964324951, 0.4545566737651825]}}
{"id": "WebQTest-261", "question": "what is cindy sherman known for", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is cindy sherman known for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.035689547657966614, -0.1596056967973709], "norm_scores": [0.5309394598007202, 0.4690605401992798]}}
{"id": "WebQTest-262", "question": "what tv shows did shawnee smith play in", "prediction": [["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.nationality", "film.film.country"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what tv shows did shawnee smith play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>"], "scores": [-0.06844553351402283, -0.08922714740037918], "norm_scores": [0.5051952004432678, 0.4948047697544098]}}
{"id": "WebQTest-264", "question": "what does australia use for currency", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does australia use for currency [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.0594438835978508, -0.06770297139883041], "norm_scores": [0.5020647048950195, 0.4979352355003357]}}
{"id": "WebQTest-265", "question": "which airport to fly into rome", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"]], "ground_paths": [["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which airport to fly into rome [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>"], "scores": [-0.027318209409713745, -0.1452578753232956], "norm_scores": [0.5294507741928101, 0.47054922580718994]}}
{"id": "WebQTest-266", "question": "what is there to do for fun in kansas city", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.place_with_neighborhoods.neighborhoods"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is there to do for fun in kansas city [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.0393395721912384, -0.0570068359375], "norm_scores": [0.5044167041778564, 0.49558326601982117]}}
{"id": "WebQTest-267", "question": "where was martin luther king jr raised", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was martin luther king jr raised [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0029001040384173393, -0.28692540526390076], "norm_scores": [0.5705327987670898, 0.42946720123291016]}}
{"id": "WebQTest-268", "question": "what year was lebron james rookie season", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"]], "ground_paths": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was lebron james rookie season [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.season </PATH>"], "scores": [-0.028252936899662018, -0.03996896371245384], "norm_scores": [0.5029289722442627, 0.4970710575580597]}}
{"id": "WebQTest-269", "question": "where does delaware river start", "prediction": [["geography.river.origin"], ["location.location.containedby"]], "ground_paths": [["geography.river.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does delaware river start [/INST]", "raw_output": {"paths": ["<PATH> geography.river.origin </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.00523519329726696, -0.4021227955818176], "norm_scores": [0.597939670085907, 0.4020603597164154]}}
{"id": "WebQTest-270", "question": "what kind of monarchy does japan have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of monarchy does japan have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>"], "scores": [-0.031240137293934822, -0.08456049859523773], "norm_scores": [0.5133269429206848, 0.4866730570793152]}}
{"id": "WebQTest-271", "question": "where did mitt romney 's parents come from", "prediction": [["people.person.place_of_birth"], ["people.person.parents"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did mitt romney 's parents come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.06552013754844666, -0.15876710414886475], "norm_scores": [0.5232948660850525, 0.4767051041126251]}}
{"id": "WebQTest-273", "question": "when did michael jordan return to the nba", "prediction": [["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.draft"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did michael jordan return to the nba [/INST]", "raw_output": {"paths": ["<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.draft </PATH>"], "scores": [-0.02866201475262642, -0.06232950836420059], "norm_scores": [0.5084160566329956, 0.491583913564682]}}
{"id": "WebQTest-276", "question": "where was mission san rafael located", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was mission san rafael located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0033118717838078737, -0.4726751446723938], "norm_scores": [0.6152330636978149, 0.38476696610450745]}}
{"id": "WebQTest-277", "question": "who was esther 's husband", "prediction": [["people.marriage.spouse", "people.marriage.spouse"], ["people.person.children", "people.person.children"]], "ground_paths": [["visual_art.art_subject.artwork_on_the_subject", "visual_art.artwork.art_subject"], ["people.person.spouse_s", "people.person.spouse_s"], ["common.topic.notable_types", "common.topic.notable_types"], ["book.translated_work.translations", "book.book_subject.works"], ["visual_art.art_subject.artwork_on_the_subject", "visual_art.art_subject.artwork_on_the_subject"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was esther 's husband [/INST]", "raw_output": {"paths": ["<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.09391149133443832, -0.1902707815170288], "norm_scores": [0.524071216583252, 0.47592881321907043]}}
{"id": "WebQTest-278", "question": "who 's dating claire danes", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["base.popstra.infidelity.perpetrator", "base.popstra.infidelity.participant"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.profession", "people.person.profession"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.dated.participant", "base.popstra.dated.participant"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who 's dating claire danes [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.04360594227910042, -0.10125669836997986], "norm_scores": [0.5144087076187134, 0.485591322183609]}}
{"id": "WebQTest-279", "question": "what county is st paul va in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st paul va in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.07039259374141693, -0.12872204184532166], "norm_scores": [0.5145782232284546, 0.4854218065738678]}}
{"id": "WebQTest-281", "question": "what type of sports do japanese play", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"]], "ground_paths": [["sports.sports_team.location"], ["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of sports do japanese play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>"], "scores": [-0.044899437576532364, -0.05882171541452408], "norm_scores": [0.5034804940223694, 0.4965195059776306]}}
{"id": "WebQTest-282", "question": "which countries share a border with russia", "prediction": [["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.partially_containedby", "location.location.partially_contains"]], "ground_paths": [["organization.organization.founders", "organization.organization.founders"], ["location.location.partially_contains", "location.location.containedby"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.location.partially_containedby", "geography.river.origin"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["location.location.containedby", "location.location.containedby"], ["time.event.locations", "location.location.events"], ["base.locations.continents.countries_within", "location.location.partially_contains"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["location.location.partially_contains", "geography.river.basin_countries"], ["freebase.valuenotation.has_no_value", "freebase.valuenotation.has_no_value"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.country.languages_spoken", "language.human_language.main_country"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["periodicals.newspaper_circulation_area.newspapers", "book.newspaper.circulation_areas"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["language.human_language.main_country", "location.country.languages_spoken"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.tropical_cyclone.affected_areas"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["location.location.partially_contains", "geography.river.origin"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["location.location.events", "time.event.locations"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.country.languages_spoken", "location.country.official_language"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["government.form_of_government.countries", "location.country.form_of_government"], ["location.location.events", "location.location.events"], ["location.location.partially_contains", "geography.lake.basin_countries"], ["language.human_language.main_country", "language.human_language.countries_spoken_in"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["sports.multi_event_tournament.participating_countries", "sports.multi_event_tournament.participating_countries"], ["location.location.partially_contains", "location.location.partially_contains"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"], ["base.rivalries.rival.rivals", "base.rivalries.rivalry.rival"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["geography.river.basin_countries", "location.location.partially_contains"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["base.locations.continents.countries_within", "base.locations.countries.continent"], ["location.location.time_zones", "time.time_zone.locations_in_this_time_zone"], ["location.location.partially_containedby", "location.location.containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.locations.continents.countries_within", "base.locations.continents.countries_within"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.location.partially_containedby", "base.locations.countries.continent"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.partially_containedby", "base.schemastaging.disputed_location.claimed_by"], ["base.rivalries.rival.rivals", "base.rivalries.rival.rivals"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["location.location.partially_containedby", "base.locations.continents.countries_within"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.cyclone_affected_area.cyclones"], ["sports.multi_event_tournament.participating_countries", "sports.sport_country.multi_event_tournaments_participated_in"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["time.event.locations", "time.event.locations"], ["location.location.partially_contains", "location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries share a border with russia [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>"], "scores": [-0.11417493224143982, -0.13592025637626648], "norm_scores": [0.50543612241745, 0.49456384778022766]}}
{"id": "WebQTest-284", "question": "where did the latin language originate from", "prediction": [["language.human_language.language_family"], ["base.rosetta.languoid.parent"]], "ground_paths": [["language.human_language.language_family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the latin language originate from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>"], "scores": [-0.020912980660796165, -0.13473565876483917], "norm_scores": [0.5284249782562256, 0.4715750217437744]}}
{"id": "WebQTest-286", "question": "what school did karl benz go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did karl benz go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.0545736625790596, -0.07021847367286682], "norm_scores": [0.5039111375808716, 0.4960888624191284]}}
{"id": "WebQTest-287", "question": "what movies has carmen electra been in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["film.film_character.portrayed_in_films", "film.film.starring"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.person_or_entity_appearing_in_film.films", "film.personal_film_appearance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["film.actor.film", "film.film.starring"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies has carmen electra been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>"], "scores": [-0.07924593985080719, -0.10545265674591064], "norm_scores": [0.5065513253211975, 0.4934487044811249]}}
{"id": "WebQTest-289", "question": "when do world war ii end", "prediction": [["time.event.included_in_event"], ["time.event.includes_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when do world war ii end [/INST]", "raw_output": {"paths": ["<PATH> time.event.included_in_event </PATH>", "<PATH> time.event.includes_event </PATH>"], "scores": [-0.04017001762986183, -0.17877927422523499], "norm_scores": [0.5345969796180725, 0.4654030501842499]}}
{"id": "WebQTest-291", "question": "where did morgan freeman graduate", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did morgan freeman graduate [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.036090388894081116, -0.04252680763602257], "norm_scores": [0.5016090869903564, 0.49839091300964355]}}
{"id": "WebQTest-294", "question": "who was carolus linnaeus", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was carolus linnaeus [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.005054891109466553, -0.33774319291114807], "norm_scores": [0.582413375377655, 0.41758668422698975]}}
{"id": "WebQTest-295", "question": "what type of government does germany have now", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does germany have now [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>"], "scores": [-0.03935203328728676, -0.050785280764102936], "norm_scores": [0.5028582811355591, 0.4971417188644409]}}
{"id": "WebQTest-296", "question": "what kind of government does libya have today", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does libya have today [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.020557677373290062, -0.08747842162847519], "norm_scores": [0.5167239308357239, 0.48327603936195374]}}
{"id": "WebQTest-297", "question": "what time zone is anaheim california", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is anaheim california [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-5.211181996855885e-05, -0.9141145944595337], "norm_scores": [0.7138307690620422, 0.28616926074028015]}}
{"id": "WebQTest-298", "question": "what year did the orioles go to the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "ground_paths": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the orioles go to the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>"], "scores": [-0.025433043017983437, -0.07685957103967667], "norm_scores": [0.5128538012504578, 0.48714616894721985]}}
{"id": "WebQTest-300", "question": "who is sir francis bacon", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is sir francis bacon [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0005970124038867652, -0.5588482022285461], "norm_scores": [0.6360477805137634, 0.3639521896839142]}}
{"id": "WebQTest-301", "question": "what were amelia earhart 's achievements", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["fictional_universe.fictional_character.occupation"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what were amelia earhart 's achievements [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.03660871461033821, -0.17040826380252838], "norm_scores": [0.5334000587463379, 0.4665999412536621]}}
{"id": "WebQTest-302", "question": "what are the official colors of the dallas cowboys", "prediction": [["sports.sports_team.colors"], ["sports.sports_team_colors.team"]], "ground_paths": [["sports.sports_team.colors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the official colors of the dallas cowboys [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.colors </PATH>", "<PATH> sports.sports_team_colors.team </PATH>"], "scores": [-0.0005181223968975246, -0.48997926712036133], "norm_scores": [0.6199795007705688, 0.38002052903175354]}}
{"id": "WebQTest-303", "question": "where did margaret hoover go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did margaret hoover go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.03280024975538254, -0.07014171034097672], "norm_scores": [0.5093342661857605, 0.4906657338142395]}}
{"id": "WebQTest-304", "question": "what is jamie oliver children names", "prediction": [["people.person.children"], ["people.person.parents"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is jamie oliver children names [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.07321583479642868, -0.09111937880516052], "norm_scores": [0.5044757723808289, 0.49552422761917114]}}
{"id": "WebQTest-305", "question": "where does bradley walsh live", "prediction": [["people.person.places_lived", "people.place_lived.location"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does bradley walsh live [/INST]", "raw_output": {"paths": ["<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.05292249843478203, -0.054721247404813766], "norm_scores": [0.5004497170448303, 0.49955031275749207]}}
{"id": "WebQTest-306", "question": "who is the head coach of inter milan", "prediction": [["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"]], "ground_paths": [["soccer.football_team_management_tenure.team", "soccer.football_team_manager.team"], ["base.schemastaging.sports_team_manager_tenure.team", "base.schemastaging.sports_team_manager_tenure.manager"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the head coach of inter milan [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach.teams_coached </PATH>"], "scores": [-0.0442294143140316, -0.05540503188967705], "norm_scores": [0.5027938485145569, 0.4972061216831207]}}
{"id": "WebQTest-308", "question": "who did reese witherspoon get married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["people.person.profession", "people.person.profession"], ["people.person.parents", "people.person.children"], ["people.person.spouse_s", "people.marriage.spouse"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did reese witherspoon get married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.04294614493846893, -0.11028214544057846], "norm_scores": [0.5168276429176331, 0.48317238688468933]}}
{"id": "WebQTest-310", "question": "what kind of money should i take to costa rica", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money should i take to costa rica [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.036367516964673996, -0.1046382412314415], "norm_scores": [0.5170610547065735, 0.4829389452934265]}}
{"id": "WebQTest-311", "question": "where is the ufc headquarters", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["sports.sports_league.sport", "sports.sports_team.location"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["common.topic.subject_of", "people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the ufc headquarters [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> sports.sports_league.sport <SEP> sports.sports_team.location </PATH>"], "scores": [-0.028090113773941994, -0.10159529000520706], "norm_scores": [0.5183680653572083, 0.4816319942474365]}}
{"id": "WebQTest-312", "question": "who plays captain kirk in star trek", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.quotations", "media_common.quotation.author"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays captain kirk in star trek [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.0761718899011612, -0.0803268700838089], "norm_scores": [0.5010387301445007, 0.4989612400531769]}}
{"id": "WebQTest-313", "question": "who does albert pujols play for 2012", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does albert pujols play for 2012 [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.027126610279083252, -0.0675773099064827], "norm_scores": [0.5101113319396973, 0.4898887276649475]}}
{"id": "WebQTest-314", "question": "what kind of government is sweden", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government is sweden [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.044662583619356155, -0.045311544090509415], "norm_scores": [0.5001622438430786, 0.4998377561569214]}}
{"id": "WebQTest-315", "question": "where was country singer george jones born", "prediction": [["location.location.people_born_here"], ["music.artist.origin"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was country singer george jones born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.04762182757258415, -0.11063778400421143], "norm_scores": [0.5157487392425537, 0.4842512309551239]}}
{"id": "WebQTest-316", "question": "what did shakespeare become famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["fictional_universe.fictional_character.occupation"], ["fictional_universe.character_occupation.characters_with_this_occupation"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did shakespeare become famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.029321223497390747, -0.1636265069246292], "norm_scores": [0.5335259437561035, 0.4664740562438965]}}
{"id": "WebQTest-317", "question": "where does russia import from", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does russia import from [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>"], "scores": [-0.05588303878903389, -0.05796484276652336], "norm_scores": [0.5005204677581787, 0.4994795620441437]}}
{"id": "WebQTest-318", "question": "what is the money of switzerland called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the money of switzerland called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.021799415349960327, -0.13662280142307281], "norm_scores": [0.5286743640899658, 0.47132566571235657]}}
{"id": "WebQTest-321", "question": "what do people in australia speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people in australia speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.030016480013728142, -0.08102010935544968], "norm_scores": [0.5127481818199158, 0.4872518479824066]}}
{"id": "WebQTest-322", "question": "what has ian somerhalder acted in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["film.film.produced_by"], ["film.performance.actor", "film.performance.film"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.film.starring"], ["film.producer.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has ian somerhalder acted in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.07062637805938721, -0.12163510173559189], "norm_scores": [0.512749433517456, 0.48725056648254395]}}
{"id": "WebQTest-323", "question": "what movies did ron howard director", "prediction": [["film.film.directed_by"], ["film.film.produced_by"]], "ground_paths": [["film.film.produced_by"], ["film.producer.film"], ["film.film.directed_by"], ["film.director.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did ron howard director [/INST]", "raw_output": {"paths": ["<PATH> film.film.directed_by </PATH>", "<PATH> film.film.produced_by </PATH>"], "scores": [-0.10913321375846863, -0.1534503549337387], "norm_scores": [0.5110774636268616, 0.4889225661754608]}}
{"id": "WebQTest-326", "question": "what did gerald r ford die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did gerald r ford die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.0041076382622122765, -0.19510574638843536], "norm_scores": [0.5476048588752747, 0.45239508152008057]}}
{"id": "WebQTest-327", "question": "what two continents is turkey on", "prediction": [["base.locations.countries.continent"], ["location.location.containedby"]], "ground_paths": [["location.location.partially_contains"], ["base.locations.countries.continent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what two continents is turkey on [/INST]", "raw_output": {"paths": ["<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.05212140455842018, -0.15180331468582153], "norm_scores": [0.5248998403549194, 0.4751001298427582]}}
{"id": "WebQTest-328", "question": "where did kevin love go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kevin love go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.05213441327214241, -0.06017407029867172], "norm_scores": [0.502009928226471, 0.49799010157585144]}}
{"id": "WebQTest-329", "question": "what type of government was formed when italy unified", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government was formed when italy unified [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.037432655692100525, -0.0543072409927845], "norm_scores": [0.504218578338623, 0.49578145146369934]}}
{"id": "WebQTest-331", "question": "where was the temple of karnak built", "prediction": [["location.location.containedby"], ["architecture.building.building_complex"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the temple of karnak built [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> architecture.building.building_complex </PATH>"], "scores": [-0.009402698837220669, -0.34579604864120483], "norm_scores": [0.5833141207695007, 0.4166858196258545]}}
{"id": "WebQTest-333", "question": "what movies did morgan freeman star in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["film.person_or_entity_appearing_in_film.films", "film.personal_film_appearance.film"], ["tv.tv_actor.guest_roles", "tv.tv_guest_role.episodes_appeared_in"], ["film.film.executive_produced_by"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["film.producer.film"], ["film.producer.films_executive_produced"], ["people.person.nationality", "film.film.country"], ["film.personal_film_appearance.person", "film.personal_film_appearance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["common.topic.notable_types", "common.topic.notable_types"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["theater.theater_actor.theater_roles", "theater.theater_role.play"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did morgan freeman star in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.0698833018541336, -0.1428196281194687], "norm_scores": [0.5182259678840637, 0.4817740023136139]}}
{"id": "WebQTest-334", "question": "who did john kennedy have affairs with", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["music.recording.featured_artists", "music.recording.artist"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did john kennedy have affairs with [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.05052274838089943, -0.1250743716955185], "norm_scores": [0.518629252910614, 0.4813707172870636]}}
{"id": "WebQTest-335", "question": "what type of money to take to cuba", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of money to take to cuba [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.05751366168260574, -0.07000864297151566], "norm_scores": [0.5031237006187439, 0.4968762993812561]}}
{"id": "WebQTest-336", "question": "what currency should you take to morocco", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should you take to morocco [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.05351199582219124, -0.0755905732512474], "norm_scores": [0.505519449710846, 0.49448058009147644]}}
{"id": "WebQTest-337", "question": "who is mary mcleod bethune for kids", "prediction": [["people.person.profession"], ["influence.influence_node.influenced"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is mary mcleod bethune for kids [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.0032892515882849693, -0.33044400811195374], "norm_scores": [0.581066906452179, 0.41893309354782104]}}
{"id": "WebQTest-338", "question": "what do they call money in japan", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do they call money in japan [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.059081703424453735, -0.09236852824687958], "norm_scores": [0.5083209276199341, 0.4916790723800659]}}
{"id": "WebQTest-339", "question": "what nationality is mother teresa", "prediction": [["people.person.nationality"], ["people.person.ethnicity"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what nationality is mother teresa [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.ethnicity </PATH>"], "scores": [-0.030145313590765, -0.18641608953475952], "norm_scores": [0.5389883518218994, 0.4610115885734558]}}
{"id": "WebQTest-340", "question": "who is richard pryor son", "prediction": [["people.person.children"], ["people.person.parents"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is richard pryor son [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.021136581897735596, -0.21496453881263733], "norm_scores": [0.548305869102478, 0.45169416069984436]}}
{"id": "WebQTest-341", "question": "what countries share borders with spain", "prediction": [["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.partially_containedby", "location.location.partially_contains"]], "ground_paths": [["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.location.primarily_containedby", "location.location.contains_major_portion_of"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["biology.animal_breed.place_of_origin", "biology.animal_breed.place_of_origin"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["location.country.form_of_government", "location.country.form_of_government"], ["location.location.time_zones", "location.location.time_zones"], ["freebase.valuenotation.has_no_value", "freebase.valuenotation.has_no_value"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.containedby", "location.location.partially_containedby"], ["location.location.partially_containedby", "location.location.partially_contains"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["royalty.royal_line.kingdom_s_ruled", "people.family.country"], ["location.location.primarily_containedby", "location.location.containedby"], ["location.location.partially_contains", "location.location.contains"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["finance.currency.countries_used", "location.country.currency_used"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["government.form_of_government.countries", "location.country.form_of_government"], ["location.country.currency_formerly_used", "finance.currency.countries_formerly_used"], ["biology.breed_origin.breeds_originating_here", "biology.breed_origin.breeds_originating_here"], ["location.location.primarily_containedby", "base.locations.continents.countries_within"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["geography.river.basin_countries", "location.location.partially_contains"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["freebase.valuenotation.has_value", "freebase.valuenotation.is_reviewed"], ["location.location.partially_contains", "location.location.primarily_containedby"], ["location.location.partially_containedby", "location.location.containedby"], ["film.film.country", "film.film.country"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["location.location.containedby", "location.location.contains_major_portion_of"], ["location.country.internet_tld", "location.country.internet_tld"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["location.country.form_of_government", "government.form_of_government.countries"], ["location.location.primarily_containedby", "location.location.primarily_containedby"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["organization.organization.founders", "organization.organization.founders"], ["location.location.partially_contains", "location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries share borders with spain [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>"], "scores": [-0.11036613583564758, -0.13389407098293304], "norm_scores": [0.5058817267417908, 0.49411827325820923]}}
{"id": "WebQTest-342", "question": "what countries have english as their official language", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries have english as their official language [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.07417548447847366, -0.08386404067277908], "norm_scores": [0.5024220943450928, 0.49757787585258484]}}
{"id": "WebQTest-343", "question": "what international organizations is china part of", "prediction": [["organization.organization_member.member_of", "organization.organization_membership.organization"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [["organization.organization_membership.member", "organization.membership_organization.members"], ["organization.organization_membership.member", "organization.organization_membership.organization"], ["organization.organization_founder.organizations_founded"], ["organization.organization.founders"], ["organization.organization_member.member_of", "organization.organization_membership.organization"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what international organizations is china part of [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_member.member_of <SEP> organization.organization_membership.organization </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.06559965759515762, -0.0852063000202179], "norm_scores": [0.5049014687538147, 0.4950984716415405]}}
{"id": "WebQTest-344", "question": "what county is west st paul in", "prediction": [["location.location.containedby"], ["location.us_county.county_seat"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is west st paul in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.county_seat </PATH>"], "scores": [-0.06868308782577515, -0.09674034267663956], "norm_scores": [0.5070138573646545, 0.49298611283302307]}}
{"id": "WebQTest-346", "question": "who does donnie wahlberg play in the sixth sense", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["film.performance.film", "film.performance.character"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does donnie wahlberg play in the sixth sense [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.06066795438528061, -0.14654703438282013], "norm_scores": [0.5214565396308899, 0.47854340076446533]}}
{"id": "WebQTest-347", "question": "what did baron de montesquie influence", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "ground_paths": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did baron de montesquie influence [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.018131520599126816, -0.07781773805618286], "norm_scores": [0.5149171352386475, 0.4850828945636749]}}
{"id": "WebQTest-349", "question": "who did carlos boozer play for", "prediction": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"]], "ground_paths": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did carlos boozer play for [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>"], "scores": [-0.02473539486527443, -0.04101154953241348], "norm_scores": [0.5040689706802368, 0.49593105912208557]}}
{"id": "WebQTest-351", "question": "who owns the portland press herald", "prediction": [["periodicals.newspaper_circulation_area.newspapers", "business.business_operation.industry"], ["book.newspaper.owner"]], "ground_paths": [["book.newspaper_owner.newspapers_owned"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who owns the portland press herald [/INST]", "raw_output": {"paths": ["<PATH> periodicals.newspaper_circulation_area.newspapers <SEP> business.business_operation.industry </PATH>", "<PATH> book.newspaper.owner </PATH>"], "scores": [-0.08909622579813004, -0.12008985131978989], "norm_scores": [0.5077477693557739, 0.4922522306442261]}}
{"id": "WebQTest-352", "question": "what did albert speer design", "prediction": [["architecture.architect.structures_designed"], ["architecture.structure.architect"]], "ground_paths": [["architecture.structure.architect"], ["architecture.architect.structures_designed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did albert speer design [/INST]", "raw_output": {"paths": ["<PATH> architecture.architect.structures_designed </PATH>", "<PATH> architecture.structure.architect </PATH>"], "scores": [-0.0025517037138342857, -0.4684155285358429], "norm_scores": [0.6144042611122131, 0.3855956792831421]}}
{"id": "WebQTest-354", "question": "what art movement did leonardo da vinci belong to", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_artist.associated_periods_or_movements"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what art movement did leonardo da vinci belong to [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>"], "scores": [-0.017051951959729195, -0.0776604413986206], "norm_scores": [0.5151475071907043, 0.48485249280929565]}}
{"id": "WebQTest-356", "question": "what airport is near arlington tx", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"]], "ground_paths": [["aviation.airport.serves"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what airport is near arlington tx [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>"], "scores": [-0.0499335415661335, -0.10547146201133728], "norm_scores": [0.5138809084892273, 0.4861190915107727]}}
{"id": "WebQTest-359", "question": "what team does heskey play for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does heskey play for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.07163020223379135, -0.08801547437906265], "norm_scores": [0.5040962100028992, 0.49590378999710083]}}
{"id": "WebQTest-360", "question": "where did emperor hirohito live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did emperor hirohito live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.07393765449523926, -0.08659534901380539], "norm_scores": [0.5031643509864807, 0.4968356192111969]}}
{"id": "WebQTest-361", "question": "what to do and see in vienna austria", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "ground_paths": [["location.location.containedby"], ["base.arthist.int_zm_nyek.helynev"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do and see in vienna austria [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>"], "scores": [-0.04476993903517723, -0.06635473668575287], "norm_scores": [0.5053960084915161, 0.4946039915084839]}}
{"id": "WebQTest-362", "question": "when did conflict start in ireland", "prediction": [["time.event.locations"], ["location.location.events"]], "ground_paths": [["time.event.locations"], ["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did conflict start in ireland [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.028351567685604095, -0.2995520532131195], "norm_scores": [0.567387580871582, 0.43261241912841797]}}
{"id": "WebQTest-365", "question": "what is there to do in peoria illinois", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["location.location.contains"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is there to do in peoria illinois [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.036049723625183105, -0.058005571365356445], "norm_scores": [0.5054887533187866, 0.49451127648353577]}}
{"id": "WebQTest-366", "question": "what language do british speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do british speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.056408390402793884, -0.08893701434135437], "norm_scores": [0.5081314444541931, 0.4918685853481293]}}
{"id": "WebQTest-367", "question": "what makes elvis presley famous", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["fictional_universe.fictional_character.occupation"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what makes elvis presley famous [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.02468203753232956, -0.20060302317142487], "norm_scores": [0.5438671708106995, 0.45613282918930054]}}
{"id": "WebQTest-370", "question": "what language do people speak in the netherlands", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people speak in the netherlands [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.050428539514541626, -0.07843848317861557], "norm_scores": [0.5070019960403442, 0.4929979741573334]}}
{"id": "WebQTest-371", "question": "what are the major languages spoken in greece", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the major languages spoken in greece [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.07064155489206314, -0.07864823192358017], "norm_scores": [0.5020016431808472, 0.49799832701683044]}}
{"id": "WebQTest-374", "question": "what type of artist is henri matisse", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_art_form.artists"]], "ground_paths": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_art_form.artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of artist is henri matisse [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>"], "scores": [-0.002439339878037572, -0.28421714901924133], "norm_scores": [0.5699820518493652, 0.43001800775527954]}}
{"id": "WebQTest-375", "question": "what was the first book charles dickens wrote", "prediction": [["book.written_work.author"], ["fictional_universe.fictional_character_creator.fictional_characters_created"]], "ground_paths": [["fictional_universe.fictional_character_creator.fictional_characters_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the first book charles dickens wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>"], "scores": [-0.016139496117830276, -0.0947440043091774], "norm_scores": [0.5196410417556763, 0.4803589880466461]}}
{"id": "WebQTest-376", "question": "what books did agatha christie wrote", "prediction": [["book.book_edition.author_editor"], ["book.written_work.author"]], "ground_paths": [["film.film.story_by"], ["film.film_story_contributor.film_story_credits"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what books did agatha christie wrote [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.06460949778556824, -0.09808041155338287], "norm_scores": [0.5083669424057007, 0.4916330575942993]}}
{"id": "WebQTest-377", "question": "what school did michael jordan attend", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did michael jordan attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.056353859603405, -0.08677545189857483], "norm_scores": [0.5076047778129578, 0.49239516258239746]}}
{"id": "WebQTest-379", "question": "what year does hitler die", "prediction": [["people.deceased_person.cause_of_death"], ["people.deceased_person.place_of_death"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year does hitler die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>"], "scores": [-0.04151491820812225, -0.0715482085943222], "norm_scores": [0.5075077414512634, 0.4924922585487366]}}
{"id": "WebQTest-380", "question": "where is mount st helens volcano", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mount st helens volcano [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.039819277822971344, -0.18063941597938538], "norm_scores": [0.535146951675415, 0.4648530185222626]}}
{"id": "WebQTest-381", "question": "what shows are shot in new york", "prediction": [["film.film.featured_film_locations"], ["tv.tv_program.filming_locations"]], "ground_paths": [["film.film.featured_film_locations"], ["tv.tv_location.tv_shows_filmed_here"], ["tv.tv_program.filming_locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what shows are shot in new york [/INST]", "raw_output": {"paths": ["<PATH> film.film.featured_film_locations </PATH>", "<PATH> tv.tv_program.filming_locations </PATH>"], "scores": [-0.06302638351917267, -0.08827449381351471], "norm_scores": [0.5063117146492004, 0.49368831515312195]}}
{"id": "WebQTest-382", "question": "what book did tupac write", "prediction": [["book.book_edition.author_editor"], ["book.written_work.author"]], "ground_paths": [["book.book_edition.author_editor"], ["book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what book did tupac write [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.06031415984034538, -0.09562429785728455], "norm_scores": [0.5088266134262085, 0.4911733567714691]}}
{"id": "WebQTest-383", "question": "what language do chinese people write in", "prediction": [["language.human_language.main_country"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.writing_system"], ["language.language_writing_system.languages"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do chinese people write in [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.03757503628730774, -0.15262973308563232], "norm_scores": [0.5287320017814636, 0.4712679982185364]}}
{"id": "WebQTest-384", "question": "what was robert burns famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was robert burns famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.033845894038677216, -0.13222362101078033], "norm_scores": [0.5245746374130249, 0.4754254221916199]}}
{"id": "WebQTest-385", "question": "when did william mckinley died", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.cause_of_death"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did william mckinley died [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.013182969763875008, -0.12439528852701187], "norm_scores": [0.527774453163147, 0.472225546836853]}}
{"id": "WebQTest-386", "question": "how deep is lake merritt oakland", "prediction": [["geography.lake.basin_countries"], ["location.location.containedby"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how deep is lake merritt oakland [/INST]", "raw_output": {"paths": ["<PATH> geography.lake.basin_countries </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.04737846925854683, -0.20808275043964386], "norm_scores": [0.5400897860527039, 0.45991015434265137]}}
{"id": "WebQTest-388", "question": "what are the important holidays of islam", "prediction": [["time.holiday.featured_in_religions"], ["religion.religion.holidays"]], "ground_paths": [["religion.religion.holidays"], ["time.holiday.featured_in_religions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the important holidays of islam [/INST]", "raw_output": {"paths": ["<PATH> time.holiday.featured_in_religions </PATH>", "<PATH> religion.religion.holidays </PATH>"], "scores": [-0.029971124604344368, -0.0698830708861351], "norm_scores": [0.5099766254425049, 0.49002331495285034]}}
{"id": "WebQTest-389", "question": "what type of music did claude debussy play", "prediction": [["music.artist.genre"], ["music.genre.artists"]], "ground_paths": [["music.genre.artists"], ["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did claude debussy play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>"], "scores": [-0.02252577804028988, -0.16842542588710785], "norm_scores": [0.5364103317260742, 0.4635896682739258]}}
{"id": "WebQTest-390", "question": "when was the last time the toronto maple leafs were in the stanley cup finals", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the toronto maple leafs were in the stanley cup finals [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.05147109553217888, -0.062260475009679794], "norm_scores": [0.5026973485946655, 0.49730268120765686]}}
{"id": "WebQTest-392", "question": "what team is hank baskett on 2010", "prediction": [["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"]], "ground_paths": [["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is hank baskett on 2010 [/INST]", "raw_output": {"paths": ["<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>"], "scores": [-0.03867824375629425, -0.051798611879348755], "norm_scores": [0.5032800436019897, 0.49671995639801025]}}
{"id": "WebQTest-393", "question": "what are the two official languages of paraguay", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the two official languages of paraguay [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.05591322481632233, -0.1296599954366684], "norm_scores": [0.5184283256530762, 0.48157164454460144]}}
{"id": "WebQTest-395", "question": "what state is the steelers from", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.location"]], "ground_paths": [["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state is the steelers from [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>"], "scores": [-0.015176535584032536, -0.13805200159549713], "norm_scores": [0.5306802988052368, 0.46931973099708557]}}
{"id": "WebQTest-396", "question": "what college did sir isaac newton go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did sir isaac newton go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.03225765377283096, -0.06501909345388412], "norm_scores": [0.5081896185874939, 0.4918103814125061]}}
{"id": "WebQTest-397", "question": "what are the school colors for harvard university", "prediction": [["education.educational_institution.colors"], ["education.school_mascot.school"]], "ground_paths": [["education.educational_institution.colors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the school colors for harvard university [/INST]", "raw_output": {"paths": ["<PATH> education.educational_institution.colors </PATH>", "<PATH> education.school_mascot.school </PATH>"], "scores": [-0.012841093353927135, -0.1936125010251999], "norm_scores": [0.5450701713562012, 0.45492979884147644]}}
{"id": "WebQTest-398", "question": "where is the university of the rockies located", "prediction": [["location.location.containedby"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the university of the rockies located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.014661568216979504, -0.1687784045934677], "norm_scores": [0.5384531617164612, 0.4615468680858612]}}
{"id": "WebQTest-399", "question": "what disease does robin roberts have", "prediction": [["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.notable_people_with_this_condition"]], "ground_paths": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what disease does robin roberts have [/INST]", "raw_output": {"paths": ["<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>"], "scores": [-0.03281835839152336, -0.0365920253098011], "norm_scores": [0.5009434223175049, 0.4990565776824951]}}
{"id": "WebQTest-400", "question": "what was the capital city of the east roman empire", "prediction": [["location.location.containedby"], ["location.country.capital"]], "ground_paths": [["location.country.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the capital city of the east roman empire [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.country.capital </PATH>"], "scores": [-0.037149328738451004, -0.17493906617164612], "norm_scores": [0.5343930125236511, 0.4656069576740265]}}
{"id": "WebQTest-401", "question": "who plays edward scissorhands", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays edward scissorhands [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.06252793967723846, -0.21141929924488068], "norm_scores": [0.5371542572975159, 0.4628457725048065]}}
{"id": "WebQTest-402", "question": "what does joey jordison play in slipknot", "prediction": [["music.group_membership.group", "music.group_membership.role"], ["music.group_member.instruments_played"]], "ground_paths": [["music.group_member.instruments_played"], ["music.group_membership.group", "music.group_membership.role"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does joey jordison play in slipknot [/INST]", "raw_output": {"paths": ["<PATH> music.group_membership.group <SEP> music.group_membership.role </PATH>", "<PATH> music.group_member.instruments_played </PATH>"], "scores": [-0.05626009404659271, -0.07803139090538025], "norm_scores": [0.5054426193237305, 0.49455738067626953]}}
{"id": "WebQTest-403", "question": "what kind of language does turkey speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language does turkey speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.02472665160894394, -0.13601142168045044], "norm_scores": [0.5277925133705139, 0.4722074866294861]}}
{"id": "WebQTest-404", "question": "what was lebron james first team", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"]], "ground_paths": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was lebron james first team [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>"], "scores": [-0.03616511449217796, -0.042661018669605255], "norm_scores": [0.5016239881515503, 0.4983760416507721]}}
{"id": "WebQTest-405", "question": "who fought the battle of gettysburg", "prediction": [["base.culturalevent.event.entity_involved"], ["military.military_conflict.military_personnel_involved"]], "ground_paths": [["military.military_person.participated_in_conflicts", "people.person.nationality"], ["military.military_person.participated_in_conflicts"], ["military.military_command.military_conflict", "military.military_combatant.military_commanders"], ["location.location.events", "location.administrative_division.second_level_division_of"], ["time.event.included_in_event", "time.event.locations"], ["military.military_conflict.combatants", "military.military_combatant_group.combatants"], ["book.written_work.subjects", "book.written_work.subjects"], ["military.military_conflict.military_personnel_involved", "people.person.nationality"], ["base.culturalevent.event.entity_involved"], ["location.location.events", "location.administrative_division.first_level_division_of"], ["base.culturalevent.event.entity_involved", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who fought the battle of gettysburg [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>"], "scores": [-0.023977555334568024, -0.07496710121631622], "norm_scores": [0.512744665145874, 0.48725539445877075]}}
{"id": "WebQTest-406", "question": "who is the coach of the sf giants", "prediction": [["baseball.baseball_team.current_coaches"], ["baseball.baseball_team.current_manager"]], "ground_paths": [["baseball.baseball_team.current_coaches", "baseball.current_coaching_tenure.baseball_coach"], ["baseball.current_coaching_tenure.baseball_team", "baseball.current_coaching_tenure.baseball_coach"], ["baseball.current_coaching_tenure.baseball_team", "baseball.baseball_coach.current_team_coaching"], ["baseball.baseball_team.current_coaches", "baseball.baseball_coach.current_team_coaching"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the coach of the sf giants [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_team.current_coaches </PATH>", "<PATH> baseball.baseball_team.current_manager </PATH>"], "scores": [-0.04238327220082283, -0.11922930181026459], "norm_scores": [0.5192020535469055, 0.4807979464530945]}}
{"id": "WebQTest-408", "question": "who was the first president of the afl", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the first president of the afl [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.0029501060489565134, -0.28644806146621704], "norm_scores": [0.5704035758972168, 0.4295963943004608]}}
{"id": "WebQTest-409", "question": "where did giuliana rancic grow up", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did giuliana rancic grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.014981870539486408, -0.22344526648521423], "norm_scores": [0.551927924156189, 0.44807207584381104]}}
{"id": "WebQTest-410", "question": "what are the major cities in ukraine", "prediction": [["location.country.capital"], ["location.location.containedby"]], "ground_paths": [["location.country.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the major cities in ukraine [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.11724821478128433, -0.11966376006603241], "norm_scores": [0.5006039142608643, 0.49939611554145813]}}
{"id": "WebQTest-414", "question": "who does brian dawkins play for 2011", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does brian dawkins play for 2011 [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.03464065119624138, -0.03601890802383423], "norm_scores": [0.5003445744514465, 0.49965545535087585]}}
{"id": "WebQTest-415", "question": "what movies did tupac star in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["people.person.languages", "film.film.language"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["music.featured_artist.recordings", "music.recording.releases"], ["music.recording.artist", "music.recording.releases"], ["music.producer.releases_produced"], ["film.actor.film", "film.film.starring"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did tupac star in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>"], "scores": [-0.07797329127788544, -0.09049463272094727], "norm_scores": [0.503130316734314, 0.4968697428703308]}}
{"id": "WebQTest-416", "question": "what form of currency does china have", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of currency does china have [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.0573192723095417, -0.06969263404607773], "norm_scores": [0.5030933022499084, 0.49690672755241394]}}
{"id": "WebQTest-417", "question": "who plays kenneth", "prediction": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays kenneth [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>"], "scores": [-0.05374262109398842, -0.05609505623579025], "norm_scores": [0.5005881190299988, 0.4994118809700012]}}
{"id": "WebQTest-418", "question": "what league are the indianapolis colts in", "prediction": [["american_football.football_team.division"], ["american_football.football_team.current_division"]], "ground_paths": [["american_football.football_team.conference", "american_football.football_conference.league"], ["sports.sports_team.sport", "sports.sports_league.sport"], ["american_football.football_team.conference"], ["american_football.football_team.division"], ["sports.sports_league_participation.team", "sports.sports_league_participation.league"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what league are the indianapolis colts in [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_team.division </PATH>", "<PATH> american_football.football_team.current_division </PATH>"], "scores": [-0.07106486707925797, -0.1824471652507782], "norm_scores": [0.5278167724609375, 0.4721831977367401]}}
{"id": "WebQTest-419", "question": "what country did magellan represent", "prediction": [["people.person.nationality"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did magellan represent [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.016769204288721085, -0.10289951413869858], "norm_scores": [0.5215192437171936, 0.478480726480484]}}
{"id": "WebQTest-420", "question": "where is perpignan located", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.capital"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is perpignan located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>"], "scores": [-0.038341596722602844, -0.08273331820964813], "norm_scores": [0.5110961198806763, 0.48890388011932373]}}
{"id": "WebQTest-421", "question": "who did kim richards marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "ground_paths": [["people.person.parents", "people.person.children"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.profession", "people.person.profession"], ["people.person.parents", "people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did kim richards marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>"], "scores": [-0.041411105543375015, -0.06184740737080574], "norm_scores": [0.5051088929176331, 0.49489107728004456]}}
{"id": "WebQTest-422", "question": "what happened in bosnia in the 90s", "prediction": [["time.event.locations"], ["location.location.events"]], "ground_paths": [["location.location.events"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what happened in bosnia in the 90s [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.037956684827804565, -0.2332269549369812], "norm_scores": [0.5486630201339722, 0.45133695006370544]}}
{"id": "WebQTest-423", "question": "when 's the last time the steelers won the superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when 's the last time the steelers won the superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.045160919427871704, -0.05242534354329109], "norm_scores": [0.5018160939216614, 0.49818387627601624]}}
{"id": "WebQTest-425", "question": "where english is spoken", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where english is spoken [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.04799504950642586, -0.10518380999565125], "norm_scores": [0.5142933130264282, 0.48570671677589417]}}
{"id": "WebQTest-426", "question": "where is giza pyramids located", "prediction": [["location.location.containedby"], ["architecture.building.building_complex"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is giza pyramids located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> architecture.building.building_complex </PATH>"], "scores": [-0.012833788990974426, -0.24255840480327606], "norm_scores": [0.557179868221283, 0.44282010197639465]}}
{"id": "WebQTest-427", "question": "what is the zip code for midland tx", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for midland tx [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0003494452976156026, -0.5807740688323975], "norm_scores": [0.6411650776863098, 0.3588348925113678]}}
{"id": "WebQTest-428", "question": "what states make up the midwest us", "prediction": [["location.location.contains"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what states make up the midwest us [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0725647360086441, -0.08304931223392487], "norm_scores": [0.5026211142539978, 0.4973788857460022]}}
{"id": "WebQTest-430", "question": "what instruments does justin bieber use", "prediction": [["music.artist.track_contributions", "music.track_contribution.role"], ["music.track_contribution.contributor", "music.track_contribution.role"]], "ground_paths": [["music.group_member.membership", "music.group_membership.role"], ["music.artist.track_contributions", "music.track_contribution.role"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what instruments does justin bieber use [/INST]", "raw_output": {"paths": ["<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>", "<PATH> music.track_contribution.contributor <SEP> music.track_contribution.role </PATH>"], "scores": [-0.013576352968811989, -0.11755581945180893], "norm_scores": [0.5259714722633362, 0.4740285277366638]}}
{"id": "WebQTest-431", "question": "what language does australians speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does australians speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.033438969403505325, -0.07874869555234909], "norm_scores": [0.5113255381584167, 0.488674521446228]}}
{"id": "WebQTest-432", "question": "who did annie oakley married", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.marriage.spouse"]], "ground_paths": [["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did annie oakley married [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>"], "scores": [-0.06270771473646164, -0.12014506012201309], "norm_scores": [0.5143554210662842, 0.4856446087360382]}}
{"id": "WebQTest-433", "question": "who has played lex luthor", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has played lex luthor [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.054240547120571136, -0.07196883112192154], "norm_scores": [0.504431962966919, 0.49556803703308105]}}
{"id": "WebQTest-434", "question": "where does the zambezi river start", "prediction": [["geography.river.origin"], ["location.location.containedby"]], "ground_paths": [["geography.river.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the zambezi river start [/INST]", "raw_output": {"paths": ["<PATH> geography.river.origin </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0020371407736092806, -0.5359687805175781], "norm_scores": [0.6303996443748474, 0.369600385427475]}}
{"id": "WebQTest-436", "question": "what is the nigeria time", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the nigeria time [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>"], "scores": [-0.000849239353556186, -0.26304835081100464], "norm_scores": [0.565176784992218, 0.4348231852054596]}}
{"id": "WebQTest-437", "question": "where did they find jenni rivera 's body", "prediction": [["people.deceased_person.place_of_death"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did they find jenni rivera 's body [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.0018677667248994112, -0.2986471354961395], "norm_scores": [0.5736550092697144, 0.42634499073028564]}}
{"id": "WebQTest-438", "question": "what university did romney graduated from", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.religion", "religion.religion.organizations"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what university did romney graduated from [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.040707994252443314, -0.07080922275781631], "norm_scores": [0.5075247287750244, 0.4924752712249756]}}
{"id": "WebQTest-439", "question": "where is the time zone line in south dakota", "prediction": [["location.location.time_zones"], ["location.location.containedby"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the time zone line in south dakota [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.009726416319608688, -0.2577621340751648], "norm_scores": [0.5616929531097412, 0.4383070170879364]}}
{"id": "WebQTest-440", "question": "where is jason mraz from", "prediction": [["music.artist.origin"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"], ["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is jason mraz from [/INST]", "raw_output": {"paths": ["<PATH> music.artist.origin </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.026742994785308838, -0.17260251939296722], "norm_scores": [0.5364003777503967, 0.46359965205192566]}}
{"id": "WebQTest-441", "question": "what killed john bonham", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what killed john bonham [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.005869478918612003, -0.16943350434303284], "norm_scores": [0.5408000946044922, 0.4591999053955078]}}
{"id": "WebQTest-442", "question": "what industry does walmart operate in", "prediction": [["business.business_operation.industry"], ["business.industry.companies"]], "ground_paths": [["business.industry.companies"], ["organization.organization.sectors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what industry does walmart operate in [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry </PATH>", "<PATH> business.industry.companies </PATH>"], "scores": [-0.023236937820911407, -0.21371996402740479], "norm_scores": [0.5474773049354553, 0.4525226950645447]}}
{"id": "WebQTest-443", "question": "what county is texarkana arkansas in", "prediction": [["location.location.containedby"], ["location.us_county.county_seat"]], "ground_paths": [["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is texarkana arkansas in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.county_seat </PATH>"], "scores": [-0.0445767305791378, -0.08337753266096115], "norm_scores": [0.5096989274024963, 0.4903010129928589]}}
{"id": "WebQTest-446", "question": "what music period did beethoven live in", "prediction": [["music.artist.genre"], ["music.artist.period"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what music period did beethoven live in [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.artist.period </PATH>"], "scores": [-0.00894166063517332, -0.3959653079509735], "norm_scores": [0.5955660343170166, 0.4044340252876282]}}
{"id": "WebQTest-447", "question": "what did laura ingalls wilder wrote", "prediction": [["book.written_work.author"], ["book.book_edition.author_editor"]], "ground_paths": [["book.literary_series.author_s"], ["book.book_edition.author_editor"], ["book.written_work.author"], ["book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did laura ingalls wilder wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.05659715458750725, -0.10983837395906448], "norm_scores": [0.5133071541786194, 0.4866928160190582]}}
{"id": "WebQTest-448", "question": "what type of government system does italy have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government system does italy have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.033158399164676666, -0.06035826355218887], "norm_scores": [0.506799578666687, 0.493200421333313]}}
{"id": "WebQTest-449", "question": "what did john irving wrote", "prediction": [["book.written_work.author"], ["book.author.works_written"]], "ground_paths": [["book.written_work.author"], ["book.author.works_written"], ["book.author.book_editions_published"], ["book.book_edition.author_editor"], ["film.film.story_by"], ["film.film_story_contributor.film_story_credits"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did john irving wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.07727207243442535, -0.13049212098121643], "norm_scores": [0.5133018493652344, 0.48669812083244324]}}
{"id": "WebQTest-450", "question": "what is the national flower of hawaii", "prediction": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.symbol_of_administrative_division.official_symbol_of"]], "ground_paths": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the national flower of hawaii [/INST]", "raw_output": {"paths": ["<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>"], "scores": [-0.021942967548966408, -0.04083773121237755], "norm_scores": [0.5047235488891602, 0.49527645111083984]}}
{"id": "WebQTest-451", "question": "what did alice paul accomplish", "prediction": [["base.activism.activist.area_of_activism"], ["base.activism.activism_issue.activists"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did alice paul accomplish [/INST]", "raw_output": {"paths": ["<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> base.activism.activism_issue.activists </PATH>"], "scores": [-0.019513430073857307, -0.10121481865644455], "norm_scores": [0.5204139947891235, 0.47958600521087646]}}
{"id": "WebQTest-453", "question": "what languages do they speak in costa rica", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what languages do they speak in costa rica [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.016182387247681618, -0.12897928059101105], "norm_scores": [0.5281693339347839, 0.4718306362628937]}}
{"id": "WebQTest-454", "question": "where did hank marvin come from", "prediction": [["people.person.place_of_birth"], ["music.artist.origin"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did hank marvin come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.048394348472356796, -0.14140591025352478], "norm_scores": [0.5232361555099487, 0.47676387429237366]}}
{"id": "WebQTest-455", "question": "when did chipper jones get drafted", "prediction": [["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.season"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did chipper jones get drafted [/INST]", "raw_output": {"paths": ["<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.season </PATH>"], "scores": [-0.054784152656793594, -0.057790085673332214], "norm_scores": [0.5007514953613281, 0.4992485046386719]}}
{"id": "WebQTest-456", "question": "what part did winona ryder play in star trek", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.performance.character"]], "ground_paths": [["film.actor.film", "film.performance.character"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character.appears_in_these_fictional_universes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part did winona ryder play in star trek [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>"], "scores": [-0.06116442009806633, -0.11968043446540833], "norm_scores": [0.514624834060669, 0.48537516593933105]}}
{"id": "WebQTest-459", "question": "where did jovan belcher kill himself", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.place_of_death", "location.location.containedby"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did jovan belcher kill himself [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>"], "scores": [-0.006662492640316486, -0.2214052975177765], "norm_scores": [0.553480327129364, 0.446519672870636]}}
{"id": "WebQTest-461", "question": "where did clay matthews go to school", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did clay matthews go to school [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.03715790435671806, -0.05151502788066864], "norm_scores": [0.5035892128944397, 0.4964107871055603]}}
{"id": "WebQTest-462", "question": "what time is it in texas houston right now", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time is it in texas houston right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>"], "scores": [-8.337229519383982e-05, -0.4474334120750427], "norm_scores": [0.610008955001831, 0.38999101519584656]}}
{"id": "WebQTest-463", "question": "who is princess leia in star wars", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is princess leia in star wars [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.03359255567193031, -0.08308198302984238], "norm_scores": [0.5123698115348816, 0.487630158662796]}}
{"id": "WebQTest-464", "question": "what is the political structure of china", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the political structure of china [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.02406846173107624, -0.07834279537200928], "norm_scores": [0.5135652422904968, 0.4864347577095032]}}
{"id": "WebQTest-465", "question": "what is new york city airport", "prediction": [["aviation.airport.serves"], ["location.location.containedby"]], "ground_paths": [["location.location.nearby_airports"], ["aviation.airport.serves"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is new york city airport [/INST]", "raw_output": {"paths": ["<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.03456488624215126, -0.16256150603294373], "norm_scores": [0.5319555401802063, 0.4680444598197937]}}
{"id": "WebQTest-466", "question": "what language brazil speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language brazil speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.0008171193185262382, -0.34735897183418274], "norm_scores": [0.5857787132263184, 0.41422125697135925]}}
{"id": "WebQTest-467", "question": "who played obi wan in episode 2", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played obi wan in episode 2 [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.08407937735319138, -0.09927236288785934], "norm_scores": [0.5037981867790222, 0.4962018132209778]}}
{"id": "WebQTest-468", "question": "where did venus williams come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did venus williams come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.017880680039525032, -0.17061547935009003], "norm_scores": [0.5381096601486206, 0.4618903398513794]}}
{"id": "WebQTest-469", "question": "what currency is used in panama", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency is used in panama [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.04811533913016319, -0.08043874800205231], "norm_scores": [0.5080801248550415, 0.4919198453426361]}}
{"id": "WebQTest-470", "question": "where does the zambezi river originate", "prediction": [["geography.river.origin"], ["location.location.partial_containedby"]], "ground_paths": [["geography.river.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the zambezi river originate [/INST]", "raw_output": {"paths": ["<PATH> geography.river.origin </PATH>", "<PATH> location.location.partial_containedby </PATH>"], "scores": [-0.0031806931365281343, -0.43113479018211365], "norm_scores": [0.6053850054740906, 0.39461496472358704]}}
{"id": "WebQTest-472", "question": "what team did ronaldo play for in 2003", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did ronaldo play for in 2003 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>"], "scores": [-0.049219872802495956, -0.06218702346086502], "norm_scores": [0.5032417178153992, 0.49675825238227844]}}
{"id": "WebQTest-473", "question": "where does robin williams live 2011", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.places_lived", "people.place_lived.location"]], "ground_paths": [["people.person.nationality", "location.administrative_division.second_level_division_of"], ["film.film.produced_by", "film.film_location.featured_in_films"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.parents", "people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does robin williams live 2011 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>"], "scores": [-0.05203655734658241, -0.07370516657829285], "norm_scores": [0.5054169297218323, 0.4945830702781677]}}
{"id": "WebQTest-474", "question": "who played jacob black in twilight", "prediction": [["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "ground_paths": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["film.film.starring", "film.actor.film"], ["film.film.country", "people.person.nationality"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["award.award_honor.honored_for", "award.award_winner.awards_won"], ["award.award_honor.honored_for", "award.award_honor.award_winner"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played jacob black in twilight [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.06512188166379929, -0.07244094461202621], "norm_scores": [0.5018297433853149, 0.49817022681236267]}}
{"id": "WebQTest-477", "question": "which airport to fly into in buenos aires", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"]], "ground_paths": [["location.location.nearby_airports"], ["aviation.airport.serves"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which airport to fly into in buenos aires [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>"], "scores": [-0.03557979688048363, -0.13552477955818176], "norm_scores": [0.5249654650688171, 0.47503453493118286]}}
{"id": "WebQTest-479", "question": "what team did adrian peterson play for in college", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did adrian peterson play for in college [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.027603449299931526, -0.04417382553219795], "norm_scores": [0.5041425228118896, 0.49585750699043274]}}
{"id": "WebQTest-480", "question": "where does name pennsylvania come from", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"]], "ground_paths": [["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does name pennsylvania come from [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.02026476338505745, -0.14007429778575897], "norm_scores": [0.5299165844917297, 0.4700833857059479]}}
{"id": "WebQTest-481", "question": "what kind of money do they use in germany", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money do they use in germany [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.024289479479193687, -0.13840223848819733], "norm_scores": [0.5284972786903381, 0.47150275111198425]}}
{"id": "WebQTest-482", "question": "who plays blaine in batman", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays blaine in batman [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.041325122117996216, -0.09274069219827652], "norm_scores": [0.5128510594367981, 0.4871489405632019]}}
{"id": "WebQTest-484", "question": "what language does australia use", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does australia use [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.03613683581352234, -0.07399116456508636], "norm_scores": [0.5094624757766724, 0.49053755402565]}}
{"id": "WebQTest-487", "question": "who did veronica lake mary", "prediction": [["people.person.spouse_s", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"]], "ground_paths": [["people.marriage.spouse", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did veronica lake mary [/INST]", "raw_output": {"paths": ["<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>"], "scores": [-0.09518619626760483, -0.11543788015842438], "norm_scores": [0.5050626993179321, 0.4949372410774231]}}
{"id": "WebQTest-488", "question": "who did sir francis drake marry", "prediction": [["people.person.children", "people.person.children"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.spouse_s", "people.marriage.spouse"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did sir francis drake marry [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.10488180071115494, -0.15343400835990906], "norm_scores": [0.5121356844902039, 0.48786431550979614]}}
{"id": "WebQTest-489", "question": "who plays stephanie plum in one for the money", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"]], "ground_paths": [["film.film.executive_produced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays stephanie plum in one for the money [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>"], "scores": [-0.08814588934183121, -0.13902035355567932], "norm_scores": [0.5127158761024475, 0.4872841238975525]}}
{"id": "WebQTest-490", "question": "what did the ancient romans speak", "prediction": [["fictional_universe.fictional_setting.languages"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did the ancient romans speak [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_setting.languages </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.041974373161792755, -0.1101837083697319], "norm_scores": [0.5170457363128662, 0.4829542934894562]}}
{"id": "WebQTest-491", "question": "what does ringo sing", "prediction": [["music.recording.artist"], ["music.artist.track"]], "ground_paths": [["music.recording.artist"], ["theater.theatrical_composer.plays_composed", "film.film.sequel"], ["music.album.artist", "music.album.compositions"], ["music.composer.compositions"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["music.album.artist"], ["music.lyricist.lyrics_written"], ["music.recording.featured_artists"], ["music.artist.album"], ["music.featured_artist.recordings"], ["organization.organization.founders", "film.film.production_companies"], ["music.composition.composer"], ["music.artist.label", "music.record_label.releases"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does ringo sing [/INST]", "raw_output": {"paths": ["<PATH> music.recording.artist </PATH>", "<PATH> music.artist.track </PATH>"], "scores": [-0.09998227655887604, -0.12363652139902115], "norm_scores": [0.5059133172035217, 0.49408671259880066]}}
{"id": "WebQTest-492", "question": "where was st. lucy born", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was st. lucy born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.0269379410892725, -0.10565288364887238], "norm_scores": [0.5196685791015625, 0.4803314208984375]}}
{"id": "WebQTest-493", "question": "what type of government does australia have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does australia have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>"], "scores": [-0.02817501686513424, -0.06639498472213745], "norm_scores": [0.5095537900924683, 0.49044615030288696]}}
{"id": "WebQTest-497", "question": "who was the father of king george vi", "prediction": [["people.person.parents"], ["people.person.children"]], "ground_paths": [["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the father of king george vi [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.06942106038331985, -0.11912296712398529], "norm_scores": [0.5124229192733765, 0.48757705092430115]}}
{"id": "WebQTest-500", "question": "what is the zip code for trenton", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for trenton [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.00044800006435252726, -0.5504091382026672], "norm_scores": [0.6341265439987183, 0.36587342619895935]}}
{"id": "WebQTest-501", "question": "what has ashley greene been in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.actor.film", "film.film.starring"], ["award.award_winner.awards_won", "award.award_honor.honored_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has ashley greene been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.06314604729413986, -0.12257852405309677], "norm_scores": [0.5148537755012512, 0.48514628410339355]}}
{"id": "WebQTest-502", "question": "where did al qaeda attack", "prediction": [["base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"]], "ground_paths": [["film.film.subjects", "film.film.subjects"], ["base.terrorism.terrorist_organization.involved_in_attacks"], ["organization.organization_founder.organizations_founded", "base.culturalevent.event.entity_involved"], ["military.military_combatant.military_conflicts", "military.military_combatant_group.conflict"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["base.disaster2.attack_process.attacker", "base.disaster2.attack.attack"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did al qaeda attack [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.06409168988466263, -0.07311011850833893], "norm_scores": [0.5022546052932739, 0.4977453947067261]}}
{"id": "WebQTest-503", "question": "what language is spoken in switzerland", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is spoken in switzerland [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.08290553092956543, -0.09292318671941757], "norm_scores": [0.5025043487548828, 0.4974955916404724]}}
{"id": "WebQTest-504", "question": "who was philip in acts chapter 8", "prediction": [["people.person.gender", "people.person.gender"], ["people.person.profession"]], "ground_paths": [["fictional_universe.person_in_fiction.representations_in_fiction"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was philip in acts chapter 8 [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.0577763207256794, -0.09828374534845352], "norm_scores": [0.510125458240509, 0.4898745119571686]}}
{"id": "WebQTest-506", "question": "what the time zone in england", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the time zone in england [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>"], "scores": [-0.006008850410580635, -0.14577347040176392], "norm_scores": [0.5348843932151794, 0.46511563658714294]}}
{"id": "WebQTest-508", "question": "what is the best currency to take to turkey", "prediction": [["finance.currency.countries_formerly_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the best currency to take to turkey [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.0682937279343605, -0.0788525864481926], "norm_scores": [0.5026397109031677, 0.4973602890968323]}}
{"id": "WebQTest-509", "question": "what system of government is practiced in nigeria", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what system of government is practiced in nigeria [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.01955355890095234, -0.09071417152881622], "norm_scores": [0.5177826285362244, 0.48221734166145325]}}
{"id": "WebQTest-512", "question": "who were mary shelley", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were mary shelley [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.00997009128332138, -0.33346763253211975], "norm_scores": [0.5801764130592346, 0.41982361674308777]}}
{"id": "WebQTest-513", "question": "what language do most australians speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do most australians speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.029269898310303688, -0.08596103638410568], "norm_scores": [0.5141689777374268, 0.48583099246025085]}}
{"id": "WebQTest-514", "question": "where is denmark situated", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is denmark situated [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.09635816514492035, -0.12694023549556732], "norm_scores": [0.5076449513435364, 0.492355078458786]}}
{"id": "WebQTest-515", "question": "where was teddy roosevelt 's house", "prediction": [["symbols.name_source.namesakes", "location.location.containedby"], ["people.person.parents", "people.person.place_of_birth"]], "ground_paths": [["book.written_work.subjects", "location.administrative_division.country"], ["book.written_work.author", "base.biblioness.bibs_location.state"], ["people.person.parents", "people.deceased_person.place_of_death"], ["people.person.children", "people.person.place_of_birth"], ["symbols.namesake.named_after", "transportation.bridge.locale"], ["people.person.children", "people.deceased_person.place_of_death"], ["base.usnris.nris_listing.significant_person", "travel.travel_destination.tourist_attractions"], ["symbols.name_source.namesakes", "location.location.containedby"], ["base.inaugurations.inauguration.president", "time.event.locations"], ["book.written_work.author", "people.deceased_person.place_of_death"], ["book.written_work.subjects", "location.location.containedby"], ["book.book_edition.author_editor", "book.book_edition.place_of_publication"], ["people.person.place_of_birth", "base.aareas.schema.administrative_area.administrative_children"], ["people.person.parents", "people.person.place_of_birth"], ["symbols.namesake.named_after", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was teddy roosevelt 's house [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes <SEP> location.location.containedby </PATH>", "<PATH> people.person.parents <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.07687445729970932, -0.12213470786809921], "norm_scores": [0.5113130807876587, 0.48868685960769653]}}
{"id": "WebQTest-516", "question": "what is the government system of malaysia", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the government system of malaysia [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.03602118790149689, -0.0560249425470829], "norm_scores": [0.5050007700920105, 0.4949992001056671]}}
{"id": "WebQTest-517", "question": "what do people speak in canada", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people speak in canada [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.057885684072971344, -0.07698910683393478], "norm_scores": [0.5047757029533386, 0.495224267244339]}}
{"id": "WebQTest-518", "question": "what type of money do they use in costa rica", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of money do they use in costa rica [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.028442366048693657, -0.12479053437709808], "norm_scores": [0.5240684151649475, 0.4759315848350525]}}
{"id": "WebQTest-519", "question": "what timezone is nashville tn", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is nashville tn [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-4.205515142530203e-05, -0.9126840233802795], "norm_scores": [0.7135404944419861, 0.2864595353603363]}}
{"id": "WebQTest-521", "question": "who was anakin skywalker", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was anakin skywalker [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> fictional_universe.fictional_character.gender </PATH>"], "scores": [-0.011694027110934258, -0.09351176768541336], "norm_scores": [0.5204430222511292, 0.47955697774887085]}}
{"id": "WebQTest-522", "question": "where did adolf hitler die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people", "people.deceased_person.place_of_death"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did adolf hitler die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.0018042215378955007, -0.2054380476474762], "norm_scores": [0.5507332682609558, 0.4492667317390442]}}
{"id": "WebQTest-523", "question": "what is the largest nation in europe", "prediction": [["location.location.containedby"], ["base.locations.continents.countries_within"]], "ground_paths": [["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the largest nation in europe [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.0639016330242157, -0.08214834332466125], "norm_scores": [0.5045615434646606, 0.49543842673301697]}}
{"id": "WebQTest-524", "question": "where was the first gulf war fought", "prediction": [["time.event.locations"], ["location.location.events"]], "ground_paths": [["location.location.events"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the first gulf war fought [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.025473762303590775, -0.2005656361579895], "norm_scores": [0.5436614751815796, 0.4563385248184204]}}
{"id": "WebQTest-525", "question": "who are the senators of virginia 2013", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"]], "ground_paths": [["location.administrative_division.country", "people.person.nationality"], ["government.election.district", "government.election.winner"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the senators of virginia 2013 [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.042623985558748245, -0.05981551110744476], "norm_scores": [0.5042977929115295, 0.49570220708847046]}}
{"id": "WebQTest-526", "question": "what band was george clinton in", "prediction": [["music.artist.genre", "music.artist.genre"], ["music.recording.artist", "music.recording.artist"]], "ground_paths": [["music.artist.origin", "music.artist.origin"], ["music.recording.producer", "music.recording.artist"], ["music.album.artist", "music.artist.album"], ["music.group_membership.member", "music.musical_group.member"], ["people.person.nationality", "music.artist.origin"], ["music.composer.compositions", "music.recording.artist"], ["music.recording.featured_artists", "music.artist.album"], ["music.recording.featured_artists", "music.featured_artist.recordings"], ["music.composition.composer", "music.artist.track"], ["music.artist.album", "music.album.artist"], ["music.producer.releases_produced", "music.album.artist"], ["music.recording.artist", "music.recording.artist"], ["common.topic.notable_types", "common.topic.notable_types"], ["music.artist.genre", "music.genre.artists"], ["music.album.artist", "music.album.artist"], ["music.producer.tracks_produced", "music.recording.artist"], ["music.featured_artist.recordings", "music.recording.featured_artists"], ["music.recording.producer", "music.album.artist"], ["base.saturdaynightlive.snl_episode.musical_guest", "base.saturdaynightlive.snl_musical_guest.episodes_as_musical_guest"], ["music.featured_artist.recordings", "music.recording.artist"], ["music.composition.composer", "music.recording.artist"], ["broadcast.artist.content", "broadcast.artist.content"], ["music.group_membership.member", "music.group_membership.group"], ["music.composer.compositions", "music.artist.track"], ["music.artist.label", "music.artist.label"], ["music.group_member.membership", "music.musical_group.member"], ["music.artist.album", "music.artist.album"], ["music.recording.artist", "music.album.artist"], ["music.recording.featured_artists", "music.recording.featured_artists"], ["music.artist.genre", "music.artist.genre"], ["music.record_label.artist", "music.record_label.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what band was george clinton in [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> music.recording.artist <SEP> music.recording.artist </PATH>"], "scores": [-0.049647435545921326, -0.17382216453552246], "norm_scores": [0.5310038924217224, 0.46899616718292236]}}
{"id": "WebQTest-527", "question": "what country borders slovakia", "prediction": [["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_containedby"]], "ground_paths": [["sports.sport_country.multi_event_tournaments_participated_in", "sports.sport_country.multi_event_tournaments_participated_in"], ["location.location.partially_containedby", "geography.river.origin"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["location.location.containedby", "location.location.containedby"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["location.location.partially_contains", "geography.river.basin_countries"], ["location.location.time_zones", "location.location.time_zones"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["military.military_combatant_group.combatants", "military.military_combatant.military_conflicts"], ["base.uncommon.topic.exceptions", "base.uncommon.topic.exceptions"], ["location.location.partially_containedby", "location.location.partially_contains"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.containedby", "location.location.contains"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["government.form_of_government.countries", "location.country.form_of_government"], ["location.location.events", "location.location.events"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.partially_containedby", "location.location.containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["base.datedlocationtest.dated_location_break_up.new_locations", "base.datedlocationtest.dated_location_break_up.new_locations"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["finance.currency.countries_formerly_used", "location.country.currency_formerly_used"], ["base.locations.countries.continent", "base.locations.countries.continent"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["location.country.official_language", "language.human_language.countries_spoken_in"], ["location.country.currency_used", "location.country.currency_used"], ["organization.organization.founders", "organization.organization.founders"], ["location.location.partially_contains", "location.location.partially_containedby"], ["base.locations.countries.continent", "base.locations.continents.countries_within"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country borders slovakia [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>"], "scores": [-0.11761928349733353, -0.12148820608854294], "norm_scores": [0.5009672045707703, 0.49903273582458496]}}
{"id": "WebQTest-528", "question": "what was james garfield most known for", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was james garfield most known for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.006684832274913788, -0.28871166706085205], "norm_scores": [0.5700430870056152, 0.42995694279670715]}}
{"id": "WebQTest-529", "question": "where did paula deen go to school", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did paula deen go to school [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.05142038315534592, -0.0621785931289196], "norm_scores": [0.5026894807815552, 0.49731045961380005]}}
{"id": "WebQTest-530", "question": "where does robert irvine come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does robert irvine come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.017596673220396042, -0.17303313314914703], "norm_scores": [0.5387810468673706, 0.461218923330307]}}
{"id": "WebQTest-531", "question": "what style of music does michael jackson perform", "prediction": [["music.artist.genre"], ["music.genre.artists"]], "ground_paths": [["music.genre.artists"], ["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what style of music does michael jackson perform [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>"], "scores": [-0.004872675519436598, -0.32387152314186096], "norm_scores": [0.5790802240371704, 0.4209197461605072]}}
{"id": "WebQTest-532", "question": "what does david beckham play", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.position"]], "ground_paths": [["sports.pro_sports_played.athlete", "sports.pro_sports_played.sport"], ["soccer.football_player.position_s", "sports.sport.positions"], ["book.written_work.subjects", "book.book_subject.works"], ["symbols.namesake.named_after", "cvg.computer_videogame.cvg_genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does david beckham play [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.position </PATH>"], "scores": [-0.07954549044370651, -0.10856391489505768], "norm_scores": [0.5072540640830994, 0.49274590611457825]}}
{"id": "WebQTest-533", "question": "what does julian lennon do", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does julian lennon do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.005919790826737881, -0.30014705657958984], "norm_scores": [0.5730307102203369, 0.4269692897796631]}}
{"id": "WebQTest-534", "question": "what is the milwaukee brewers mascot", "prediction": [["sports.mascot.team"], ["sports.sports_team.team_mascot"]], "ground_paths": [["sports.sports_team.team_mascot"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the milwaukee brewers mascot [/INST]", "raw_output": {"paths": ["<PATH> sports.mascot.team </PATH>", "<PATH> sports.sports_team.team_mascot </PATH>"], "scores": [-0.006093839183449745, -0.17224793136119843], "norm_scores": [0.5414432287216187, 0.45855677127838135]}}
{"id": "WebQTest-535", "question": "what is the second biggest state in the united states", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the second biggest state in the united states [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.009080607444047928, -0.17355892062187195], "norm_scores": [0.5410271286964417, 0.45897287130355835]}}
{"id": "WebQTest-537", "question": "what has charlie hunnam been in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["film.actor.film", "film.film.starring"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has charlie hunnam been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.08303982019424438, -0.10236401855945587], "norm_scores": [0.5048309564590454, 0.49516910314559937]}}
{"id": "WebQTest-538", "question": "what are all the movies taylor swift has been in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["people.person.languages", "film.film.language"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["music.album.genre", "media_common.netflix_title.netflix_genres"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.person_or_entity_appearing_in_film.films", "film.film.personal_appearances"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are all the movies taylor swift has been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.10415981709957123, -0.16622453927993774], "norm_scores": [0.5155112147331238, 0.4844888150691986]}}
{"id": "WebQTest-540", "question": "what college did brett favre", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did brett favre [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.053634218871593475, -0.054071687161922455], "norm_scores": [0.5001093745231628, 0.49989062547683716]}}
{"id": "WebQTest-542", "question": "where did alex chilton die", "prediction": [["people.deceased_person.place_of_death"], ["location.location.people_born_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did alex chilton die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0010246364399790764, -0.45183441042900085], "norm_scores": [0.6108317375183105, 0.38916826248168945]}}
{"id": "WebQTest-543", "question": "what college did martin luther king jr go to", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did martin luther king jr go to [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.04394358769059181, -0.0634913221001625], "norm_scores": [0.5048868060112, 0.49511322379112244]}}
{"id": "WebQTest-545", "question": "what kind of cancer did farrah fawcett die of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of cancer did farrah fawcett die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.029176507145166397, -0.08043146878480911], "norm_scores": [0.5128109455108643, 0.48718905448913574]}}
{"id": "WebQTest-546", "question": "who is michael j fox 's wife", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["people.person.spouse_s", "people.marriage.spouse"], ["award.award_winner.awards_won", "award.award_honor.award_winner"], ["common.image.appears_in_topic_gallery", "common.topic.image"], ["people.person.children", "people.person.parents"], ["people.person.profession", "people.person.profession"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.parents"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is michael j fox 's wife [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.052301570773124695, -0.08358193933963776], "norm_scores": [0.5078194737434387, 0.49218055605888367]}}
{"id": "WebQTest-547", "question": "what money do i use in costa rica", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money do i use in costa rica [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.02770719677209854, -0.12704047560691833], "norm_scores": [0.5248129367828369, 0.4751870930194855]}}
{"id": "WebQTest-548", "question": "what guitar did joe walsh play", "prediction": [["music.guitarist.guitars_played"], ["music.group_member.instruments_played"]], "ground_paths": [["music.guitar.guitarists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what guitar did joe walsh play [/INST]", "raw_output": {"paths": ["<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.group_member.instruments_played </PATH>"], "scores": [-0.0715925395488739, -0.07808671146631241], "norm_scores": [0.5016235113143921, 0.4983764588832855]}}
{"id": "WebQTest-549", "question": "what are the official languages of the eu", "prediction": [["organization.organization.geographic_scope", "language.human_language.countries_spoken_in"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_language"], ["organization.organization_founder.organizations_founded", "location.country.official_language"], ["organization.organization.founders", "location.country.languages_spoken"], ["organization.organization.founders", "location.country.official_language"], ["organization.organization.geographic_scope", "language.human_language.region"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the official languages of the eu [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.geographic_scope <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.07615479081869125, -0.08449868112802505], "norm_scores": [0.5020859241485596, 0.49791401624679565]}}
{"id": "WebQTest-550", "question": "what is ryan seacrest radio show", "prediction": [["broadcast.content.producer"], ["broadcast.producer.content", "broadcast.content.producer"]], "ground_paths": [["broadcast.artist.content"], ["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is ryan seacrest radio show [/INST]", "raw_output": {"paths": ["<PATH> broadcast.content.producer </PATH>", "<PATH> broadcast.producer.content <SEP> broadcast.content.producer </PATH>"], "scores": [-0.17685428261756897, -0.18239127099514008], "norm_scores": [0.5013842582702637, 0.4986157715320587]}}
{"id": "WebQTest-552", "question": "what language do most italians speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do most italians speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.07477559894323349, -0.07492702454328537], "norm_scores": [0.5000378489494324, 0.4999621510505676]}}
{"id": "WebQTest-554", "question": "what money do they use in spain", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money do they use in spain [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.01995919831097126, -0.15519873797893524], "norm_scores": [0.5337584614753723, 0.4662415683269501]}}
{"id": "WebQTest-555", "question": "what did drew barrymore go to rehab for", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.person.profession", "people.profession.specializations"]], "ground_paths": [["celebrities.celebrity.substance_abuse_problems", "celebrities.abused_substance.abused_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did drew barrymore go to rehab for [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.person.profession <SEP> people.profession.specializations </PATH>"], "scores": [-0.06824196875095367, -0.09333283454179764], "norm_scores": [0.5062723755836487, 0.49372759461402893]}}
{"id": "WebQTest-556", "question": "when is saint george day celebrated", "prediction": [["time.recurring_event.current_frequency"], ["time.day_of_year.holidays"]], "ground_paths": [["time.holiday.day_of_year"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is saint george day celebrated [/INST]", "raw_output": {"paths": ["<PATH> time.recurring_event.current_frequency </PATH>", "<PATH> time.day_of_year.holidays </PATH>"], "scores": [-0.05782485380768776, -0.10242883116006851], "norm_scores": [0.5111491680145264, 0.488850861787796]}}
{"id": "WebQTest-557", "question": "what government did the roman empire have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what government did the roman empire have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.017368987202644348, -0.10024440288543701], "norm_scores": [0.5207070112228394, 0.47929298877716064]}}
{"id": "WebQTest-559", "question": "what instrument did robin gibb play", "prediction": [["music.group_member.instruments_played"], ["music.instrument.family"]], "ground_paths": [["music.track_contribution.contributor", "music.track_contribution.role"], ["music.group_membership.member", "music.group_membership.role"], ["music.group_member.instruments_played"], ["music.group_membership.member", "music.performance_role.regular_performances"], ["music.artist.track_contributions", "music.track_contribution.role"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what instrument did robin gibb play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.family </PATH>"], "scores": [-0.0032851011492311954, -0.43516403436660767], "norm_scores": [0.6063222885131836, 0.3936777412891388]}}
{"id": "WebQTest-560", "question": "who voiced darth vader", "prediction": [["film.performance.special_performance_type", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who voiced darth vader [/INST]", "raw_output": {"paths": ["<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.08451767265796661, -0.10863883048295975], "norm_scores": [0.5060300230979919, 0.49397000670433044]}}
{"id": "WebQTest-561", "question": "when was abe lincoln president", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_holder"]], "ground_paths": [["base.inaugurations.inauguration.president"], ["base.inaugurations.inauguration_speaker.inauguration"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was abe lincoln president [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.055751048028469086, -0.08509434014558792], "norm_scores": [0.5073353052139282, 0.4926646947860718]}}
{"id": "WebQTest-562", "question": "where are the texas rangers playing", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the texas rangers playing [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>"], "scores": [-0.004387719091027975, -0.21285493671894073], "norm_scores": [0.5519288778305054, 0.44807112216949463]}}
{"id": "WebQTest-564", "question": "what is arkansas state capitol", "prediction": [["base.biblioness.bibs_location.state"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is arkansas state capitol [/INST]", "raw_output": {"paths": ["<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.08854246884584427, -0.10557287931442261], "norm_scores": [0.5042575001716614, 0.4957424998283386]}}
{"id": "WebQTest-567", "question": "what is the zip code for nogales az", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for nogales az [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0015660342760384083, -0.43430763483047485], "norm_scores": [0.6065281629562378, 0.3934718370437622]}}
{"id": "WebQTest-568", "question": "when did michael vick start playing for the eagles", "prediction": [["american_football.football_player.rushing", "american_football.player_rushing_statistics.season"], ["american_football.football_player.games", "american_football.player_game_statistics.season"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did michael vick start playing for the eagles [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.rushing <SEP> american_football.player_rushing_statistics.season </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.season </PATH>"], "scores": [-0.0542718805372715, -0.059325095266103745], "norm_scores": [0.5012632608413696, 0.4987366795539856]}}
{"id": "WebQTest-570", "question": "what is william taft famous for", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is william taft famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>"], "scores": [-0.055521272122859955, -0.08502831310033798], "norm_scores": [0.5073761940002441, 0.49262377619743347]}}
{"id": "WebQTest-572", "question": "where did robin cook die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people_deceased_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did robin cook die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>"], "scores": [-0.0010933217126876116, -0.3480275571346283], "norm_scores": [0.5858739614486694, 0.41412606835365295]}}
{"id": "WebQTest-573", "question": "where did melba beals live", "prediction": [["people.person.place_of_birth"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.nationality", "location.country.first_level_divisions"], ["people.person.place_of_birth", "location.us_state.capital"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did melba beals live [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.07123832404613495, -0.116996631026268], "norm_scores": [0.5114375948905945, 0.4885624051094055]}}
{"id": "WebQTest-575", "question": "who is louisiana state senator", "prediction": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["location.administrative_division.country", "people.person.nationality"]], "ground_paths": [["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["base.locations.countries.states_provinces_within", "people.person.nationality"], ["government.election.district", "government.election.winner"], ["location.location.containedby", "people.person.place_of_birth"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["location.location.containedby", "symbols.name_source.namesakes"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["location.location.partially_containedby", "people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is louisiana state senator [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> location.administrative_division.country <SEP> people.person.nationality </PATH>"], "scores": [-0.054392240941524506, -0.06861832737922668], "norm_scores": [0.5035564303398132, 0.4964435398578644]}}
{"id": "WebQTest-576", "question": "what are the 7 countries that are part of central america", "prediction": [["location.location.contains"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the 7 countries that are part of central america [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.052307333797216415, -0.16059957444667816], "norm_scores": [0.5270466208457947, 0.4729533791542053]}}
{"id": "WebQTest-577", "question": "where did derek fisher go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did derek fisher go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.06070181354880333, -0.07189919799566269], "norm_scores": [0.5027992725372314, 0.4972006678581238]}}
{"id": "WebQTest-578", "question": "who was kobe bryant drafted by", "prediction": [["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was kobe bryant drafted by [/INST]", "raw_output": {"paths": ["<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>"], "scores": [-0.059630874544382095, -0.06109651178121567], "norm_scores": [0.5003664493560791, 0.4996335804462433]}}
{"id": "WebQTest-579", "question": "where are the new orleans hornets moving to", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.location"]], "ground_paths": [["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the new orleans hornets moving to [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>"], "scores": [-0.044804275035858154, -0.1409093588590622], "norm_scores": [0.5240077972412109, 0.47599220275878906]}}
{"id": "WebQTest-580", "question": "what language does algerian people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does algerian people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.05384404957294464, -0.06416140496730804], "norm_scores": [0.5025793313980103, 0.49742069840431213]}}
{"id": "WebQTest-581", "question": "where was anne boleyn executed", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.place_of_burial"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was anne boleyn executed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>"], "scores": [-0.004219572525471449, -0.3940269947052002], "norm_scores": [0.5962363481521606, 0.40376368165016174]}}
{"id": "WebQTest-582", "question": "what did galileo do to become famous", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did galileo do to become famous [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.010129143483936787, -0.2880195379257202], "norm_scores": [0.5690289735794067, 0.43097105622291565]}}
{"id": "WebQTest-583", "question": "what school did martin luther king jr received his doctorate degree from", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did martin luther king jr received his doctorate degree from [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.02175024524331093, -0.08847249299287796], "norm_scores": [0.5166743397712708, 0.48332563042640686]}}
{"id": "WebQTest-584", "question": "where buddha come from", "prediction": [["people.person.place_of_birth"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where buddha come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.018647687509655952, -0.20108594000339508], "norm_scores": [0.5454834699630737, 0.45451653003692627]}}
{"id": "WebQTest-585", "question": "what did king henry viii rule", "prediction": [["royalty.kingdom.rulers"], ["people.person.nationality"]], "ground_paths": [["royalty.monarch.kingdom"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did king henry viii rule [/INST]", "raw_output": {"paths": ["<PATH> royalty.kingdom.rulers </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.06489725410938263, -0.15862290561199188], "norm_scores": [0.5234143137931824, 0.4765857458114624]}}
{"id": "WebQTest-586", "question": "what type of cancer did larry hagman have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of cancer did larry hagman have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.04637129604816437, -0.053068455308675766], "norm_scores": [0.5016742944717407, 0.4983257055282593]}}
{"id": "WebQTest-588", "question": "what wars did robert e lee served in", "prediction": [["base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"]], "ground_paths": [["military.military_command.military_commander", "military.military_conflict.commanders"], ["military.military_person.participated_in_conflicts"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"], ["military.military_commander.military_commands", "military.military_command.military_conflict"], ["base.culturalevent.event.entity_involved"], ["military.military_commander.military_commands", "military.military_conflict.commanders"], ["people.person.nationality", "base.culturalevent.event.entity_involved"], ["military.military_conflict.military_personnel_involved"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what wars did robert e lee served in [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.included_in_event </PATH>"], "scores": [-0.022775037214159966, -0.06653469055891037], "norm_scores": [0.5109381675720215, 0.4890618324279785]}}
{"id": "WebQTest-589", "question": "what russian language called", "prediction": [["language.human_language.main_country"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what russian language called [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.06219059228897095, -0.113815538585186], "norm_scores": [0.5129033923149109, 0.4870966374874115]}}
{"id": "WebQTest-590", "question": "who is in charge of libya now", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["people.person.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is in charge of libya now [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.049643032252788544, -0.05337310582399368], "norm_scores": [0.500932514667511, 0.4990675151348114]}}
{"id": "WebQTest-592", "question": "when did shays rebellion start", "prediction": [["time.event.included_in_event"], ["time.event.includes_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did shays rebellion start [/INST]", "raw_output": {"paths": ["<PATH> time.event.included_in_event </PATH>", "<PATH> time.event.includes_event </PATH>"], "scores": [-0.06054096668958664, -0.10839761793613434], "norm_scores": [0.5119618773460388, 0.4880381226539612]}}
{"id": "WebQTest-593", "question": "what kind of disease does montel williams have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"]], "ground_paths": [["medicine.notable_person_with_medical_condition.condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of disease does montel williams have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>"], "scores": [-0.029567761346697807, -0.04119366779923439], "norm_scores": [0.5029064416885376, 0.4970935583114624]}}
{"id": "WebQTest-595", "question": "when is the last time the ravens won a superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is the last time the ravens won a superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.044940970838069916, -0.07888217270374298], "norm_scores": [0.5084844827651978, 0.49151551723480225]}}
{"id": "WebQTest-596", "question": "where did kansas city royals come from", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.location"]], "ground_paths": [["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kansas city royals come from [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>"], "scores": [-0.0579339936375618, -0.10104575753211975], "norm_scores": [0.5107762813568115, 0.4892237186431885]}}
{"id": "WebQTest-597", "question": "what style of music does john mayer play", "prediction": [["music.artist.genre"], ["music.genre.artists"]], "ground_paths": [["music.genre.artists"], ["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what style of music does john mayer play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>"], "scores": [-0.003501779166981578, -0.3461140990257263], "norm_scores": [0.5848249197006226, 0.41517505049705505]}}
{"id": "WebQTest-599", "question": "what inventions did thomas edison invent list", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"]], "ground_paths": [["law.inventor.us_patents"], ["law.invention.inventor"], ["law.inventor.inventions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inventions did thomas edison invent list [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>"], "scores": [-0.0421549528837204, -0.11287080496549606], "norm_scores": [0.5176715850830078, 0.4823283851146698]}}
{"id": "WebQTest-600", "question": "where did djokovic come from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did djokovic come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.03735707327723503, -0.11421111971139908], "norm_scores": [0.5192040801048279, 0.4807959496974945]}}
{"id": "WebQTest-602", "question": "what part of the country is new england", "prediction": [["location.location.contains"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part of the country is new england [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0435713492333889, -0.12391839921474457], "norm_scores": [0.5200759768486023, 0.4799240231513977]}}
{"id": "WebQTest-603", "question": "what money to take to turkey", "prediction": [["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money to take to turkey [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.06038118898868561, -0.08789867907762527], "norm_scores": [0.5068789720535278, 0.49312105774879456]}}
{"id": "WebQTest-604", "question": "what kind of music did jimi hendrix play", "prediction": [["music.artist.genre"], ["music.genre.artists"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of music did jimi hendrix play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>"], "scores": [-0.0049408250488340855, -0.30819016695022583], "norm_scores": [0.5752366185188293, 0.42476335167884827]}}
{"id": "WebQTest-605", "question": "what degrees did obama get in college", "prediction": [["education.education.student", "education.education.degree"], ["people.person.education", "education.education.degree"]], "ground_paths": [["people.person.education", "education.education.degree"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what degrees did obama get in college [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> people.person.education <SEP> education.education.degree </PATH>"], "scores": [-0.031876541674137115, -0.05919519439339638], "norm_scores": [0.5068292617797852, 0.49317076802253723]}}
{"id": "WebQTest-606", "question": "who were alexander graham bell 's siblings", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.children"]], "ground_paths": [["people.person.parents", "people.person.children"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.gender", "people.person.gender"], ["people.person.parents", "people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were alexander graham bell 's siblings [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.054017648100852966, -0.09740577638149261], "norm_scores": [0.5108453035354614, 0.4891546666622162]}}
{"id": "WebQTest-608", "question": "when did liverpool fc last win the champions league", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did liverpool fc last win the champions league [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>"], "scores": [-0.01689811609685421, -0.12602223455905914], "norm_scores": [0.5272539854049683, 0.4727460443973541]}}
{"id": "WebQTest-609", "question": "who played jeannie", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "ground_paths": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_winning_work.awards_won", "award.award_honor.award_winner"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played jeannie [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.042681220918893814, -0.07298982888460159], "norm_scores": [0.5075765252113342, 0.492423415184021]}}
{"id": "WebQTest-611", "question": "where was st peter killed", "prediction": [["people.deceased_person.place_of_death"], ["location.location.events"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was st peter killed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.003052178304642439, -0.48395177721977234], "norm_scores": [0.6179602742195129, 0.38203972578048706]}}
{"id": "WebQTest-612", "question": "who does jeremy shockey play for in 2012", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does jeremy shockey play for in 2012 [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.039352357387542725, -0.04792226105928421], "norm_scores": [0.5021424293518066, 0.4978575110435486]}}
{"id": "WebQTest-613", "question": "what college did deion sanders jr go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did deion sanders jr go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.06884153932332993, -0.07527255266904831], "norm_scores": [0.5016077160835266, 0.4983922243118286]}}
{"id": "WebQTest-614", "question": "when did the battle of alamo start", "prediction": [["time.event.locations"], ["time.event.included_in_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the battle of alamo start [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> time.event.included_in_event </PATH>"], "scores": [-0.06343434005975723, -0.11931208521127701], "norm_scores": [0.5139657855033875, 0.48603421449661255]}}
{"id": "WebQTest-615", "question": "what were the names of henry viii 's three children", "prediction": [["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what were the names of henry viii 's three children [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.08017401397228241, -0.08434181660413742], "norm_scores": [0.5010419487953186, 0.4989580512046814]}}
{"id": "WebQTest-616", "question": "what was queen elizabeth ii childhood nickname", "prediction": [["people.person.children"], ["people.person.parents"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was queen elizabeth ii childhood nickname [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.1581372171640396, -0.17713870108127594], "norm_scores": [0.5047502517700195, 0.49524977803230286]}}
{"id": "WebQTest-617", "question": "where did tim tebow grow up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did tim tebow grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.041863858699798584, -0.08556607365608215], "norm_scores": [0.5109238028526306, 0.4890761971473694]}}
{"id": "WebQTest-618", "question": "where is chesapeake bay bridge", "prediction": [["transportation.bridge.locale"], ["transportation.bridge.body_of_water_spanned"]], "ground_paths": [["transportation.bridge.locale"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is chesapeake bay bridge [/INST]", "raw_output": {"paths": ["<PATH> transportation.bridge.locale </PATH>", "<PATH> transportation.bridge.body_of_water_spanned </PATH>"], "scores": [-0.0056181056424975395, -0.23560144007205963], "norm_scores": [0.5572437644004822, 0.4427562952041626]}}
{"id": "WebQTest-619", "question": "who played victoria barkley", "prediction": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "ground_paths": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["film.film.country", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played victoria barkley [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>"], "scores": [-0.054673369973897934, -0.058471281081438065], "norm_scores": [0.5009494423866272, 0.4990505278110504]}}
{"id": "WebQTest-620", "question": "where did kennedy 's inaugural address take place", "prediction": [["time.event.locations"], ["location.location.events"]], "ground_paths": [["base.inaugurations.inauguration.president", "location.location.events"], ["common.image.appears_in_topic_gallery", "common.image.image_location"], ["base.inaugurations.inauguration.president", "time.event.locations"], ["symbols.name_source.namesakes", "location.location.containedby"], ["symbols.name_source.namesakes", "people.person.place_of_birth"], ["people.person.nationality", "location.administrative_division.country"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kennedy 's inaugural address take place [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.004328818991780281, -0.4640779197216034], "norm_scores": [0.6129546761512756, 0.38704535365104675]}}
{"id": "WebQTest-621", "question": "what countries have german as the official language", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries have german as the official language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.07500412315130234, -0.07628709822893143], "norm_scores": [0.5003207325935364, 0.4996792674064636]}}
{"id": "WebQTest-622", "question": "where did the taliban began", "prediction": [["organization.organization.place_founded"], ["organization.organization.founders"]], "ground_paths": [["organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the taliban began [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.place_founded </PATH>", "<PATH> organization.organization.founders </PATH>"], "scores": [-0.08038074523210526, -0.12026271224021912], "norm_scores": [0.509969174861908, 0.49003082513809204]}}
{"id": "WebQTest-624", "question": "who plays elena gilbert on vampire diaries", "prediction": [["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"]], "ground_paths": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["common.image.appears_in_topic_gallery", "common.image.appears_in_topic_gallery"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays elena gilbert on vampire diaries [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.07066613435745239, -0.07209636270999908], "norm_scores": [0.5003575682640076, 0.49964243173599243]}}
{"id": "WebQTest-626", "question": "what does the missouri river bisect", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "ground_paths": [["location.location.partially_contains"], ["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does the missouri river bisect [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.04724027216434479, -0.09438424557447433], "norm_scores": [0.5117838382720947, 0.48821619153022766]}}
{"id": "WebQTest-627", "question": "what nationality is taylor lautner jacob black", "prediction": [["film.performance.character", "film.performance.actor"], ["people.person.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what nationality is taylor lautner jacob black [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.08919057250022888, -0.11978807300329208], "norm_scores": [0.507648766040802, 0.492351233959198]}}
{"id": "WebQTest-628", "question": "what animal represents california", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"]], "ground_paths": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what animal represents california [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.03727533295750618, -0.06185769662261009], "norm_scores": [0.5061452984809875, 0.49385470151901245]}}
{"id": "WebQTest-629", "question": "what company does nike own", "prediction": [["business.business_operation.industry", "business.business_operation.industry"], ["business.business_operation.industry"]], "ground_paths": [["business.business_operation.industry", "business.business_operation.industry"], ["business.industry.companies", "business.industry.companies"], ["business.acquisition.acquiring_company", "business.acquisition.company_acquired"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what company does nike own [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> business.business_operation.industry </PATH>"], "scores": [-0.07838477939367294, -0.11877939105033875], "norm_scores": [0.5100972652435303, 0.4899027347564697]}}
{"id": "WebQTest-630", "question": "what language do colombian people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do colombian people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.03973698988556862, -0.06585223227739334], "norm_scores": [0.5065284371376038, 0.49347156286239624]}}
{"id": "WebQTest-631", "question": "what year did ray allen join the nba", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did ray allen join the nba [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>"], "scores": [-0.048113010823726654, -0.060014307498931885], "norm_scores": [0.5029752850532532, 0.4970247149467468]}}
{"id": "WebQTest-633", "question": "what is claire danes famous for", "prediction": [["people.person.profession"], ["film.actor.film", "film.film_character.portrayed_in_films"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is claire danes famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.005599969532340765, -0.2190345823764801], "norm_scores": [0.5531569719314575, 0.4468429684638977]}}
{"id": "WebQTest-634", "question": "what county is san antonio in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["location.us_county.county_seat"], ["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is san antonio in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0321669839322567, -0.1392524093389511], "norm_scores": [0.5267457962036133, 0.47325417399406433]}}
{"id": "WebQTest-635", "question": "what characters does matt stone do", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"]], "ground_paths": [["film.actor.film", "film.performance.character"], ["tv.tv_guest_role.actor", "tv.tv_character.appeared_in_tv_episodes"], ["freebase.valuenotation.has_value", "freebase.valuenotation.has_no_value"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["people.person.profession", "people.person.profession"], ["tv.tv_actor.guest_roles", "tv.tv_character.appeared_in_tv_episodes"], ["people.person.ethnicity", "people.person.ethnicity"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.friendship.participant", "base.popstra.friendship.participant"], ["people.person.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what characters does matt stone do [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_character.appeared_in_tv_program </PATH>"], "scores": [-0.04676514118909836, -0.08943445980548859], "norm_scores": [0.5106657147407532, 0.4893342852592468]}}
{"id": "WebQTest-636", "question": "what timezone is the state of michigan and", "prediction": [["location.location.time_zones"], ["base.locations.states_and_provences.country", "location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is the state of michigan and [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> base.locations.states_and_provences.country <SEP> location.location.time_zones </PATH>"], "scores": [-5.827698259963654e-05, -0.3863039016723633], "norm_scores": [0.595378577709198, 0.404621422290802]}}
{"id": "WebQTest-637", "question": "what type of government did soviet union have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government did soviet union have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.026623444631695747, -0.07251302152872086], "norm_scores": [0.511470377445221, 0.48852962255477905]}}
{"id": "WebQTest-638", "question": "what languages are there in japan", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what languages are there in japan [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.05153349041938782, -0.09697607904672623], "norm_scores": [0.5113586783409119, 0.48864129185676575]}}
{"id": "WebQTest-639", "question": "what year did the bulls get rodman", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.player_statistics", "sports.sports_team.roster"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the bulls get rodman [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.player_statistics <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.0446808747947216, -0.12921424210071564], "norm_scores": [0.5211207866668701, 0.47887924313545227]}}
{"id": "WebQTest-640", "question": "who was saint paul the apostle", "prediction": [["people.person.profession"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was saint paul the apostle [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.01766377128660679, -0.22775854170322418], "norm_scores": [0.5523313283920288, 0.4476686418056488]}}
{"id": "WebQTest-642", "question": "where did vince scully go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did vince scully go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.05065292865037918, -0.05991682782769203], "norm_scores": [0.5023159384727478, 0.4976840317249298]}}
{"id": "WebQTest-643", "question": "where does princess leia live", "prediction": [["fictional_universe.fictional_character.places_lived"], ["fictional_universe.fictional_character.place_of_birth"]], "ground_paths": [["fictional_universe.fictional_character.places_lived"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does princess leia live [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.places_lived </PATH>", "<PATH> fictional_universe.fictional_character.place_of_birth </PATH>"], "scores": [-0.011593608185648918, -0.10558588057756424], "norm_scores": [0.5234807729721069, 0.47651922702789307]}}
{"id": "WebQTest-644", "question": "what famous people are from el salvador", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what famous people are from el salvador [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.033125318586826324, -0.15168990194797516], "norm_scores": [0.5296064615249634, 0.4703935384750366]}}
{"id": "WebQTest-645", "question": "what type of art did leonardo da vinci make", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art did leonardo da vinci make [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.0008131581707857549, -0.3354611396789551], "norm_scores": [0.5828898549079895, 0.4171101152896881]}}
{"id": "WebQTest-646", "question": "where did mary bell live", "prediction": [["people.person.place_of_birth"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did mary bell live [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.06537766754627228, -0.11294014006853104], "norm_scores": [0.5118883848190308, 0.4881116449832916]}}
{"id": "WebQTest-648", "question": "who did kimberly williams marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["people.person.spouse_s", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["film.film.produced_by", "film.music_contributor.film"], ["people.person.parents", "people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did kimberly williams marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.048599086701869965, -0.05393014848232269], "norm_scores": [0.5013327598571777, 0.49866724014282227]}}
{"id": "WebQTest-649", "question": "what other movies has ryan gosling been in", "prediction": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["people.person.languages", "film.film.language"], ["film.film.executive_produced_by"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["film.actor.film", "film.film.starring"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.performance.actor", "film.film.starring"], ["people.person.nationality", "film.film.featured_film_locations"], ["film.actor.film", "film.performance.film"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.languages", "tv.tv_program.languages"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other movies has ryan gosling been in [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.06319756805896759, -0.07985331118106842], "norm_scores": [0.5041638016700745, 0.49583616852760315]}}
{"id": "WebQTest-650", "question": "what political party does julia gillard belong to", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"]], "ground_paths": [["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party does julia gillard belong to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>"], "scores": [-0.026127859950065613, -0.06124458461999893], "norm_scores": [0.5087782740592957, 0.49122172594070435]}}
{"id": "WebQTest-651", "question": "what is the currency of germany now", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency of germany now [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.040838275104761124, -0.10799282789230347], "norm_scores": [0.5167823433876038, 0.48321768641471863]}}
{"id": "WebQTest-652", "question": "what kind of hispanic is selena gomez", "prediction": [["people.ethnicity.included_in_group"], ["people.person.ethnicity"]], "ground_paths": [["people.person.ethnicity"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of hispanic is selena gomez [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.included_in_group </PATH>", "<PATH> people.person.ethnicity </PATH>"], "scores": [-0.04751375690102577, -0.15161707997322083], "norm_scores": [0.5260023474693298, 0.47399765253067017]}}
{"id": "WebQTest-653", "question": "who did denise van outen married", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["people.person.profession", "people.person.profession"], ["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.marriage.spouse"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did denise van outen married [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.040293771773576736, -0.08420905470848083], "norm_scores": [0.510977029800415, 0.4890229403972626]}}
{"id": "WebQTest-654", "question": "what are egyptian money called", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are egyptian money called [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.03768262267112732, -0.14509418606758118], "norm_scores": [0.5268270969390869, 0.4731729030609131]}}
{"id": "WebQTest-655", "question": "what is the zip code for wyoming michigan", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for wyoming michigan [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.006791939493268728, -0.2813076376914978], "norm_scores": [0.5682011246681213, 0.4317988157272339]}}
{"id": "WebQTest-656", "question": "who is the president of costa rica in 2012", "prediction": [["people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the president of costa rica in 2012 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.020108859986066818, -0.07040400058031082], "norm_scores": [0.5125711560249329, 0.4874288737773895]}}
{"id": "WebQTest-657", "question": "where did martin luther king got shot at", "prediction": [["people.deceased_person.place_of_death"], ["people.person.place_of_birth"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did martin luther king got shot at [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.0007822607876732945, -0.4952579140663147], "norm_scores": [0.6211602091789246, 0.37883979082107544]}}
{"id": "WebQTest-658", "question": "who wrote the gospel according to john", "prediction": [["book.book_subject.works"], ["book.author.works_written"]], "ground_paths": [["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who wrote the gospel according to john [/INST]", "raw_output": {"paths": ["<PATH> book.book_subject.works </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.07158815860748291, -0.12607763707637787], "norm_scores": [0.5136190056800842, 0.48638099431991577]}}
{"id": "WebQTest-659", "question": "what form of government is the uk", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government is the uk [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>"], "scores": [-0.029232794418931007, -0.06451240181922913], "norm_scores": [0.5088189840316772, 0.49118101596832275]}}
{"id": "WebQTest-661", "question": "who did nasri play for before arsenal", "prediction": [["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "ground_paths": [["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did nasri play for before arsenal [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_player_stats.team </PATH>"], "scores": [-0.10359005630016327, -0.10808000713586807], "norm_scores": [0.5011224746704102, 0.49887749552726746]}}
{"id": "WebQTest-662", "question": "what currency does the czech republic use now", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does the czech republic use now [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.07129915803670883, -0.10143057256937027], "norm_scores": [0.5075322985649109, 0.4924677014350891]}}
{"id": "WebQTest-663", "question": "what age did william penn get married", "prediction": [["people.person.spouse_s", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what age did william penn get married [/INST]", "raw_output": {"paths": ["<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>"], "scores": [-0.04563043266534805, -0.11051242053508759], "norm_scores": [0.5162147879600525, 0.4837851822376251]}}
{"id": "WebQTest-664", "question": "what was firefox programmed in", "prediction": [["film.film.story_by"], ["computer.software.language"]], "ground_paths": [["computer.software.languages_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was firefox programmed in [/INST]", "raw_output": {"paths": ["<PATH> film.film.story_by </PATH>", "<PATH> computer.software.language </PATH>"], "scores": [-0.26729458570480347, -0.31431496143341064], "norm_scores": [0.5117529034614563, 0.4882470965385437]}}
{"id": "WebQTest-665", "question": "what government does new zealand have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what government does new zealand have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>"], "scores": [-0.023422643542289734, -0.07719533890485764], "norm_scores": [0.5134398937225342, 0.48656004667282104]}}
{"id": "WebQTest-666", "question": "where is isthmus of panama located on the map", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is isthmus of panama located on the map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.006338576320558786, -0.4006928503513336], "norm_scores": [0.5973304510116577, 0.4026695787906647]}}
{"id": "WebQTest-667", "question": "what is the capital of republic of ireland", "prediction": [["location.location.containedby"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["location.country.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of republic of ireland [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.039314452558755875, -0.10385151952505112], "norm_scores": [0.516128659248352, 0.48387131094932556]}}
{"id": "WebQTest-668", "question": "what has kristen stewart acted in", "prediction": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["people.person.languages", "film.film.language"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["people.person.place_of_birth", "film.film.subjects"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.film.starring"], ["award.award_winner.awards_won", "award.award_honor.honored_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has kristen stewart acted in [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.07483147829771042, -0.08910955488681793], "norm_scores": [0.5035694241523743, 0.49643054604530334]}}
{"id": "WebQTest-669", "question": "what did dr josef mengele do", "prediction": [["people.person.profession"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did dr josef mengele do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.012869228608906269, -0.21051035821437836], "norm_scores": [0.549250066280365, 0.450749933719635]}}
{"id": "WebQTest-670", "question": "what kind of language does egypt speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language does egypt speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.08541742712259293, -0.11685022711753845], "norm_scores": [0.5078575611114502, 0.4921424686908722]}}
{"id": "WebQTest-671", "question": "who are the senators of hawaii 2012", "prediction": [["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the senators of hawaii 2012 [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.052525728940963745, -0.057335264980793], "norm_scores": [0.5012023448944092, 0.49879759550094604]}}
{"id": "WebQTest-672", "question": "what form of government does north korea have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does north korea have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.0365045964717865, -0.055491458624601364], "norm_scores": [0.5047465562820435, 0.49525341391563416]}}
{"id": "WebQTest-673", "question": "where did robert kardashian go to college", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did robert kardashian go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.04035608097910881, -0.06446625292301178], "norm_scores": [0.5060272216796875, 0.4939727485179901]}}
{"id": "WebQTest-675", "question": "what county is st james city fl in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st james city fl in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.07779692113399506, -0.12407739460468292], "norm_scores": [0.5115680694580078, 0.4884319603443146]}}
{"id": "WebQTest-676", "question": "what did the akkadian empire include", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did the akkadian empire include [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.06503583490848541, -0.19387663900852203], "norm_scores": [0.5321657061576843, 0.4678342938423157]}}
{"id": "WebQTest-677", "question": "after how many years are the olympic games held", "prediction": [["time.recurring_event.instances"], ["time.event.instance_of_recurring_event"]], "ground_paths": [["time.recurring_event.current_frequency"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: after how many years are the olympic games held [/INST]", "raw_output": {"paths": ["<PATH> time.recurring_event.instances </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>"], "scores": [-0.04666271060705185, -0.0627107247710228], "norm_scores": [0.5040119290351868, 0.49598804116249084]}}
{"id": "WebQTest-678", "question": "what system of government does guyana have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what system of government does guyana have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.026636328548192978, -0.07252807915210724], "norm_scores": [0.5114709138870239, 0.4885290563106537]}}
{"id": "WebQTest-680", "question": "what team did aguero play for", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"]], "ground_paths": [["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did aguero play for [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.matches_played <SEP> soccer.football_player_match_participation.team </PATH>"], "scores": [-0.055605895817279816, -0.06331048160791397], "norm_scores": [0.5019261240959167, 0.49807384610176086]}}
{"id": "WebQTest-681", "question": "who is the president of the european union 2012", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the president of the european union 2012 [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.02351382188498974, -0.0383014976978302], "norm_scores": [0.503696858882904, 0.49630317091941833]}}
{"id": "WebQTest-683", "question": "what year was kenya moore crowned miss usa", "prediction": [["people.person.nationality", "location.location.events"], ["award.competitor.competitions", "award.competitor.competitions"]], "ground_paths": [["time.recurring_event.instances"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was kenya moore crowned miss usa [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.events </PATH>", "<PATH> award.competitor.competitions <SEP> award.competitor.competitions </PATH>"], "scores": [-0.06483294814825058, -0.13852466642856598], "norm_scores": [0.5184146165847778, 0.48158541321754456]}}
{"id": "WebQTest-684", "question": "what sort of government does nigeria have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sort of government does nigeria have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.02088213339447975, -0.08674734085798264], "norm_scores": [0.5164603590965271, 0.4835396409034729]}}
{"id": "WebQTest-685", "question": "where does chef ina garten live", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does chef ina garten live [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.06522384285926819, -0.06971733272075653], "norm_scores": [0.5011233687400818, 0.4988766610622406]}}
{"id": "WebQTest-686", "question": "when the atlanta braves won the world series", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when the atlanta braves won the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.04252079874277115, -0.05633194372057915], "norm_scores": [0.503452718257904, 0.49654725193977356]}}
{"id": "WebQTest-688", "question": "what kind of language do they speak in iran", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language do they speak in iran [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.05131375044584274, -0.08934646844863892], "norm_scores": [0.5095070004463196, 0.49049296975135803]}}
{"id": "WebQTest-689", "question": "what was the first language spoken in spain", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the first language spoken in spain [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.07793623208999634, -0.08823354542255402], "norm_scores": [0.5025742650032043, 0.49742570519447327]}}
{"id": "WebQTest-690", "question": "what did scott joplin play", "prediction": [["music.group_member.instruments_played"], ["music.instrument.family"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did scott joplin play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.family </PATH>"], "scores": [-0.0076266867108643055, -0.4816651940345764], "norm_scores": [0.6163392066955566, 0.38366082310676575]}}
{"id": "WebQTest-691", "question": "what school did miley cyrus", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did miley cyrus [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.05897798016667366, -0.06842982023954391], "norm_scores": [0.5023629069328308, 0.4976370632648468]}}
{"id": "WebQTest-693", "question": "what language do most people speak in afghanistan", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do most people speak in afghanistan [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.059040989726781845, -0.08596771210432053], "norm_scores": [0.5067312717437744, 0.4932687282562256]}}
{"id": "WebQTest-694", "question": "who is the prime minister of jamaica west indies", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["people.person.nationality"]], "ground_paths": [["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the prime minister of jamaica west indies [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.057798635214567184, -0.061348218470811844], "norm_scores": [0.500887393951416, 0.499112606048584]}}
{"id": "WebQTest-695", "question": "what role did paul mccartney play in the beatles", "prediction": [["music.musical_group.member", "music.group_membership.role"], ["music.musical_group.member", "music.performance_role.regular_performances"]], "ground_paths": [["music.group_member.membership", "music.group_membership.role"], ["music.group_membership.member", "music.group_membership.role"], ["music.group_member.instruments_played", "music.instrument.family"], ["music.group_member.membership", "music.performance_role.regular_performances"], ["music.group_member.instruments_played", "music.instrument.variation"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what role did paul mccartney play in the beatles [/INST]", "raw_output": {"paths": ["<PATH> music.musical_group.member <SEP> music.group_membership.role </PATH>", "<PATH> music.musical_group.member <SEP> music.performance_role.regular_performances </PATH>"], "scores": [-0.0439264252781868, -0.06517714262008667], "norm_scores": [0.5053125023841858, 0.4946875274181366]}}
{"id": "WebQTest-696", "question": "when did juventus win the champions league", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did juventus win the champions league [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>"], "scores": [-0.019004398956894875, -0.09715728461742401], "norm_scores": [0.5195282697677612, 0.48047173023223877]}}
{"id": "WebQTest-698", "question": "who is robin roberts father", "prediction": [["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is robin roberts father [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.04700949043035507, -0.13687549531459808], "norm_scores": [0.5224514007568359, 0.47754859924316406]}}
{"id": "WebQTest-699", "question": "what is the capital of australia victoria state", "prediction": [["location.location.containedby"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["base.biblioness.bibs_location.state"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of australia victoria state [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.08396076411008835, -0.14284834265708923], "norm_scores": [0.5147176384925842, 0.48528236150741577]}}
{"id": "WebQTest-700", "question": "where is french spoken most", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["base.rosetta.local_name.locale"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is french spoken most [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.07747416943311691, -0.10356766730546951], "norm_scores": [0.5065230131149292, 0.4934769868850708]}}
{"id": "WebQTest-701", "question": "what team is reggie bush on 2011", "prediction": [["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"]], "ground_paths": [["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is reggie bush on 2011 [/INST]", "raw_output": {"paths": ["<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>"], "scores": [-0.04961451143026352, -0.05875096470117569], "norm_scores": [0.5022841095924377, 0.49771589040756226]}}
{"id": "WebQTest-702", "question": "what highschool did emily osment go to", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what highschool did emily osment go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.03873109444975853, -0.06241564825177193], "norm_scores": [0.5059208869934082, 0.4940791130065918]}}
{"id": "WebQTest-703", "question": "what currency should i take to mexico", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should i take to mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.028596242889761925, -0.14769351482391357], "norm_scores": [0.5297392010688782, 0.4702608287334442]}}
{"id": "WebQTest-704", "question": "who is thor heyerdahl", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is thor heyerdahl [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.007219547405838966, -0.27275586128234863], "norm_scores": [0.5659967660903931, 0.43400323390960693]}}
{"id": "WebQTest-706", "question": "where did troy aikman play football", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did troy aikman play football [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.030865423381328583, -0.03557301312685013], "norm_scores": [0.5011768937110901, 0.4988231062889099]}}
{"id": "WebQTest-707", "question": "who played cletus hogg", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played cletus hogg [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.057909850031137466, -0.06797000020742416], "norm_scores": [0.5025150179862976, 0.4974850118160248]}}
{"id": "WebQTest-708", "question": "who won fa cup 1976", "prediction": [["sports.sports_award_type.winners", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship", "sports.sports_team.championships"], ["sports.sports_award.award", "sports.sports_award_winner.awards"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won fa cup 1976 [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_type.winners <SEP> sports.sports_award.award_winner </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.045441608875989914, -0.054783567786216736], "norm_scores": [0.5023354887962341, 0.49766454100608826]}}
{"id": "WebQTest-709", "question": "what did general robert e lee die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did general robert e lee die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.005931751802563667, -0.16838307678699493], "norm_scores": [0.5405237674713135, 0.4594762623310089]}}
{"id": "WebQTest-710", "question": "what style of art did henri matisse do", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what style of art did henri matisse do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.005598940886557102, -0.12237697839736938], "norm_scores": [0.5291613936424255, 0.47083863615989685]}}
{"id": "WebQTest-711", "question": "who plays chuck bass in gossip girl", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"]], "ground_paths": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["base.gossipgirl.character.played_by"], ["award.award_winning_work.awards_won", "award.award_honor.award_winner"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays chuck bass in gossip girl [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.06528328359127045, -0.07350221276283264], "norm_scores": [0.5020546913146973, 0.49794527888298035]}}
{"id": "WebQTest-712", "question": "what countries have territories in oceania", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries have territories in oceania [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.046850644052028656, -0.22072526812553406], "norm_scores": [0.5433594584465027, 0.4566405117511749]}}
{"id": "WebQTest-713", "question": "what county is st. louis park in", "prediction": [["location.location.containedby"], ["location.us_county.county_seat"]], "ground_paths": [["location.hud_county_place.county"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st. louis park in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.county_seat </PATH>"], "scores": [-0.03474351763725281, -0.15558761358261108], "norm_scores": [0.5301743149757385, 0.46982571482658386]}}
{"id": "WebQTest-714", "question": "what is the political system of south africa", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the political system of south africa [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.04077660292387009, -0.05175216123461723], "norm_scores": [0.5027438402175903, 0.4972561299800873]}}
{"id": "WebQTest-716", "question": "what language group does germany belong to", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.language_family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language group does germany belong to [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.04635761305689812, -0.12544088065624237], "norm_scores": [0.5197604894638062, 0.48023948073387146]}}
{"id": "WebQTest-717", "question": "what timezone is tucson arizona in right now", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is tucson arizona in right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-4.537015047390014e-05, -0.8871192336082458], "norm_scores": [0.708285927772522, 0.29171404242515564]}}
{"id": "WebQTest-718", "question": "when was lucy lawless born", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was lucy lawless born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0029919459484517574, -0.3896515965461731], "norm_scores": [0.595478355884552, 0.4045216739177704]}}
{"id": "WebQTest-719", "question": "which political party does barack obama represent", "prediction": [["government.politician.party", "government.political_party_tenure.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "ground_paths": [["government.politician.election_campaigns", "government.election_campaign.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"], ["government.election.winner", "government.primary_election.party"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which political party does barack obama represent [/INST]", "raw_output": {"paths": ["<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>"], "scores": [-0.052116312086582184, -0.0672575831413269], "norm_scores": [0.503785252571106, 0.49621477723121643]}}
{"id": "WebQTest-721", "question": "what kind of leukemia did jill clayburgh have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of leukemia did jill clayburgh have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>"], "scores": [-0.03833910450339317, -0.04322483763098717], "norm_scores": [0.5012214183807373, 0.4987785518169403]}}
{"id": "WebQTest-723", "question": "what is there to see in sydney australia", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is there to see in sydney australia [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>"], "scores": [-0.04796094074845314, -0.05496259033679962], "norm_scores": [0.5017504096031189, 0.4982495903968811]}}
{"id": "WebQTest-724", "question": "where was marilyn monroe found dead", "prediction": [["people.deceased_person.place_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was marilyn monroe found dead [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.0010613788617774844, -0.3486674129962921], "norm_scores": [0.5860369205474854, 0.41396307945251465]}}
{"id": "WebQTest-725", "question": "what states does the sierra nevada run through", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "ground_paths": [["location.location.partially_contains"], ["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what states does the sierra nevada run through [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.03120870143175125, -0.13631205260753632], "norm_scores": [0.5262516736984253, 0.4737483263015747]}}
{"id": "WebQTest-726", "question": "what language is spoken in greek", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is spoken in greek [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.05099955201148987, -0.08052462339401245], "norm_scores": [0.5073807239532471, 0.49261924624443054]}}
{"id": "WebQTest-728", "question": "what is the name of the speaker of the house of representatives now 2011", "prediction": [["government.government_office_or_title.office_holders", "government.government_position_held.office_holder"], ["common.topic.image", "common.topic.image"]], "ground_paths": [["common.topic.image", "common.image.appears_in_topic_gallery"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the speaker of the house of representatives now 2011 [/INST]", "raw_output": {"paths": ["<PATH> government.government_office_or_title.office_holders <SEP> government.government_position_held.office_holder </PATH>", "<PATH> common.topic.image <SEP> common.topic.image </PATH>"], "scores": [-0.039574652910232544, -0.0484623983502388], "norm_scores": [0.502221941947937, 0.497778058052063]}}
{"id": "WebQTest-729", "question": "who plays stanley hudson", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays stanley hudson [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.06112069636583328, -0.1140638142824173], "norm_scores": [0.5132327079772949, 0.48676732182502747]}}
{"id": "WebQTest-730", "question": "what record label is kelly clarkson signed with", "prediction": [["music.artist.label"], ["music.record_label.artist"]], "ground_paths": [["music.artist.label"], ["music.record_label.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what record label is kelly clarkson signed with [/INST]", "raw_output": {"paths": ["<PATH> music.artist.label </PATH>", "<PATH> music.record_label.artist </PATH>"], "scores": [-0.011094470508396626, -0.2970285415649414], "norm_scores": [0.5710003972053528, 0.42899957299232483]}}
{"id": "WebQTest-731", "question": "where does tim roth come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does tim roth come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.02851189859211445, -0.14382131397724152], "norm_scores": [0.5287954807281494, 0.471204549074173]}}
{"id": "WebQTest-733", "question": "what currency should you take to vietnam", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"]], "ground_paths": [["base.monetarydebase.debased_currency.issuing_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should you take to vietnam [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.051159605383872986, -0.08451025187969208], "norm_scores": [0.5083369016647339, 0.4916630983352661]}}
{"id": "WebQTest-734", "question": "who did roger federer married", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "ground_paths": [["people.marriage.spouse", "people.marriage.spouse"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.parents"], ["people.person.profession", "people.person.profession"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did roger federer married [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>"], "scores": [-0.05920298770070076, -0.08240626007318497], "norm_scores": [0.5058005452156067, 0.4941994249820709]}}
{"id": "WebQTest-735", "question": "what kind of money do i need in costa rica", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money do i need in costa rica [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.030879346653819084, -0.1183534637093544], "norm_scores": [0.5218545794487, 0.47814539074897766]}}
{"id": "WebQTest-737", "question": "what is the name of the capital of belgium", "prediction": [["location.country.capital"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["location.country.capital"], ["base.biblioness.bibs_location.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the capital of belgium [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.07083263993263245, -0.08473187685012817], "norm_scores": [0.503474771976471, 0.49652522802352905]}}
{"id": "WebQTest-738", "question": "what kind of government does japan have now", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does japan have now [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>"], "scores": [-0.03798767179250717, -0.052420832216739655], "norm_scores": [0.503608226776123, 0.49639177322387695]}}
{"id": "WebQTest-739", "question": "what pieces of music did claude debussy compose", "prediction": [["music.composition.composer"], ["music.artist.genre"]], "ground_paths": [["music.recording.artist"], ["book.written_work.author"], ["book.author.works_written"], ["music.lyricist.lyrics_written"], ["opera.opera.librettist"], ["music.composition.composer"], ["music.composition.lyricist"], ["book.author.works_written", "book.book.editions"], ["opera.librettist.libretti"], ["base.musiteca.music_teacher.composition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what pieces of music did claude debussy compose [/INST]", "raw_output": {"paths": ["<PATH> music.composition.composer </PATH>", "<PATH> music.artist.genre </PATH>"], "scores": [-0.04110819101333618, -0.18743810057640076], "norm_scores": [0.536517322063446, 0.46348264813423157]}}
{"id": "WebQTest-740", "question": "who was selena gomez in barney and friends", "prediction": [["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["people.person.gender", "fictional_universe.fictional_character.gender"]], "ground_paths": [["tv.tv_guest_role.actor", "tv.tv_guest_role.character"], ["tv.tv_guest_role.actor", "tv.tv_character.appeared_in_tv_episodes"], ["tv.tv_actor.guest_roles", "tv.tv_guest_role.character"], ["tv.tv_actor.guest_roles", "tv.tv_character.appeared_in_tv_episodes"], ["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["tv.tv_program.regular_cast", "tv.tv_character.appeared_in_tv_program"], ["film.actor.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was selena gomez in barney and friends [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>"], "scores": [-0.06907925754785538, -0.07266314327716827], "norm_scores": [0.5008959770202637, 0.49910402297973633]}}
{"id": "WebQTest-741", "question": "who plays caesar flickerman in the hunger games", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["film.film.starring", "film.performance.actor"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays caesar flickerman in the hunger games [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.07891042530536652, -0.09315817058086395], "norm_scores": [0.5035618543624878, 0.4964381456375122]}}
{"id": "WebQTest-742", "question": "where did the gettysburg address", "prediction": [["book.written_work.subjects"], ["book.book_subject.works"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the gettysburg address [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects </PATH>", "<PATH> book.book_subject.works </PATH>"], "scores": [-0.058061204850673676, -0.2514416575431824], "norm_scores": [0.5481950044631958, 0.4518049657344818]}}
{"id": "WebQTest-743", "question": "what was the name of jfk 's brothers", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_no_value"], ["people.person.ethnicity", "people.person.ethnicity"], ["people.person.religion", "people.person.religion"], ["people.person.parents", "people.person.parents"], ["film.film_subject.films", "film.film.subjects"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["people.deceased_person.cause_of_death", "people.deceased_person.cause_of_death"], ["people.person.nationality", "people.person.nationality"], ["people.family.members", "people.family.members"], ["government.government_position_held.appointed_by", "government.politician.government_positions_held"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["base.activism.activist.area_of_activism", "base.activism.activist.area_of_activism"], ["people.person.profession", "people.person.profession"], ["people.family.members", "people.family_member.family"], ["people.person.gender", "people.person.gender"], ["people.person.parents", "people.person.children"], ["people.person.sibling_s", "people.sibling_relationship.sibling"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["military.military_person.participated_in_conflicts", "military.military_person.participated_in_conflicts"], ["base.americancomedy.impersonated_celebrity.impersonated_by", "base.americancomedy.impersonated_celebrity.impersonated_by"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the name of jfk 's brothers [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>"], "scores": [-0.07973367720842361, -0.09606489539146423], "norm_scores": [0.5040827393531799, 0.49591729044914246]}}
{"id": "WebQTest-744", "question": "who was tupac signed to", "prediction": [["music.artist.label"], ["music.record_label.artist"]], "ground_paths": [["music.artist.label"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was tupac signed to [/INST]", "raw_output": {"paths": ["<PATH> music.artist.label </PATH>", "<PATH> music.record_label.artist </PATH>"], "scores": [-0.05237279087305069, -0.11417729407548904], "norm_scores": [0.5154462456703186, 0.48455381393432617]}}
{"id": "WebQTest-745", "question": "what was augustus caesar famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was augustus caesar famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.027387509122490883, -0.2157607227563858], "norm_scores": [0.5469545722007751, 0.45304548740386963]}}
{"id": "WebQTest-747", "question": "what language does colombia", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does colombia [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.04536188021302223, -0.06806304305791855], "norm_scores": [0.5056750774383545, 0.4943249821662903]}}
{"id": "WebQTest-748", "question": "what is the zip code for schenectady new york", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for schenectady new york [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0023528237361460924, -0.37316346168518066], "norm_scores": [0.5916548371315002, 0.40834519267082214]}}
{"id": "WebQTest-749", "question": "who was president in 1988 in the united states", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_office_or_title.jurisdiction", "people.person.nationality"]], "ground_paths": [["freebase.type_profile.equivalent_topic", "common.topic.notable_types"], ["government.government_office_or_title.jurisdiction", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was president in 1988 in the united states [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_office_or_title.jurisdiction <SEP> people.person.nationality </PATH>"], "scores": [-0.050673648715019226, -0.06927767395973206], "norm_scores": [0.504650890827179, 0.49534913897514343]}}
{"id": "WebQTest-751", "question": "who owns google 2012", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "ground_paths": [["organization.organization.leadership", "business.board_member.leader_of"], ["organization.organization_founder.organizations_founded"], ["organization.leadership.organization", "business.board_member.leader_of"], ["conferences.conference.focus", "time.event.people_involved"], ["organization.leadership.organization", "organization.leadership.person"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who owns google 2012 [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.founders </PATH>"], "scores": [-0.07278310507535934, -0.11010768264532089], "norm_scores": [0.5093300342559814, 0.49066993594169617]}}
{"id": "WebQTest-752", "question": "what drink did john pemberton create", "prediction": [["food.beer_country_region.beers_from_here"], ["food.beer_country_region.beers_from_here", "food.beer_country_region.beers_from_here"]], "ground_paths": [["law.inventor.inventions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what drink did john pemberton create [/INST]", "raw_output": {"paths": ["<PATH> food.beer_country_region.beers_from_here </PATH>", "<PATH> food.beer_country_region.beers_from_here <SEP> food.beer_country_region.beers_from_here </PATH>"], "scores": [-0.027803370729088783, -0.05923493579030037], "norm_scores": [0.5078572034835815, 0.49214276671409607]}}
{"id": "WebQTest-753", "question": "what position did stanley matthews play", "prediction": [["sports.pro_athlete.teams", "sports.sports_position.teams"], ["sports.sports_position.sport", "sports.sports_team.sport"]], "ground_paths": [["soccer.football_player.position_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what position did stanley matthews play [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_position.teams </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>"], "scores": [-0.08736464381217957, -0.10500603914260864], "norm_scores": [0.5044102072715759, 0.4955897629261017]}}
{"id": "WebQTest-755", "question": "what was bonnie prince charlie full name", "prediction": [["people.person.nationality"], ["common.topic.notable_types"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was bonnie prince charlie full name [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.040611881762742996, -0.2546349763870239], "norm_scores": [0.5533024668693542, 0.44669753313064575]}}
{"id": "WebQTest-756", "question": "who makes the voice of stewie griffin", "prediction": [["fictional_universe.fictional_character_creator.fictional_characters_created"], ["fictional_universe.fictional_character.character_created_by"]], "ground_paths": [["fictional_universe.fictional_character.character_created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who makes the voice of stewie griffin [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>"], "scores": [-0.050609927624464035, -0.06772289425134659], "norm_scores": [0.5042781233787537, 0.49572184681892395]}}
{"id": "WebQTest-757", "question": "who is angelina jolie mom and dad", "prediction": [["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is angelina jolie mom and dad [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.06441628932952881, -0.10567651689052582], "norm_scores": [0.5103135704994202, 0.48968639969825745]}}
{"id": "WebQTest-758", "question": "who is the governor of indiana 2009", "prediction": [["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"]], "ground_paths": [["common.topic.image"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the governor of indiana 2009 [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.040516264736652374, -0.059440381824970245], "norm_scores": [0.5047308802604675, 0.4952690899372101]}}
{"id": "WebQTest-759", "question": "who brad pitt has dated", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["base.popstra.celebrity.canoodled", "base.popstra.celebrity.canoodled"], ["base.popstra.celebrity.insult_victim", "base.popstra.public_insult.perpetrator"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["base.popstra.lived_with.participant", "base.popstra.celebrity.lived_with"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["people.person.profession", "people.person.profession"], ["common.topic.notable_types", "common.topic.notable_types"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["people.person.nationality", "people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who brad pitt has dated [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.03828848525881767, -0.1068510189652443], "norm_scores": [0.517133891582489, 0.4828660786151886]}}
{"id": "WebQTest-760", "question": "what party does barack obama belong to", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"]], "ground_paths": [["government.politician.election_campaigns", "government.election_campaign.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"], ["government.election.winner", "government.primary_election.party"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what party does barack obama belong to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>"], "scores": [-0.05289021134376526, -0.054430149495601654], "norm_scores": [0.5003849864006042, 0.49961501359939575]}}
{"id": "WebQTest-761", "question": "what is san francisco zip code", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is san francisco zip code [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0010461413767188787, -0.4493018686771393], "norm_scores": [0.6102244257926941, 0.3897755742073059]}}
{"id": "WebQTest-762", "question": "who has stephen ireland played for", "prediction": [["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "ground_paths": [["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has stephen ireland played for [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_player_stats.team </PATH>"], "scores": [-0.09018691629171371, -0.1021847203373909], "norm_scores": [0.5029994249343872, 0.4970006048679352]}}
{"id": "WebQTest-764", "question": "where did john mayer go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did john mayer go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.0514044426381588, -0.05851681903004646], "norm_scores": [0.5017781257629395, 0.4982219338417053]}}
{"id": "WebQTest-765", "question": "what celebrities has brody jenner dated", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["base.popstra.breakup.participant", "base.popstra.breakup.participant"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["base.popstra.celebrity.canoodled", "base.popstra.celebrity.canoodled"], ["base.popstra.infidelity.perpetrator", "base.popstra.infidelity.participant"], ["base.popstra.breakup.participant", "base.popstra.celebrity.breakup"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["people.person.profession", "people.person.profession"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.popstra.canoodled.participant", "base.popstra.celebrity.canoodled"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["base.popstra.celebrity.canoodled", "base.popstra.canoodled.participant"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.friendship.participant", "base.popstra.friendship.participant"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["people.person.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what celebrities has brody jenner dated [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.04147287830710411, -0.10644877701997757], "norm_scores": [0.516238272190094, 0.483761727809906]}}
{"id": "WebQTest-766", "question": "what is malaysian currency", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is malaysian currency [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.034731362015008926, -0.10305175930261612], "norm_scores": [0.5170734524726868, 0.48292651772499084]}}
{"id": "WebQTest-768", "question": "who owns volkswagen car company", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "ground_paths": [["business.business_operation.industry", "business.business_operation.industry"], ["base.schemastaging.corresponding_entity.entities", "base.schemastaging.entity_hint.corresponds_to"], ["organization.organization.parent", "organization.organization_relationship.parent"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who owns volkswagen car company [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.founders </PATH>"], "scores": [-0.0878317579627037, -0.14775358140468597], "norm_scores": [0.5149759650230408, 0.48502403497695923]}}
{"id": "WebQTest-769", "question": "which is the main train station in rome", "prediction": [["location.location.containedby"], ["trains.travel_destination.tourist_attractions"]], "ground_paths": [["location.location.containedby", "location.location.containedby"], ["travel.travel_destination.how_to_get_here", "travel.transportation.transport_terminus"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which is the main train station in rome [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> trains.travel_destination.tourist_attractions </PATH>"], "scores": [-0.01549512892961502, -0.24624814093112946], "norm_scores": [0.5574336051940918, 0.4425663650035858]}}
{"id": "WebQTest-770", "question": "what did steve mcnair died from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did steve mcnair died from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.0034432471729815006, -0.20875601470470428], "norm_scores": [0.5511486530303955, 0.4488513469696045]}}
{"id": "WebQTest-771", "question": "who is the newly elected governor of california", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the newly elected governor of california [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.04325415566563606, -0.04797735810279846], "norm_scores": [0.5011808276176453, 0.4988192319869995]}}
{"id": "WebQTest-772", "question": "who plays roxanne in ghost rider", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"]], "ground_paths": [["film.film.starring", "film.performance.actor"], ["film.film.language", "people.person.languages"], ["film.performance.film", "film.performance.actor"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays roxanne in ghost rider [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>"], "scores": [-0.07927394658327103, -0.09121311455965042], "norm_scores": [0.5029847621917725, 0.4970152676105499]}}
{"id": "WebQTest-773", "question": "where is the sony ericsson company", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the sony ericsson company [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>"], "scores": [-0.015759916976094246, -0.09139849990606308], "norm_scores": [0.5189006328582764, 0.48109936714172363]}}
{"id": "WebQTest-774", "question": "what kind of currency does the dominican republic have", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of currency does the dominican republic have [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.0003457650018390268, -0.5067456364631653], "norm_scores": [0.6239621043205261, 0.3760378658771515]}}
{"id": "WebQTest-775", "question": "what to do this weekend in manchester with kids", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do this weekend in manchester with kids [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.04412712901830673, -0.05661192536354065], "norm_scores": [0.5031211376190186, 0.49687883257865906]}}
{"id": "WebQTest-776", "question": "what sport is popular in italy", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"]], "ground_paths": [["cricket.cricket_team.country"], ["sports.sports_team.location"], ["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sport is popular in italy [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>"], "scores": [-0.04817738011479378, -0.055209461599588394], "norm_scores": [0.5017579793930054, 0.49824196100234985]}}
{"id": "WebQTest-777", "question": "where is the main headquarters of google", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["location.location.containedby"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.phone_number", "common.phone_number.service_location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the main headquarters of google [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.04668687656521797, -0.1310378462076187], "norm_scores": [0.5210752487182617, 0.4789247214794159]}}
{"id": "WebQTest-778", "question": "what was jesse owens education", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was jesse owens education [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.03825739398598671, -0.06345627456903458], "norm_scores": [0.5062993764877319, 0.4937005937099457]}}
{"id": "WebQTest-779", "question": "where does carlos santana live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["music.artist.origin", "location.location.containedby"], ["people.person.children", "people.person.place_of_birth"], ["people.person.nationality", "location.administrative_division.country"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.country.first_level_divisions"], ["people.person.nationality", "location.location.containedby"], ["symbols.namesake.named_after", "music.artist.origin"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does carlos santana live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.0600491538643837, -0.07443755865097046], "norm_scores": [0.50359708070755, 0.49640294909477234]}}
{"id": "WebQTest-780", "question": "what is the new movie john carter about", "prediction": [["film.film_story_contributor.film_story_credits"], ["film.film.subjects"]], "ground_paths": [["film.film.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the new movie john carter about [/INST]", "raw_output": {"paths": ["<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> film.film.subjects </PATH>"], "scores": [-0.07431446015834808, -0.12799113988876343], "norm_scores": [0.5134159326553345, 0.4865840673446655]}}
{"id": "WebQTest-782", "question": "what countries does nafta include", "prediction": [["organization.membership_organization.members", "organization.organization_member.member_of"], ["organization.organization_membership.organization", "organization.organization_member.member_of"]], "ground_paths": [["organization.organization_founder.organizations_founded", "people.person.nationality"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["organization.membership_organization.members", "organization.organization_member.member_of"], ["organization.membership_organization.members", "organization.organization_membership.member"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does nafta include [/INST]", "raw_output": {"paths": ["<PATH> organization.membership_organization.members <SEP> organization.organization_member.member_of </PATH>", "<PATH> organization.organization_membership.organization <SEP> organization.organization_member.member_of </PATH>"], "scores": [-0.02808861806988716, -0.06522275507450104], "norm_scores": [0.5092824101448059, 0.4907175302505493]}}
{"id": "WebQTest-783", "question": "what county is houston tx in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"]], "ground_paths": [["location.hud_county_place.county"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is houston tx in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.021102165803313255, -0.17887897789478302], "norm_scores": [0.5393625497817993, 0.4606374204158783]}}
{"id": "WebQTest-784", "question": "where do the seattle seahawks play", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the seattle seahawks play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>"], "scores": [-0.034145090728998184, -0.06553942710161209], "norm_scores": [0.5078479647636414, 0.49215206503868103]}}
{"id": "WebQTest-785", "question": "who does michael keaton play in cars", "prediction": [["film.performance.actor", "film.performance.character"], ["film.performance.film", "film.performance.character"]], "ground_paths": [["fictional_universe.fictional_universe.works_set_here", "fictional_universe.fictional_universe.characters"], ["film.performance.film", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.film_character.portrayed_in_films"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does michael keaton play in cars [/INST]", "raw_output": {"paths": ["<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>"], "scores": [-0.10436642915010452, -0.11616449803113937], "norm_scores": [0.5029494762420654, 0.49705052375793457]}}
{"id": "WebQTest-787", "question": "where is the riviera casino", "prediction": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the riviera casino [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.04551045596599579, -0.1159604862332344], "norm_scores": [0.5176052451133728, 0.4823947548866272]}}
{"id": "WebQTest-788", "question": "what is the holy book of the buddhist religion", "prediction": [["religion.religion.texts"], ["religion.religious_text.religious_text_of"]], "ground_paths": [["religion.religion.texts"], ["religion.religious_text.religious_text_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the holy book of the buddhist religion [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.texts </PATH>", "<PATH> religion.religious_text.religious_text_of </PATH>"], "scores": [-0.020933691412210464, -0.08832253515720367], "norm_scores": [0.5168408155441284, 0.4831591546535492]}}
{"id": "WebQTest-789", "question": "who became president when henry harrison died", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who became president when henry harrison died [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>"], "scores": [-0.0783572643995285, -0.09325592964887619], "norm_scores": [0.5037246346473694, 0.496275395154953]}}
{"id": "WebQTest-790", "question": "where did richard arkwright grow up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did richard arkwright grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.007147093303501606, -0.2055252194404602], "norm_scores": [0.5494325160980225, 0.45056745409965515]}}
{"id": "WebQTest-791", "question": "what other books did louis sachar write", "prediction": [["book.book_edition.author_editor"], ["book.author.works_written"]], "ground_paths": [["book.book_edition.author_editor"], ["book.written_work.author"], ["book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other books did louis sachar write [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.054662108421325684, -0.14422720670700073], "norm_scores": [0.522376298904419, 0.47762367129325867]}}
{"id": "WebQTest-792", "question": "when did roth ira originate", "prediction": [["finance.currency.countries_used"], ["common.topic.notable_types"]], "ground_paths": [["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did roth ira originate [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.06182148680090904, -0.14488352835178375], "norm_scores": [0.5207535624504089, 0.4792464077472687]}}
{"id": "WebQTest-794", "question": "who does the voice for darth vader", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does the voice for darth vader [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.06654435396194458, -0.08951741456985474], "norm_scores": [0.5057430267333984, 0.49425700306892395]}}
{"id": "WebQTest-796", "question": "what did thomas malthus do", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did thomas malthus do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.010279239155352116, -0.26987746357917786], "norm_scores": [0.564537525177002, 0.43546247482299805]}}
{"id": "WebQTest-797", "question": "what is the zip code for newark california", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for newark california [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0006279848166741431, -0.5108355283737183], "norm_scores": [0.6248551607131958, 0.3751448690891266]}}
{"id": "WebQTest-798", "question": "what states share a border with tennessee", "prediction": [["meteorology.cyclone_affected_area.cyclones", "meteorology.tropical_cyclone.affected_areas"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.cyclone_affected_area.cyclones"]], "ground_paths": [["base.locations.countries.states_provinces_within", "location.country.first_level_divisions"], ["location.location.containedby", "location.location.containedby"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["time.event.locations", "location.location.events"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.tropical_cyclone.affected_areas"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.partially_containedby", "location.location.partially_contains"], ["base.locations.countries.states_provinces_within", "base.aareas.schema.administrative_area.administrative_children"], ["periodicals.newspaper_circulation_area.newspapers", "book.newspaper.circulation_areas"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.tropical_cyclone.affected_areas"], ["location.location.containedby", "location.location.contains"], ["location.location.partially_contains", "geography.river.origin"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.events", "time.event.locations"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["base.locations.countries.states_provinces_within", "base.aareas.schema.administrative_area.administrative_parent"], ["base.locations.countries.states_provinces_within", "base.locations.countries.states_provinces_within"], ["location.location.events", "location.location.events"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.containedby", "base.usnationalparks.us_national_park.state"], ["book.written_work.subjects", "book.written_work.subjects"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["base.locations.countries.states_provinces_within", "location.administrative_division.country"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["base.locations.countries.states_provinces_within", "location.administrative_division.first_level_division_of"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.cyclone_affected_area.cyclones"], ["book.newspaper.circulation_areas", "periodicals.newspaper_circulation_area.newspapers"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["time.event.locations", "time.event.locations"], ["location.location.partially_contains", "location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what states share a border with tennessee [/INST]", "raw_output": {"paths": ["<PATH> meteorology.cyclone_affected_area.cyclones <SEP> meteorology.tropical_cyclone.affected_areas </PATH>", "<PATH> meteorology.cyclone_affected_area.cyclones <SEP> meteorology.cyclone_affected_area.cyclones </PATH>"], "scores": [-0.0735662654042244, -0.07481184601783752], "norm_scores": [0.5003113746643066, 0.4996885657310486]}}
{"id": "WebQTest-800", "question": "what war did the us lose the most soldiers", "prediction": [["base.culturalevent.event.entity_involved"], ["military.military_conflict.casualties", "military.casualties.military_conflict"]], "ground_paths": [["military.military_command.military_combatant", "military.military_conflict.commanders"], ["military.military_combatant.military_commanders", "military.military_conflict.commanders"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"], ["people.person.nationality", "military.military_person.participated_in_conflicts"], ["military.military_combatant.casualties", "military.casualties.military_conflict"], ["location.location.partially_containedby", "location.location.events"], ["military.military_command.military_combatant", "military.military_command.military_conflict"], ["military.military_combatant.military_commanders", "military.military_command.military_conflict"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what war did the us lose the most soldiers [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.casualties.military_conflict </PATH>"], "scores": [-0.053273629397153854, -0.0709979385137558], "norm_scores": [0.5044309496879578, 0.49556905031204224]}}
{"id": "WebQTest-801", "question": "what kind of government is egypt", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government is egypt [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.021177494898438454, -0.08598601818084717], "norm_scores": [0.5161964297294617, 0.48380354046821594]}}
{"id": "WebQTest-802", "question": "what musical instruments does justin bieber play", "prediction": [["music.artist.track_contributions", "music.track_contribution.role"], ["music.group_member.instruments_played"]], "ground_paths": [["music.group_member.membership", "music.group_membership.role"], ["music.artist.track_contributions", "music.track_contribution.role"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what musical instruments does justin bieber play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>", "<PATH> music.group_member.instruments_played </PATH>"], "scores": [-0.010979089885950089, -0.1769639551639557], "norm_scores": [0.541401207447052, 0.4585988223552704]}}
{"id": "WebQTest-805", "question": "where do john lennon die", "prediction": [["people.deceased_person.place_of_death"], ["location.deceased_person.place_of_death"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do john lennon die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>"], "scores": [-0.002295907586812973, -0.2718304395675659], "norm_scores": [0.5669786334037781, 0.4330213665962219]}}
{"id": "WebQTest-809", "question": "what is the current time in bangalore india", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the current time in bangalore india [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>"], "scores": [-0.000921981583815068, -0.24489955604076385], "norm_scores": [0.560693621635437, 0.439306378364563]}}
{"id": "WebQTest-810", "question": "what language do people from greece speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people from greece speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.05212390050292015, -0.06529834866523743], "norm_scores": [0.5032935738563538, 0.49670642614364624]}}
{"id": "WebQTest-811", "question": "where is the phillies spring training stadium", "prediction": [["sports.sports_team.arena_stadium"], ["location.location.containedby"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the phillies spring training stadium [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.012926251627504826, -0.2697925567626953], "norm_scores": [0.5638658404350281, 0.4361342191696167]}}
{"id": "WebQTest-812", "question": "what movies does matt dallas play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.film.starring"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies does matt dallas play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.04669320955872536, -0.1352672129869461], "norm_scores": [0.5221290588378906, 0.47787097096443176]}}
{"id": "WebQTest-813", "question": "who was john f kennedy 's family", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_no_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "ground_paths": [["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_no_value"], ["people.person.ethnicity", "people.person.ethnicity"], ["people.person.religion", "people.person.religion"], ["symbols.name_source.namesakes"], ["people.person.parents", "people.person.parents"], ["film.film_subject.films", "film.film.subjects"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["people.deceased_person.cause_of_death", "people.deceased_person.cause_of_death"], ["people.person.nationality", "people.person.nationality"], ["people.family.members", "people.family.members"], ["government.government_position_held.appointed_by", "government.politician.government_positions_held"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["base.activism.activist.area_of_activism", "base.activism.activist.area_of_activism"], ["people.person.profession", "people.person.profession"], ["people.sibling_relationship.sibling", "people.person.sibling_s"], ["people.person.profession", "common.topic.notable_types"], ["people.person.parents"], ["people.family.members", "people.family_member.family"], ["people.person.gender", "people.person.gender"], ["people.person.parents", "people.person.children"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["military.military_person.participated_in_conflicts", "military.military_person.participated_in_conflicts"], ["base.americancomedy.impersonated_celebrity.impersonated_by", "base.americancomedy.impersonated_celebrity.impersonated_by"], ["people.person.sibling_s", "people.sibling_relationship.sibling"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was john f kennedy 's family [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_no_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>"], "scores": [-0.08034559339284897, -0.08130917698144913], "norm_scores": [0.5002409219741821, 0.49975910782814026]}}
{"id": "WebQTest-814", "question": "where is harley davidson corporate headquarters", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"]], "ground_paths": [["organization.organization.place_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is harley davidson corporate headquarters [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>"], "scores": [-0.0158152524381876, -0.11332076042890549], "norm_scores": [0.5243570804595947, 0.4756429195404053]}}
{"id": "WebQTest-818", "question": "where was the ottoman empire based", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the ottoman empire based [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.03620981425046921, -0.25784832239151], "norm_scores": [0.5551838874816895, 0.44481611251831055]}}
{"id": "WebQTest-819", "question": "where was bin laden found and killed", "prediction": [["base.culturalevent.event.entity_involved", "time.event.locations"], ["base.culturalevent.event.entity_involved", "location.location.events"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was bin laden found and killed [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>"], "scores": [-0.05656789243221283, -0.0970773696899414], "norm_scores": [0.510125994682312, 0.4898740351200104]}}
{"id": "WebQTest-820", "question": "what are the names of harry potter movies in order", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes"], ["fictional_universe.fictional_universe.works_set_here"]], "ground_paths": [["film.film_series.films_in_series"], ["fictional_universe.fictional_universe.works_set_here"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes"], ["film.film.film_series"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the names of harry potter movies in order [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes </PATH>", "<PATH> fictional_universe.fictional_universe.works_set_here </PATH>"], "scores": [-0.016952145844697952, -0.1034814864397049], "norm_scores": [0.5216188430786133, 0.4783811569213867]}}
{"id": "WebQTest-821", "question": "what did pamela courson die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did pamela courson die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.007742827292531729, -0.1495538204908371], "norm_scores": [0.535393476486206, 0.46460655331611633]}}
{"id": "WebQTest-822", "question": "what did dr jack kevorkian do", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["base.activism.activist.area_of_activism"], ["base.activism.activism_issue.activists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did dr jack kevorkian do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.01030831690877676, -0.24090147018432617], "norm_scores": [0.5573942065238953, 0.4426058232784271]}}
{"id": "WebQTest-823", "question": "what does bolivia border", "prediction": [["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.partially_containedby", "location.location.partially_contains"]], "ground_paths": [["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.containedby", "location.location.containedby"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["time.event.locations", "location.location.events"], ["location.location.partially_contains", "geography.river.basin_countries"], ["location.country.form_of_government", "location.country.form_of_government"], ["people.deceased_person.place_of_death", "people.person.nationality"], ["location.location.contains", "location.location.containedby"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["military.military_combatant_group.combatants", "military.military_combatant.military_conflicts"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["location.location.partially_containedby", "location.location.partially_contains"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["location.country.official_language", "language.human_language.main_country"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.events", "time.event.locations"], ["location.country.languages_spoken", "location.country.official_language"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["location.location.contains", "location.location.contains"], ["location.location.events", "location.location.events"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.partially_contains", "location.location.partially_contains"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"], ["language.language_family.geographic_distribution", "language.human_language.main_country"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["symbols.name_source.namesakes", "people.person.nationality"], ["location.country.official_language", "location.country.languages_spoken"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["language.language_family.geographic_distribution", "location.country.official_language"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["location.location.containedby", "location.location.contains_major_portion_of"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["location.country.form_of_government", "government.form_of_government.countries"], ["location.country.official_language", "language.human_language.countries_spoken_in"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["time.event.locations", "time.event.locations"], ["location.location.partially_contains", "location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does bolivia border [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>"], "scores": [-0.11509593576192856, -0.12982052564620972], "norm_scores": [0.5036810636520386, 0.49631890654563904]}}
{"id": "WebQTest-825", "question": "who does nolan ryan play for", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does nolan ryan play for [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>"], "scores": [-0.021891696378588676, -0.047341980040073395], "norm_scores": [0.50636225938797, 0.4936377704143524]}}
{"id": "WebQTest-827", "question": "what country does justin bieber come from", "prediction": [["people.person.nationality"], ["music.artist.origin"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country does justin bieber come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.022166676819324493, -0.25036686658859253], "norm_scores": [0.5568037629127502, 0.44319623708724976]}}
{"id": "WebQTest-828", "question": "what is the head judge of the supreme court called", "prediction": [["law.judicial_tenure.title", "law.judge.courts"], ["law.court.jurisdiction", "government.government_position_held.office_holder"]], "ground_paths": [["government.governmental_body.offices_positions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the head judge of the supreme court called [/INST]", "raw_output": {"paths": ["<PATH> law.judicial_tenure.title <SEP> law.judge.courts </PATH>", "<PATH> law.court.jurisdiction <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.08964153379201889, -0.11536248028278351], "norm_scores": [0.50642991065979, 0.49357014894485474]}}
{"id": "WebQTest-829", "question": "who was frida kahlo influenced by", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was frida kahlo influenced by [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.03889094665646553, -0.04529334604740143], "norm_scores": [0.5016006231307983, 0.49839943647384644]}}
{"id": "WebQTest-830", "question": "where is taylor swift from", "prediction": [["music.artist.origin"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is taylor swift from [/INST]", "raw_output": {"paths": ["<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.08301553875207901, -0.08823217451572418], "norm_scores": [0.5013041496276855, 0.49869582056999207]}}
{"id": "WebQTest-831", "question": "what type of religion does argentina have", "prediction": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"]], "ground_paths": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of religion does argentina have [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>"], "scores": [-0.02340906485915184, -0.04680493474006653], "norm_scores": [0.5058487057685852, 0.4941512942314148]}}
{"id": "WebQTest-832", "question": "when is the last time the giants won the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is the last time the giants won the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>"], "scores": [-0.02728090062737465, -0.07379607111215591], "norm_scores": [0.5116267204284668, 0.4883733093738556]}}
{"id": "WebQTest-833", "question": "what is the main language used in china", "prediction": [["language.human_language.main_country"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the main language used in china [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.030420467257499695, -0.1402728408575058], "norm_scores": [0.5274354815483093, 0.4725644886493683]}}
{"id": "WebQTest-834", "question": "where did the battle of waterloo occur", "prediction": [["time.event.locations"], ["location.location.events"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the battle of waterloo occur [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.01783207431435585, -0.2427428662776947], "norm_scores": [0.5559918284416199, 0.44400814175605774]}}
{"id": "WebQTest-836", "question": "who did vladimir lenin marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "ground_paths": [["people.person.profession", "people.person.profession"], ["freebase.valuenotation.has_no_value", "freebase.valuenotation.has_no_value"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.nationality", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did vladimir lenin marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>"], "scores": [-0.061593055725097656, -0.10661230981349945], "norm_scores": [0.5112528800964355, 0.48874709010124207]}}
{"id": "WebQTest-837", "question": "what year did the baltimore ravens win superbowl", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the baltimore ravens win superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>"], "scores": [-0.055709224194288254, -0.05757001414895058], "norm_scores": [0.5004652142524719, 0.4995347857475281]}}
{"id": "WebQTest-838", "question": "what team does cristiano ronaldo play for 2011", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"]], "ground_paths": [["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player.transfers", "soccer.football_player_transfer.purchasing_team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does cristiano ronaldo play for 2011 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>"], "scores": [-0.05026662349700928, -0.05825822427868843], "norm_scores": [0.5019978880882263, 0.4980021119117737]}}
{"id": "WebQTest-841", "question": "what is money called in italy", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is money called in italy [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.05028830096125603, -0.07855706661939621], "norm_scores": [0.5070667266845703, 0.4929332733154297]}}
{"id": "WebQTest-843", "question": "where did theodor schwann study", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.deceased_person.place_of_death", "location.location.contains"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did theodor schwann study [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.058818746358156204, -0.07102937251329422], "norm_scores": [0.5030526518821716, 0.49694737792015076]}}
{"id": "WebQTest-845", "question": "who are the virginia senators 2013", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.governmental_body.jurisdiction", "people.person.nationality"]], "ground_paths": [["location.administrative_division.country", "people.person.nationality"], ["government.election.district", "government.election.winner"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the virginia senators 2013 [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_body.jurisdiction <SEP> people.person.nationality </PATH>"], "scores": [-0.04139957204461098, -0.05711778625845909], "norm_scores": [0.5039294958114624, 0.49607053399086]}}
{"id": "WebQTest-846", "question": "what was dr seuss 's real name", "prediction": [["book.book.interior_illustrations_by", "people.person.nationality"], ["book.book.interior_illustrations_by", "people.person.interred_names"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was dr seuss 's real name [/INST]", "raw_output": {"paths": ["<PATH> book.book.interior_illustrations_by <SEP> people.person.nationality </PATH>", "<PATH> book.book.interior_illustrations_by <SEP> people.person.interred_names </PATH>"], "scores": [-0.06376807391643524, -0.13842733204364777], "norm_scores": [0.5186561942100525, 0.4813438653945923]}}
{"id": "WebQTest-847", "question": "what countries are members of the security council", "prediction": [["base.unitednations.united_nations_body_membership.body", "base.unitednations.united_nations_member_state.member_of"], ["base.unitednations.united_nations_body.members", "base.unitednations.united_nations_member_state.member_of"]], "ground_paths": [["base.unitednations.united_nations_body.members", "base.unitednations.united_nations_body_membership.member"], ["base.unitednations.united_nations_body.members", "base.unitednations.united_nations_member_state.member_of"], ["base.unitednations.united_nations_body_membership.body", "base.unitednations.united_nations_member_state.member_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are members of the security council [/INST]", "raw_output": {"paths": ["<PATH> base.unitednations.united_nations_body_membership.body <SEP> base.unitednations.united_nations_member_state.member_of </PATH>", "<PATH> base.unitednations.united_nations_body.members <SEP> base.unitednations.united_nations_member_state.member_of </PATH>"], "scores": [-0.020276151597499847, -0.026319537311792374], "norm_scores": [0.5015108585357666, 0.4984891414642334]}}
{"id": "WebQTest-848", "question": "where are the nordic countries", "prediction": [["location.location.contains"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the nordic countries [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.05222726985812187, -0.13084009289741516], "norm_scores": [0.5196431279182434, 0.480356901884079]}}
{"id": "WebQTest-849", "question": "what country sponsored ferdinand magellan", "prediction": [["people.person.nationality"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country sponsored ferdinand magellan [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.019178051501512527, -0.23805461823940277], "norm_scores": [0.5545017123222351, 0.4454982876777649]}}
{"id": "WebQTest-850", "question": "what did vera ellen die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did vera ellen die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.007790388539433479, -0.14733627438545227], "norm_scores": [0.5348299741744995, 0.4651700258255005]}}
{"id": "WebQTest-851", "question": "what is the capital of modern day egypt", "prediction": [["location.country.capital"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of modern day egypt [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.05997394025325775, -0.13087137043476105], "norm_scores": [0.5177169442176819, 0.4822830855846405]}}
{"id": "WebQTest-852", "question": "what province is toronto in ( 7 letters )", "prediction": [["location.province.capital"], ["base.biblioness.bibs_location.state"]], "ground_paths": [["location.province.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what province is toronto in ( 7 letters ) [/INST]", "raw_output": {"paths": ["<PATH> location.province.capital </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>"], "scores": [-0.015512531623244286, -0.16280882060527802], "norm_scores": [0.5367575883865356, 0.4632423520088196]}}
{"id": "WebQTest-855", "question": "what is the currency of brazil now", "prediction": [["location.country.currency_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency of brazil now [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.006636483129113913, -0.2527221739292145], "norm_scores": [0.5612128376960754, 0.43878719210624695]}}
{"id": "WebQTest-856", "question": "what types of art did andy warhol do", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what types of art did andy warhol do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.020302794873714447, -0.07972148805856705], "norm_scores": [0.5148502588272095, 0.48514968156814575]}}
{"id": "WebQTest-857", "question": "what other states border florida", "prediction": [["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.tropical_cyclone.affected_areas"]], "ground_paths": [["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.contains", "location.location.contains"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.containedby", "location.location.contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_contains", "location.location.partially_contains"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.tropical_cyclone.affected_areas"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.events", "time.event.locations"], ["location.administrative_division.first_level_division_of", "location.administrative_division.first_level_division_of"], ["location.location.time_zones", "location.location.time_zones"], ["location.administrative_division.first_level_division_of", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_contains", "location.location.partially_containedby"], ["location.location.partially_containedby", "location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other states border florida [/INST]", "raw_output": {"paths": ["<PATH> meteorology.tropical_cyclone.affected_areas <SEP> meteorology.cyclone_affected_area.cyclones </PATH>", "<PATH> meteorology.tropical_cyclone.affected_areas <SEP> meteorology.tropical_cyclone.affected_areas </PATH>"], "scores": [-0.05523070693016052, -0.08494040369987488], "norm_scores": [0.507426917552948, 0.4925731420516968]}}
{"id": "WebQTest-859", "question": "what are the predominant religions in russia", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "ground_paths": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the predominant religions in russia [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>"], "scores": [-0.029258456081151962, -0.03388254716992378], "norm_scores": [0.5011560320854187, 0.4988439977169037]}}
{"id": "WebQTest-861", "question": "who will coach carolina panthers in 2013", "prediction": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["sports.sports_team.sport", "sports.sports_team_coach.sports_coached"]], "ground_paths": [["american_football.football_coach.current_team_head_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who will coach carolina panthers in 2013 [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sports_team_coach.sports_coached </PATH>"], "scores": [-0.009568771347403526, -0.09037840366363525], "norm_scores": [0.5201914310455322, 0.4798085689544678]}}
{"id": "WebQTest-862", "question": "where did louis sachar grow up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did louis sachar grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.019113285467028618, -0.13095103204250336], "norm_scores": [0.5279303193092346, 0.4720696806907654]}}
{"id": "WebQTest-865", "question": "what to do for fun in houston", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do for fun in houston [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.04568862169981003, -0.058909881860017776], "norm_scores": [0.5033052563667297, 0.4966947138309479]}}
{"id": "WebQTest-866", "question": "where did otto frank go to college", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did otto frank go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.043131012469530106, -0.06095949932932854], "norm_scores": [0.5044569969177246, 0.4955430030822754]}}
{"id": "WebQTest-867", "question": "what else has jennifer lawrence played in", "prediction": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.film.starring"], ["award.award_winner.awards_won", "award.award_honor.honored_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what else has jennifer lawrence played in [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.07120723277330399, -0.07841748744249344], "norm_scores": [0.5018025636672974, 0.49819743633270264]}}
{"id": "WebQTest-868", "question": "who is the governor of pennsylvania state now", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.administrative_children", "people.person.place_of_birth"], ["location.location.containedby", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the governor of pennsylvania state now [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.050000421702861786, -0.052086882293224335], "norm_scores": [0.5005216002464294, 0.49947839975357056]}}
{"id": "WebQTest-869", "question": "where is ancient phoenician", "prediction": [["language.human_language.countries_spoken_in"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is ancient phoenician [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.09570957720279694, -0.11795912683010101], "norm_scores": [0.5055621862411499, 0.4944378435611725]}}
{"id": "WebQTest-870", "question": "where does the celtics practice", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the celtics practice [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>"], "scores": [-0.007619101088494062, -0.15709082782268524], "norm_scores": [0.5372985005378723, 0.4627014994621277]}}
{"id": "WebQTest-872", "question": "where bin laden got killed", "prediction": [["people.deceased_person.place_of_death"], ["base.culturalevent.event.entity_involved", "time.event.locations"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where bin laden got killed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>"], "scores": [-0.04486440122127533, -0.07402607053518295], "norm_scores": [0.5072899460792542, 0.4927101135253906]}}
{"id": "WebQTest-873", "question": "where does toronto get its water from", "prediction": [["base.infrastructure.sewage_treatment_plant.serves_location_s"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does toronto get its water from [/INST]", "raw_output": {"paths": ["<PATH> base.infrastructure.sewage_treatment_plant.serves_location_s </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.06651733070611954, -0.07693076878786087], "norm_scores": [0.5026033520698547, 0.49739667773246765]}}
{"id": "WebQTest-874", "question": "what language do people from bosnia speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people from bosnia speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.052896883338689804, -0.09518516808748245], "norm_scores": [0.5105705261230469, 0.4894295036792755]}}
{"id": "WebQTest-875", "question": "where did hugo chavez go to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.contains"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did hugo chavez go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.047616634517908096, -0.07184907048940659], "norm_scores": [0.5060577988624573, 0.4939422011375427]}}
{"id": "WebQTest-876", "question": "who does japan import from", "prediction": [["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["organization.organization.founders", "organization.organization.founders"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["travel.travel_destination.tour_operators", "organization.organization.geographic_scope"], ["location.country.form_of_government", "location.country.form_of_government"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.locations.countries.planet"], ["fictional_universe.fictional_universe.locations", "fictional_universe.fictional_universe.locations"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["film.film.featured_film_locations", "film.film.country"], ["location.statistical_region.places_imported_from", "location.statistical_region.places_exported_to"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"], ["book.newspaper.circulation_areas", "book.newspaper.circulation_areas"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.imports_and_exports.exported_to", "location.statistical_region.places_exported_to"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.statistical_region.places_imported_from", "location.imports_and_exports.imported_from"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["base.charities.charity.geographic_scope", "organization.organization.geographic_scope"], ["location.imports_and_exports.exported_to", "location.imports_and_exports.imported_from"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"], ["location.country.form_of_government", "government.form_of_government.countries"], ["cvg.computer_game_region.versions_released_in_this_region", "cvg.computer_game_region.versions_released_in_this_region"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["location.imports_and_exports.imported_from", "location.imports_and_exports.exported_to"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does japan import from [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.06704837083816528, -0.07406537979841232], "norm_scores": [0.5017542839050293, 0.4982457458972931]}}
{"id": "WebQTest-877", "question": "what is the name of sonia gandhi daughter", "prediction": [["people.person.children"], ["people.person.parents"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of sonia gandhi daughter [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.05734270066022873, -0.11347886919975281], "norm_scores": [0.5140303373336792, 0.4859696328639984]}}
{"id": "WebQTest-878", "question": "who do islamic people pray to", "prediction": [["religion.religion.deities"], ["religion.deity.deity_of"]], "ground_paths": [["religion.deity.deity_of"], ["religion.religion.deities"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who do islamic people pray to [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.deities </PATH>", "<PATH> religion.deity.deity_of </PATH>"], "scores": [-0.05492616072297096, -0.05545315146446228], "norm_scores": [0.5001317858695984, 0.4998682737350464]}}
{"id": "WebQTest-879", "question": "where was mary todd lincoln from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was mary todd lincoln from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.015482381917536259, -0.15030623972415924], "norm_scores": [0.5336549878120422, 0.4663449823856354]}}
{"id": "WebQTest-880", "question": "where did charles darwin die", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.place_of_burial"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did charles darwin die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>"], "scores": [-0.002947290660813451, -0.24202963709831238], "norm_scores": [0.5594874620437622, 0.4405125081539154]}}
{"id": "WebQTest-881", "question": "what movies does nick cannon play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["tv.tv_producer.programs_produced", "tv.tv_program.tv_producer"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.film.starring"], ["music.artist.track", "music.recording.releases"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies does nick cannon play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.060263730585575104, -0.12993445992469788], "norm_scores": [0.5174106359481812, 0.48258936405181885]}}
{"id": "WebQTest-882", "question": "when did the big earthquake hit san francisco", "prediction": [["time.event.locations"], ["location.location.events"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the big earthquake hit san francisco [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.043598026037216187, -0.19614745676517487], "norm_scores": [0.5380635857582092, 0.46193647384643555]}}
{"id": "WebQTest-883", "question": "who was king george vi married to", "prediction": [["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.religion", "people.person.religion"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.family.members", "people.family.members"], ["film.film.subjects", "film.film_subject.films"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was king george vi married to [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.1256781667470932, -0.15176628530025482], "norm_scores": [0.5065216422080994, 0.49347832798957825]}}
{"id": "WebQTest-884", "question": "where was jesus after he died on the cross", "prediction": [["people.deceased_person.place_of_death"], ["location.location.events"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was jesus after he died on the cross [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.032641079276800156, -0.1876441091299057], "norm_scores": [0.5386733412742615, 0.46132662892341614]}}
{"id": "WebQTest-885", "question": "who is sir james dyson", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is sir james dyson [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0038444167003035545, -0.32771411538124084], "norm_scores": [0.5802670121192932, 0.4197329580783844]}}
{"id": "WebQTest-886", "question": "who voiced darth vader in the original star wars movies", "prediction": [["film.performance.special_performance_type", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who voiced darth vader in the original star wars movies [/INST]", "raw_output": {"paths": ["<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.07375083863735199, -0.08899328112602234], "norm_scores": [0.5038105249404907, 0.4961894750595093]}}
{"id": "WebQTest-887", "question": "what high school did maya angelou go to", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what high school did maya angelou go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.04220554232597351, -0.06598348170518875], "norm_scores": [0.5059442520141602, 0.4940558075904846]}}
{"id": "WebQTest-888", "question": "what 5 countries border switzerland", "prediction": [["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.partially_containedby", "location.location.partially_contains"]], "ground_paths": [["fictional_universe.work_of_fiction.setting", "fictional_universe.fictional_setting.works_set_here"], ["location.location.containedby", "location.location.containedby"], ["freebase.valuenotation.has_no_value", "freebase.valuenotation.is_reviewed"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["time.event.locations", "location.location.events"], ["location.location.containedby", "base.locations.countries.continent"], ["location.location.contains", "location.location.partially_containedby"], ["location.location.partially_contains", "geography.river.basin_countries"], ["location.country.form_of_government", "location.country.form_of_government"], ["location.location.contains", "location.location.containedby"], ["location.location.time_zones", "location.location.time_zones"], ["base.aareas.schema.administrative_area.subdivides_place"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.country.languages_spoken", "language.human_language.main_country"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["geography.lake.basin_countries", "geography.lake.basin_countries"], ["geography.lake.basin_countries", "location.location.partially_contains"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["fictional_universe.fictional_setting.universe", "fictional_universe.fictional_setting.universe"], ["book.newspaper.circulation_areas", "book.newspaper.circulation_areas"], ["location.location.events", "time.event.locations"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.country.official_language", "location.country.official_language"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["people.person.nationality", "people.person.nationality"], ["biology.breed_origin.breeds_originating_here", "biology.breed_origin.breeds_originating_here"], ["location.location.contains", "location.location.contains"], ["location.location.events", "location.location.events"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["sports.multi_event_tournament.participating_countries", "sports.multi_event_tournament.participating_countries"], ["location.location.partially_contains", "location.location.partially_contains"], ["film.film_location.featured_in_films", "film.film.featured_film_locations"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["geography.river.origin", "location.location.partially_contains"], ["geography.lake.basin_countries", "location.location.partially_containedby"], ["book.written_work.subjects", "book.written_work.subjects"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["film.film.country", "film.film.country"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["location.location.containedby", "location.location.contains_major_portion_of"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["base.mystery.cryptid_classification.area_of_occurrence", "base.mystery.cryptid_observation_location.cryptid_s_occurring_here"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["location.location.containedby", "base.nations.supranational_body.member"], ["location.country.form_of_government", "government.form_of_government.countries"], ["book.newspaper.circulation_areas", "periodicals.newspaper_circulation_area.newspapers"], ["sports.multi_event_tournament.participating_countries", "sports.sport_country.multi_event_tournaments_participated_in"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["time.event.locations", "time.event.locations"], ["location.location.partially_contains", "location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what 5 countries border switzerland [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>"], "scores": [-0.12940126657485962, -0.1383240818977356], "norm_scores": [0.502230703830719, 0.4977693259716034]}}
{"id": "WebQTest-889", "question": "what country did benito mussolini govern", "prediction": [["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"], ["government.politician.government_positions_held", "government.governmental_jurisdiction.governing_officials"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did benito mussolini govern [/INST]", "raw_output": {"paths": ["<PATH> government.politician.government_positions_held <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.governmental_jurisdiction.governing_officials </PATH>"], "scores": [-0.06248348951339722, -0.07308358699083328], "norm_scores": [0.5026499629020691, 0.49734997749328613]}}
{"id": "WebQTest-891", "question": "who was the leader of the first communist party of china", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "ground_paths": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the leader of the first communist party of china [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.founders </PATH>"], "scores": [-0.014371298253536224, -0.15455883741378784], "norm_scores": [0.534989595413208, 0.4650104343891144]}}
{"id": "WebQTest-892", "question": "what did miles davis died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did miles davis died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.0071210903115570545, -0.1531519889831543], "norm_scores": [0.5364429950714111, 0.46355703473091125]}}
{"id": "WebQTest-893", "question": "what is the zip code for azle texas", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for azle texas [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0006074084085412323, -0.5278648734092712], "norm_scores": [0.6288432478904724, 0.37115678191185]}}
{"id": "WebQTest-894", "question": "what date did sally pearson won gold", "prediction": [["people.person.nationality", "olympics.olympic_participating_country.olympics_participated_in"], ["people.person.nationality", "olympics.olympic_games.participating_countries"]], "ground_paths": [["people.person.nationality", "olympics.olympic_games.participating_countries"], ["olympics.olympic_athlete_affiliation.athlete", "olympics.olympic_athlete_affiliation.olympics"], ["olympics.olympic_athlete.medals_won", "olympics.olympic_medal_honor.olympics"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what date did sally pearson won gold [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> people.person.nationality <SEP> olympics.olympic_games.participating_countries </PATH>"], "scores": [-0.0450410433113575, -0.08345703780651093], "norm_scores": [0.5096027851104736, 0.490397185087204]}}
{"id": "WebQTest-895", "question": "when did the detroit pistons last win the championship", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the detroit pistons last win the championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>"], "scores": [-0.04098616912961006, -0.049588464200496674], "norm_scores": [0.5021505951881409, 0.4978494346141815]}}
{"id": "WebQTest-896", "question": "when did the philadelphia flyers win the cup", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the philadelphia flyers win the cup [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.037982307374477386, -0.05265543982386589], "norm_scores": [0.5036681890487671, 0.4963318109512329]}}
{"id": "WebQTest-898", "question": "what county is st paul nc in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st paul nc in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0398501418530941, -0.15000537037849426], "norm_scores": [0.5275110006332397, 0.47248899936676025]}}
{"id": "WebQTest-899", "question": "where is olympic national park wa", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "ground_paths": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is olympic national park wa [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>"], "scores": [-0.04504188522696495, -0.10229460895061493], "norm_scores": [0.5143092274665833, 0.485690712928772]}}
{"id": "WebQTest-900", "question": "where did nat turner 1831 rebellion occur", "prediction": [["location.location.events"], ["time.event.locations"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did nat turner 1831 rebellion occur [/INST]", "raw_output": {"paths": ["<PATH> location.location.events </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.0846312940120697, -0.08903229236602783], "norm_scores": [0.5011002421379089, 0.49889975786209106]}}
{"id": "WebQTest-901", "question": "who would play luke skywalker", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who would play luke skywalker [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.059540990740060806, -0.06369295716285706], "norm_scores": [0.5010380148887634, 0.49896201491355896]}}
{"id": "WebQTest-903", "question": "where turkish people originate", "prediction": [["people.ethnicity.geographic_distribution"], ["people.ethnicity.included_in_group"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where turkish people originate [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> people.ethnicity.included_in_group </PATH>"], "scores": [-0.0003065757337026298, -0.5013646483421326], "norm_scores": [0.6227079033851624, 0.37729203701019287]}}
{"id": "WebQTest-904", "question": "what state is barack obama from", "prediction": [["people.person.place_of_birth", "base.biblioness.bibs_location.state"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["base.popstra.celebrity.vacations_in", "base.popstra.location.vacationers"], ["people.person.place_of_birth", "base.biblioness.bibs_location.state"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state is barack obama from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth <SEP> base.biblioness.bibs_location.state </PATH>", "<PATH> people.person.nationality <SEP> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.04569602385163307, -0.05515575036406517], "norm_scores": [0.5023649334907532, 0.4976350665092468]}}
{"id": "WebQTest-905", "question": "what years did joe montana win super bowl", "prediction": [["sports.sports_championship_event.champion"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what years did joe montana win super bowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.021985294297337532, -0.07955785095691681], "norm_scores": [0.514389157295227, 0.48561084270477295]}}
{"id": "WebQTest-908", "question": "what awards gary paulsen won", "prediction": [["award.award_honor.award_winner", "award.award_honor.award"], ["award.award_nomination.award_nominee", "award.award_nomination.award"]], "ground_paths": [["award.award_winner.awards_won", "award.award_honor.award"], ["award.award_honor.award_winner", "award.award_category.winners"], ["award.award_nomination.award_nominee", "award.award_nomination.award"], ["award.award_nominee.award_nominations", "award.award_nomination.award"], ["award.award_honor.award_winner", "award.award_honor.award"], ["award.award_winner.awards_won", "award.award_category.winners"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what awards gary paulsen won [/INST]", "raw_output": {"paths": ["<PATH> award.award_honor.award_winner <SEP> award.award_honor.award </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award </PATH>"], "scores": [-0.05007430166006088, -0.059784889221191406], "norm_scores": [0.5024276375770569, 0.4975723326206207]}}
{"id": "WebQTest-910", "question": "where did alexander graham bell die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people", "people.deceased_person.place_of_death"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did alexander graham bell die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.0013828384689986706, -0.2296178787946701], "norm_scores": [0.5568123459815979, 0.4431876540184021]}}
{"id": "WebQTest-911", "question": "what year did the golden state warriors win their first nba championship", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the golden state warriors win their first nba championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>"], "scores": [-0.0406903401017189, -0.05015888810157776], "norm_scores": [0.5023671388626099, 0.4976328909397125]}}
{"id": "WebQTest-912", "question": "what currency should i take to turkey", "prediction": [["finance.currency.countries_formerly_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should i take to turkey [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.06448079645633698, -0.07669514417648315], "norm_scores": [0.5030535459518433, 0.49694642424583435]}}
{"id": "WebQTest-914", "question": "what countries are in the uk yahoo", "prediction": [["base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are in the uk yahoo [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.056411173194646835, -0.08140502870082855], "norm_scores": [0.5062481760978699, 0.4937518537044525]}}
{"id": "WebQTest-915", "question": "what superbowl did the baltimore ravens win", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what superbowl did the baltimore ravens win [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.04846983030438423, -0.05114872008562088], "norm_scores": [0.5006697177886963, 0.4993302822113037]}}
{"id": "WebQTest-916", "question": "what is the language of russia s spoken", "prediction": [["language.human_language.main_country"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the language of russia s spoken [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.08834930509328842, -0.09315889328718185], "norm_scores": [0.501202404499054, 0.49879759550094604]}}
{"id": "WebQTest-918", "question": "who is the king of cambodia now", "prediction": [["royalty.monarch.kingdom"], ["people.person.nationality"]], "ground_paths": [["royalty.kingdom.rulers"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the king of cambodia now [/INST]", "raw_output": {"paths": ["<PATH> royalty.monarch.kingdom </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.07589155435562134, -0.08434613794088364], "norm_scores": [0.5021136403083801, 0.4978863596916199]}}
{"id": "WebQTest-919", "question": "who are serena williams parents", "prediction": [["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are serena williams parents [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.04848402366042137, -0.13072755932807922], "norm_scores": [0.5205492973327637, 0.47945067286491394]}}
{"id": "WebQTest-920", "question": "what schools did sir isaac newton go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what schools did sir isaac newton go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.033623967319726944, -0.08049707859754562], "norm_scores": [0.5117161273956299, 0.4882838726043701]}}
{"id": "WebQTest-921", "question": "what was the cause of death for huell howser", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the cause of death for huell howser [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.0038275541737675667, -0.2069607675075531], "norm_scores": [0.5506094098091125, 0.44939059019088745]}}
{"id": "WebQTest-922", "question": "what books are written by suzanne collins", "prediction": [["book.book_edition.author_editor"], ["book.author.works_written"]], "ground_paths": [["book.author.book_editions_published"], ["book.book_edition.author_editor"], ["book.written_work.author"], ["book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what books are written by suzanne collins [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.06141039356589317, -0.11371693760156631], "norm_scores": [0.5130736827850342, 0.4869263470172882]}}
{"id": "WebQTest-923", "question": "what countries does england border", "prediction": [["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"]], "ground_paths": [["base.biblioness.bibs_location.country", "base.aareas.schema.administrative_area.administrative_children"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["time.event.locations", "location.location.events"], ["biology.animal_breed.place_of_origin", "biology.animal_breed.place_of_origin"], ["law.court_jurisdiction_area.courts", "law.court_jurisdiction_area.courts"], ["cricket.cricket_tournament_event.host", "time.event.locations"], ["cricket.cricket_tournament_event.host", "location.location.events"], ["location.country.form_of_government", "location.country.form_of_government"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.partially_containedby", "location.location.partially_contains"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["periodicals.newspaper_circulation_area.newspapers", "book.newspaper.circulation_areas"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.events", "time.event.locations"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.country.languages_spoken", "location.country.official_language"], ["finance.currency.countries_used", "location.country.currency_used"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["people.person.nationality", "people.person.nationality"], ["biology.breed_origin.breeds_originating_here", "biology.breed_origin.breeds_originating_here"], ["government.governmental_jurisdiction.government_bodies", "government.governmental_jurisdiction.government_bodies"], ["law.court_jurisdiction_area.courts", "law.court.jurisdiction"], ["location.location.events", "location.location.events"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.multi_event_tournament.participating_countries"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["base.charities.charity.geographic_scope", "organization.organization.geographic_scope"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["sports.sports_team_location.teams", "cricket.cricket_team.country"], ["film.film.featured_film_locations", "film.film.featured_film_locations"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["government.government_agency.jurisdiction", "government.governmental_jurisdiction.agencies"], ["location.country.internet_tld", "location.country.internet_tld"], ["film.film.featured_film_locations", "film.film_location.featured_in_films"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["government.government_agency.jurisdiction", "government.government_agency.jurisdiction"], ["periodicals.newspaper_circulation_area.newspapers", "periodicals.newspaper_circulation_area.newspapers"], ["organization.organization.geographic_scope", "common.topic.subject_of"], ["book.newspaper.circulation_areas", "periodicals.newspaper_circulation_area.newspapers"], ["biology.breed_origin.breeds_originating_here", "biology.animal_breed.place_of_origin"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["time.event.locations", "time.event.locations"], ["location.location.partially_contains", "location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does england border [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.geographic_scope <SEP> organization.organization_scope.organizations_with_this_scope </PATH>", "<PATH> organization.organization.geographic_scope <SEP> organization.organization.geographic_scope </PATH>"], "scores": [-0.10497839003801346, -0.12145784497261047], "norm_scores": [0.5041197538375854, 0.49588021636009216]}}
{"id": "WebQTest-924", "question": "what team did messi play for in 2010", "prediction": [["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"]], "ground_paths": [["sports.sports_award.award_winner", "sports.sports_award.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did messi play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>"], "scores": [-0.06832018494606018, -0.11714146286249161], "norm_scores": [0.5122029185295105, 0.4877971112728119]}}
{"id": "WebQTest-925", "question": "what time zone is greensburg indiana", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is greensburg indiana [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-0.00011122361320303753, -0.9099122881889343], "norm_scores": [0.7129594087600708, 0.2870405316352844]}}
{"id": "WebQTest-927", "question": "who plays voldemort", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["cvg.game_performance.character", "cvg.game_performance.voice_actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays voldemort [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.059059225022792816, -0.061432454735040665], "norm_scores": [0.5005933046340942, 0.4994066655635834]}}
{"id": "WebQTest-928", "question": "what colleges did harper lee attend", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what colleges did harper lee attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.04257304221391678, -0.08003661781549454], "norm_scores": [0.5093647837638855, 0.4906352162361145]}}
{"id": "WebQTest-929", "question": "who founded the kkk", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who founded the kkk [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.04563133791089058, -0.05954017490148544], "norm_scores": [0.503477156162262, 0.49652281403541565]}}
{"id": "WebQTest-931", "question": "what jobs did john adams have before he was president", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"]], "ground_paths": [["common.topic.image", "common.topic.image"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["tv.tv_program.country_of_origin", "government.government_office_or_title.jurisdiction"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what jobs did john adams have before he was president [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>"], "scores": [-0.05033884197473526, -0.06787310540676117], "norm_scores": [0.5043834447860718, 0.4956165552139282]}}
{"id": "WebQTest-932", "question": "what year did super mario bros 2 come out", "prediction": [["film.film.sequel"], ["film.film.country"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did super mario bros 2 come out [/INST]", "raw_output": {"paths": ["<PATH> film.film.sequel </PATH>", "<PATH> film.film.country </PATH>"], "scores": [-0.13850624859333038, -0.2454245686531067], "norm_scores": [0.5267041325569153, 0.47329583764076233]}}
{"id": "WebQTest-933", "question": "what political party did andrew johnson belong to", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"]], "ground_paths": [["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["government.politician.party", "government.political_party_tenure.party"], ["people.person.nationality", "organization.organization.geographic_scope"], ["government.us_president.vice_president", "organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party did andrew johnson belong to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>"], "scores": [-0.05091981217265129, -0.06298323720693588], "norm_scores": [0.5030158162117004, 0.49698418378829956]}}
{"id": "WebQTest-935", "question": "where was osama bin laden killed", "prediction": [["base.culturalevent.event.entity_involved", "time.event.locations"], ["people.deceased_person.place_of_death"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was osama bin laden killed [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>"], "scores": [-0.04611583426594734, -0.08585769683122635], "norm_scores": [0.50993412733078, 0.4900658428668976]}}
{"id": "WebQTest-936", "question": "where were greek temples built", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["architecture.building.building_function", "travel.tourist_attraction.near_travel_destination"], ["architecture.building_function.buildings", "location.location.containedby"], ["common.image.appears_in_topic_gallery", "common.image.appears_in_topic_gallery"], ["architecture.building.building_function", "location.location.containedby"], ["architecture.building_function.buildings", "location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where were greek temples built [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.032329197973012924, -0.28832218050956726], "norm_scores": [0.5636509656906128, 0.43634897470474243]}}
{"id": "WebQTest-937", "question": "where are people who speak arabic from", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are people who speak arabic from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.05901072174310684, -0.11385145038366318], "norm_scores": [0.5137067437171936, 0.486293226480484]}}
{"id": "WebQTest-938", "question": "who was queen elizabeth ii mom", "prediction": [["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was queen elizabeth ii mom [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.02982143871486187, -0.18394693732261658], "norm_scores": [0.5384552478790283, 0.4615447521209717]}}
{"id": "WebQTest-939", "question": "what type of art did pablo picasso paint", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art did pablo picasso paint [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.005166165065020323, -0.13348764181137085], "norm_scores": [0.5320364236831665, 0.4679635763168335]}}
{"id": "WebQTest-941", "question": "what is ella fitzgerald parents name", "prediction": [["people.person.children"], ["people.person.parents"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is ella fitzgerald parents name [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.06835567206144333, -0.09721777588129044], "norm_scores": [0.5072150230407715, 0.4927849769592285]}}
{"id": "WebQTest-942", "question": "who is anne hathaway parents", "prediction": [["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is anne hathaway parents [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.07842390984296799, -0.08627672493457794], "norm_scores": [0.501963198184967, 0.49803677201271057]}}
{"id": "WebQTest-943", "question": "who does don draper gets engaged to", "prediction": [["fictional_universe.fictional_character.character_created_by", "fictional_universe.fictional_character.character_created_by"], ["fictional_universe.fictional_character.married_to", "fictional_universe.marriage_of_fictional_characters.spouses"]], "ground_paths": [["fictional_universe.fictional_character_creator.fictional_characters_created", "fictional_universe.fictional_character.character_created_by"], ["fictional_universe.fictional_character.married_to", "fictional_universe.fictional_character.married_to"], ["common.topic.notable_types", "common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does don draper gets engaged to [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.character_created_by <SEP> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> fictional_universe.fictional_character.married_to <SEP> fictional_universe.marriage_of_fictional_characters.spouses </PATH>"], "scores": [-0.045965876430273056, -0.051525700837373734], "norm_scores": [0.5013899803161621, 0.49861007928848267]}}
{"id": "WebQTest-944", "question": "what was van halen first album", "prediction": [["music.artist.album"], ["music.album.artist"]], "ground_paths": [["music.album.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was van halen first album [/INST]", "raw_output": {"paths": ["<PATH> music.artist.album </PATH>", "<PATH> music.album.artist </PATH>"], "scores": [-0.12039056420326233, -0.13951489329338074], "norm_scores": [0.5047809481620789, 0.49521905183792114]}}
{"id": "WebQTest-945", "question": "what political system did spain have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political system did spain have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>"], "scores": [-0.016895124688744545, -0.0920337662100792], "norm_scores": [0.5187758207321167, 0.4812241792678833]}}
{"id": "WebQTest-948", "question": "what are the supreme court cases", "prediction": [["law.court.jurisdiction", "government.governmental_jurisdiction.government_bodies"], ["law.court.jurisdiction", "government.governmental_body.jurisdiction"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the supreme court cases [/INST]", "raw_output": {"paths": ["<PATH> law.court.jurisdiction <SEP> government.governmental_jurisdiction.government_bodies </PATH>", "<PATH> law.court.jurisdiction <SEP> government.governmental_body.jurisdiction </PATH>"], "scores": [-0.04808029532432556, -0.09818746149539948], "norm_scores": [0.5125241875648499, 0.48747584223747253]}}
{"id": "WebQTest-949", "question": "what is penn state 's main campus", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is penn state 's main campus [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.016820590943098068, -0.30849239230155945], "norm_scores": [0.5724053978919983, 0.4275946319103241]}}
{"id": "WebQTest-952", "question": "what country does alaska belong to", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["base.biblioness.bibs_location.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country does alaska belong to [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.08158960193395615, -0.0963679850101471], "norm_scores": [0.5036945343017578, 0.4963054656982422]}}
{"id": "WebQTest-953", "question": "where is kansas city", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.hud_county_place.county"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is kansas city [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.029151489958167076, -0.2961713373661041], "norm_scores": [0.566361129283905, 0.4336388409137726]}}
{"id": "WebQTest-954", "question": "what was pink floyd 's first album", "prediction": [["music.recording.artist"], ["music.artist.album"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was pink floyd 's first album [/INST]", "raw_output": {"paths": ["<PATH> music.recording.artist </PATH>", "<PATH> music.artist.album </PATH>"], "scores": [-0.148840993642807, -0.20674407482147217], "norm_scores": [0.514471709728241, 0.48552829027175903]}}
{"id": "WebQTest-955", "question": "who did granville t. woods married", "prediction": [["people.person.children", "people.person.children"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.spouse_s", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did granville t. woods married [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.12576715648174286, -0.14628632366657257], "norm_scores": [0.5051295757293701, 0.4948703944683075]}}
{"id": "WebQTest-957", "question": "what language did the sioux indians speak", "prediction": [["people.ethnicity.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language did the sioux indians speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.0029779132455587387, -0.3666480779647827], "norm_scores": [0.5899286270141602, 0.41007140278816223]}}
{"id": "WebQTest-958", "question": "what are some famous people from el salvador", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some famous people from el salvador [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.014509673230350018, -0.18785421550273895], "norm_scores": [0.5432279706001282, 0.4567720592021942]}}
{"id": "WebQTest-959", "question": "who did brett gardner play for in 2008", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did brett gardner play for in 2008 [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>"], "scores": [-0.02790001407265663, -0.049020349979400635], "norm_scores": [0.5052798986434937, 0.49472010135650635]}}
{"id": "WebQTest-960", "question": "who plays dwight in the office", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["tv.tv_series_season.episodes", "tv.tv_director.episodes_directed"], ["award.award_winning_work.awards_won", "award.award_honor.award_winner"], ["tv.tv_series_season.regular_cast", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays dwight in the office [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.04501171410083771, -0.09866393357515335], "norm_scores": [0.5134098529815674, 0.4865901470184326]}}
{"id": "WebQTest-962", "question": "what county is san diego ca", "prediction": [["location.us_county.hud_county_place"], ["location.hud_county_place.county"]], "ground_paths": [["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is san diego ca [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.05546209216117859, -0.09025027602910995], "norm_scores": [0.5086961388587952, 0.49130383133888245]}}
{"id": "WebQTest-963", "question": "who is the miami dolphins quarterback", "prediction": [["american_football.football_player.position_s"], ["american_football.football_team.current_head_coach"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the miami dolphins quarterback [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.position_s </PATH>", "<PATH> american_football.football_team.current_head_coach </PATH>"], "scores": [-0.03065435402095318, -0.17404630780220032], "norm_scores": [0.5357866883277893, 0.4642133116722107]}}
{"id": "WebQTest-965", "question": "where did anne frank go to school", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["book.book_subject.works", "book.written_work.subjects"], ["education.education.student", "education.education.institution"], ["book.book_subject.works", "book.book_subject.works"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did anne frank go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.06443599611520767, -0.06858591735363007], "norm_scores": [0.5010374784469604, 0.49896255135536194]}}
{"id": "WebQTest-966", "question": "what basketball player is married to monica", "prediction": [["sports.sports_team_coach.sports_coached", "people.person.nationality", "people.person.nationality"], ["sports.sports_team_coach.sports_coached", "people.person.gender", "people.person.gender"]], "ground_paths": [["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.marriage.spouse", "people.person.spouse_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what basketball player is married to monica [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_coach.sports_coached <SEP> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.0892522931098938, -0.10480086505413055], "norm_scores": [0.5038870573043823, 0.4961129426956177]}}
{"id": "WebQTest-967", "question": "what did hitler use to kill himself", "prediction": [["people.deceased_person.cause_of_death"], ["base.activism.activist.area_of_activism", "base.activism.activist.area_of_activism"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did hitler use to kill himself [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> base.activism.activist.area_of_activism </PATH>"], "scores": [-0.02828853391110897, -0.06883933395147324], "norm_scores": [0.5101363062858582, 0.48986369371414185]}}
{"id": "WebQTest-968", "question": "what do people speak in germany", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people speak in germany [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.04743247851729393, -0.08733558654785156], "norm_scores": [0.509974479675293, 0.4900255501270294]}}
{"id": "WebQTest-969", "question": "when is the last time the denver broncos won the superbowl", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is the last time the denver broncos won the superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>"], "scores": [-0.061041682958602905, -0.07020334154367447], "norm_scores": [0.5022904276847839, 0.49770960211753845]}}
{"id": "WebQTest-970", "question": "where was caesar when he was stabbed", "prediction": [["people.deceased_person.place_of_death"], ["people.person.place_of_birth"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was caesar when he was stabbed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.005871744826436043, -0.3058946430683136], "norm_scores": [0.5744481086730957, 0.4255518913269043]}}
{"id": "WebQTest-971", "question": "what style of music did bessie smith perform", "prediction": [["music.artist.genre"], ["music.genre.artists"]], "ground_paths": [["book.book_subject.works"], ["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what style of music did bessie smith perform [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>"], "scores": [-0.00307621737010777, -0.39792317152023315], "norm_scores": [0.5974489450454712, 0.4025510549545288]}}
{"id": "WebQTest-972", "question": "what team did jackie robinson", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team.roster"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did jackie robinson [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>"], "scores": [-0.04090943932533264, -0.04531208798289299], "norm_scores": [0.5011006593704224, 0.49889934062957764]}}
{"id": "WebQTest-973", "question": "what time zone is chicago in right now", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is chicago in right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-4.322591848904267e-05, -0.9091341495513916], "norm_scores": [0.7128140926361084, 0.2871859073638916]}}
{"id": "WebQTest-975", "question": "who was the vice president under ronald reagan", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the vice president under ronald reagan [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>"], "scores": [-0.02433782070875168, -0.07023978978395462], "norm_scores": [0.5114734768867493, 0.4885265529155731]}}
{"id": "WebQTest-976", "question": "where do the chicago white sox play home games", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the chicago white sox play home games [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>"], "scores": [-0.002140313619747758, -0.2513333559036255], "norm_scores": [0.5619778633117676, 0.4380221366882324]}}
{"id": "WebQTest-977", "question": "what part did jeff conaway play in grease", "prediction": [["film.actor.film", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["theater.theater_character.plays_appears_in"], ["film.performance.actor", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part did jeff conaway play in grease [/INST]", "raw_output": {"paths": ["<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.0971606969833374, -0.13442860543727875], "norm_scores": [0.5093159079551697, 0.4906840920448303]}}
{"id": "WebQTest-978", "question": "what book did benjamin franklin published", "prediction": [["book.written_work.author"], ["book.author.book_editions_published"]], "ground_paths": [["book.written_work.author"], ["people.person.profession", "common.topic.subject_of"], ["book.author.book_editions_published"], ["book.book_edition.author_editor"], ["book.book_edition.author_editor", "book.book.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what book did benjamin franklin published [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.book_editions_published </PATH>"], "scores": [-0.062399327754974365, -0.13533899188041687], "norm_scores": [0.5182268023490906, 0.48177313804626465]}}
{"id": "WebQTest-979", "question": "what currency to take to barbados", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency to take to barbados [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.05532778054475784, -0.07191254943609238], "norm_scores": [0.5041460990905762, 0.4958539307117462]}}
{"id": "WebQTest-980", "question": "where did they get the name stanley cup", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"]], "ground_paths": [["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did they get the name stanley cup [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.060427743941545486, -0.10432866960763931], "norm_scores": [0.5109734535217285, 0.4890265166759491]}}
{"id": "WebQTest-982", "question": "what is the zip code of syracuse ny", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code of syracuse ny [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0012601498747244477, -0.4329875409603119], "norm_scores": [0.6062860488891602, 0.39371389150619507]}}
{"id": "WebQTest-983", "question": "where did tennessee williams go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did tennessee williams go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.04670138284564018, -0.0648990124464035], "norm_scores": [0.5045492649078369, 0.4954507052898407]}}
{"id": "WebQTest-984", "question": "what is the name of airport in new york", "prediction": [["aviation.airport.serves"], ["location.location.nearby_airports"]], "ground_paths": [["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of airport in new york [/INST]", "raw_output": {"paths": ["<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.nearby_airports </PATH>"], "scores": [-0.04911169782280922, -0.12734833359718323], "norm_scores": [0.5195491909980774, 0.4804508090019226]}}
{"id": "WebQTest-985", "question": "who was queen victoria 's husband", "prediction": [["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"]], "ground_paths": [["royalty.kingdom.rulers", "people.person.nationality"], ["people.person.parents", "people.person.children"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.children", "people.person.parents"], ["base.crime.crime_victim.crime", "base.crime.crime_victim.crime"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was queen victoria 's husband [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.13986484706401825, -0.15439648926258087], "norm_scores": [0.5036328434944153, 0.4963671565055847]}}
{"id": "WebQTest-986", "question": "where did benjamin franklin work", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.employment_history", "business.employer.employees"]], "ground_paths": [["people.person.nationality", "government.government_agency.jurisdiction"], ["people.person.children"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["people.person.employment_history", "business.employer.employees"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did benjamin franklin work [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.employment_history <SEP> business.employer.employees </PATH>"], "scores": [-0.05677562952041626, -0.08064591139554977], "norm_scores": [0.5059673190116882, 0.49403274059295654]}}
{"id": "WebQTest-987", "question": "what does barbara bush do for work", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["business.employment_tenure.person", "business.job_title.people_with_this_title"], ["government.politician.government_positions_held", "government.government_office_category.officeholders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does barbara bush do for work [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.011430309154093266, -0.32121554017066956], "norm_scores": [0.5768328309059143, 0.4231671690940857]}}
{"id": "WebQTest-988", "question": "who is jacob latimore", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is jacob latimore [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0021915396209806204, -0.3658515214920044], "norm_scores": [0.5899261236190796, 0.4100738763809204]}}
{"id": "WebQTest-989", "question": "what happened to dunkirk during ww2", "prediction": [["military.military_conflict.casualties", "military.casualties.event"], ["military.military_conflict.casualties", "military.military_conflict.casualties"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what happened to dunkirk during ww2 [/INST]", "raw_output": {"paths": ["<PATH> military.military_conflict.casualties <SEP> military.casualties.event </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.military_conflict.casualties </PATH>"], "scores": [-0.08358351141214371, -0.09334362298250198], "norm_scores": [0.5024400353431702, 0.4975599944591522]}}
{"id": "WebQTest-990", "question": "what was scottie pippen good at", "prediction": [["basketball.basketball_player.position_s"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.player"]], "ground_paths": [["sports.pro_sports_played.athlete", "sports.pro_sports_played.sport"], ["basketball.basketball_player.position_s", "sports.sport.positions"], ["common.topic.notable_types", "type.type.domain"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was scottie pippen good at [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.position_s </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.player </PATH>"], "scores": [-0.06811128556728363, -0.08608885854482651], "norm_scores": [0.5044942498207092, 0.49550575017929077]}}
{"id": "WebQTest-991", "question": "who played donna on west wing", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played donna on west wing [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.06225983798503876, -0.09743475168943405], "norm_scores": [0.5087928175926208, 0.49120718240737915]}}
{"id": "WebQTest-993", "question": "who played meg in season 1 of family guy", "prediction": [["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played meg in season 1 of family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.05524320527911186, -0.08750682324171066], "norm_scores": [0.5080651640892029, 0.49193480610847473]}}
{"id": "WebQTest-994", "question": "when was the last time the dallas cowboys won the superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the dallas cowboys won the superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.027512596920132637, -0.10129740834236145], "norm_scores": [0.5184378027915955, 0.48156213760375977]}}
{"id": "WebQTest-996", "question": "where do ireland play rugby union", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_team_location.teams"]], "ground_paths": [["sports.sports_facility.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do ireland play rugby union [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_team_location.teams </PATH>"], "scores": [-0.018171343952417374, -0.16074702143669128], "norm_scores": [0.5355836749076843, 0.4644163250923157]}}
{"id": "WebQTest-997", "question": "who killed lee harvey oswald shot", "prediction": [["people.deceased_person.cause_of_death"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who killed lee harvey oswald shot [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.036548420786857605, -0.06265106797218323], "norm_scores": [0.5065252780914307, 0.49347469210624695]}}
{"id": "WebQTest-998", "question": "when was the last time knicks won the championship", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time knicks won the championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.0361420214176178, -0.05476103723049164], "norm_scores": [0.504654586315155, 0.4953453540802002]}}
{"id": "WebQTest-999", "question": "when did celtics win the championship", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "ground_paths": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did celtics win the championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>"], "scores": [-0.04135160893201828, -0.04906288534402847], "norm_scores": [0.5019277930259705, 0.49807217717170715]}}
{"id": "WebQTest-1000", "question": "what countries included in oceania", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries included in oceania [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.03260491415858269, -0.2321719229221344], "norm_scores": [0.5497268438339233, 0.45027315616607666]}}
{"id": "WebQTest-1001", "question": "what country is nicki minaj from", "prediction": [["people.person.nationality"], ["music.artist.origin"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country is nicki minaj from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.033292822539806366, -0.2306557446718216], "norm_scores": [0.5491811633110046, 0.450818806886673]}}
{"id": "WebQTest-1002", "question": "who played harry potter dumbledore actor", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played harry potter dumbledore actor [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>"], "scores": [-0.049267519265413284, -0.06952356547117233], "norm_scores": [0.5050638318061829, 0.49493613839149475]}}
{"id": "WebQTest-1003", "question": "who was the prime minister of pakistan in 1999", "prediction": [["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"]], "ground_paths": [["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the prime minister of pakistan in 1999 [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>"], "scores": [-0.021801013499498367, -0.04123133793473244], "norm_scores": [0.5048574209213257, 0.4951425790786743]}}
{"id": "WebQTest-1004", "question": "what movies did christopher plummer play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["music.artist.album", "film.film.soundtrack"], ["tv.tv_guest_role.actor", "tv.tv_guest_role.episodes_appeared_in"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.dubbing_performances", "film.film.dubbing_performances"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.producer.films_executive_produced"], ["film.performance.actor"], ["music.recording.artist", "music.recording.releases"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_winner.awards_won", "award.award_honor.honored_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did christopher plummer play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.06007179617881775, -0.1440133899450302], "norm_scores": [0.5209730863571167, 0.4790269136428833]}}
{"id": "WebQTest-1005", "question": "what is capital city of morocco", "prediction": [["location.country.capital"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is capital city of morocco [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.034960612654685974, -0.17945098876953125], "norm_scores": [0.5360598564147949, 0.4639401137828827]}}
{"id": "WebQTest-1006", "question": "what movies is omar epps in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.film.starring"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies is omar epps in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.054896462708711624, -0.11365538835525513], "norm_scores": [0.5146855115890503, 0.4853144884109497]}}
{"id": "WebQTest-1007", "question": "what district does nancy pelosi represent", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.district_represented"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["government.politician.government_positions_held", "government.government_position_held.district_represented"], ["base.government2.elected_government_positions_held.officeholder", "base.government2.elected_government_positions_held.district_represented"], ["government.government_position_held.office_holder", "government.government_position_held.district_represented"], ["government.politician.government_positions_held", "government.political_district.representatives"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what district does nancy pelosi represent [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.district_represented </PATH>", "<PATH> people.person.nationality <SEP> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.0460774302482605, -0.08373824506998062], "norm_scores": [0.5094140768051147, 0.49058589339256287]}}
{"id": "WebQTest-1008", "question": "where is walton indiana", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"], ["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is walton indiana [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.027728969231247902, -0.3041907250881195], "norm_scores": [0.5686785578727722, 0.4313214421272278]}}
{"id": "WebQTest-1009", "question": "where did george herbert walker bush go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["base.yalebase.person.residential_college", "education.educational_institution.parent_institution"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did george herbert walker bush go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.05586487799882889, -0.0560820996761322], "norm_scores": [0.5000542998313904, 0.4999457001686096]}}
{"id": "WebQTest-1010", "question": "what did bruce jenner win gold medal for", "prediction": [["olympics.olympic_athlete.medals_won", "olympics.olympic_event_competition.medalists"], ["olympics.olympic_athlete.medals_won", "olympics.olympic_medal_honor.event"]], "ground_paths": [["olympics.olympic_athlete.medals_won", "olympics.olympic_event_competition.medalists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did bruce jenner win gold medal for [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_event_competition.medalists </PATH>", "<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_medal_honor.event </PATH>"], "scores": [-0.0159334484487772, -0.044669851660728455], "norm_scores": [0.5071836113929749, 0.49281638860702515]}}
{"id": "WebQTest-1011", "question": "where do they speak tibetan", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do they speak tibetan [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.04751669242978096, -0.15967585146427155], "norm_scores": [0.5280104279518127, 0.47198957204818726]}}
{"id": "WebQTest-1012", "question": "what to do in hollywood ca this weekend", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "ground_paths": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do in hollywood ca this weekend [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>"], "scores": [-0.051004808396101, -0.05210164934396744], "norm_scores": [0.5002742409706116, 0.4997258186340332]}}
{"id": "WebQTest-1014", "question": "where did kim kardashian come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kim kardashian come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.01753304712474346, -0.18208861351013184], "norm_scores": [0.5410463213920593, 0.45895370841026306]}}
{"id": "WebQTest-1015", "question": "what town did justin bieber grew up in", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.nationality"], ["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what town did justin bieber grew up in [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.05899711325764656, -0.12343180179595947], "norm_scores": [0.5161031484603882, 0.4838969111442566]}}
{"id": "WebQTest-1017", "question": "who plays charlie in the santa clause movies", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["film.performance.film", "film.actor.film"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays charlie in the santa clause movies [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.08079948276281357, -0.08447836339473724], "norm_scores": [0.5009196996688843, 0.49908027052879333]}}
{"id": "WebQTest-1020", "question": "what highschool did r. kelly attend", "prediction": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what highschool did r. kelly attend [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.04890913516283035, -0.0608924925327301], "norm_scores": [0.5029957890510559, 0.4970042109489441]}}
{"id": "WebQTest-1021", "question": "what happened at benghazi", "prediction": [["time.event.locations"], ["location.location.events"]], "ground_paths": [["time.event.locations"], ["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what happened at benghazi [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.02800881862640381, -0.27121931314468384], "norm_scores": [0.560504674911499, 0.4394952952861786]}}
{"id": "WebQTest-1022", "question": "what country is nike based in", "prediction": [["organization.organization.headquarters", "location.mailing_address.country"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"]], "ground_paths": [["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"], ["organization.organization.headquarters", "location.mailing_address.country"], ["organization.organization.phone_number", "common.phone_number.service_location"], ["organization.organization.founders", "people.person.nationality"], ["film.film.production_companies", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country is nike based in [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>"], "scores": [-0.014944495633244514, -0.10457363724708557], "norm_scores": [0.5223923325538635, 0.47760772705078125]}}
{"id": "WebQTest-1024", "question": "what kind of money do i bring to mexico", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money do i bring to mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.039382077753543854, -0.1222602128982544], "norm_scores": [0.5207076668739319, 0.4792923331260681]}}
{"id": "WebQTest-1025", "question": "who did steve spurrier play pro football for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["common.topic.notable_types"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did steve spurrier play pro football for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.022539883852005005, -0.11284275352954865], "norm_scores": [0.5225603580474854, 0.47743961215019226]}}
{"id": "WebQTest-1027", "question": "where do the blackhawks play", "prediction": [["sports.sports_facility.teams"], ["sports.sports_team.arena_stadium"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the blackhawks play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.arena_stadium </PATH>"], "scores": [-0.010066290386021137, -0.13009586930274963], "norm_scores": [0.5299714207649231, 0.4700285494327545]}}
{"id": "WebQTest-1028", "question": "who plays john connor", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays john connor [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.0737321525812149, -0.07585667073726654], "norm_scores": [0.5005311369895935, 0.4994688630104065]}}
{"id": "WebQTest-1029", "question": "what is the main language spoken in mexico", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the main language spoken in mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.08027782291173935, -0.10407059639692307], "norm_scores": [0.5059478878974915, 0.49405208230018616]}}
{"id": "WebQTest-1030", "question": "what are italians classified as", "prediction": [["people.ethnicity.included_in_group"], ["people.ethnicity.includes_groups"]], "ground_paths": [["people.ethnicity.includes_groups"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are italians classified as [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.included_in_group </PATH>", "<PATH> people.ethnicity.includes_groups </PATH>"], "scores": [-0.03775577247142792, -0.09290894120931625], "norm_scores": [0.5137847661972046, 0.486215204000473]}}
{"id": "WebQTest-1031", "question": "what has angelina jolie accomplished", "prediction": [["award.award_nominee.award_nominations", "award.award_nomination.award"], ["award.award_nomination.award_nominee", "award.award_nomination.award"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has angelina jolie accomplished [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.award </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award </PATH>"], "scores": [-0.06794831901788712, -0.07500709593296051], "norm_scores": [0.501764714717865, 0.4982353150844574]}}
{"id": "WebQTest-1032", "question": "who played captain kirk in star trek movie", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.quotations", "media_common.quotation.author"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played captain kirk in star trek movie [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.07224499434232712, -0.07793053984642029], "norm_scores": [0.5014213919639587, 0.49857860803604126]}}
{"id": "WebQTest-1033", "question": "who played dana scully in x files", "prediction": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "ground_paths": [["tv.tv_guest_role.character", "tv.tv_guest_role.actor"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.quotations", "people.person.quotations"], ["tv.tv_guest_role.character", "tv.tv_actor.guest_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["people.person.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played dana scully in x files [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>"], "scores": [-0.06909909844398499, -0.07228930294513702], "norm_scores": [0.5007975697517395, 0.4992024302482605]}}
{"id": "WebQTest-1035", "question": "what was f. scott fitzgerald", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was f. scott fitzgerald [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0032032050658017397, -0.345002144575119], "norm_scores": [0.5846274495124817, 0.4153725802898407]}}
{"id": "WebQTest-1036", "question": "what is the dominant language spoken in jamaica", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the dominant language spoken in jamaica [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.058197297155857086, -0.10946066677570343], "norm_scores": [0.5128130316734314, 0.4871869683265686]}}
{"id": "WebQTest-1037", "question": "who is the coach for the new york knicks", "prediction": [["basketball.basketball_team.head_coach"], ["basketball.basketball_coach.team"]], "ground_paths": [["basketball.basketball_team.head_coach"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the coach for the new york knicks [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_team.head_coach </PATH>", "<PATH> basketball.basketball_coach.team </PATH>"], "scores": [-0.01734142191708088, -0.1306481659412384], "norm_scores": [0.5282964110374451, 0.47170358896255493]}}
{"id": "WebQTest-1038", "question": "who was wesley snipes", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was wesley snipes [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0021255817264318466, -0.39756762981414795], "norm_scores": [0.5975920557975769, 0.4024079442024231]}}
{"id": "WebQTest-1039", "question": "where does sherrod brown live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does sherrod brown live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.046100784093141556, -0.0750364139676094], "norm_scores": [0.5072334408760071, 0.4927666187286377]}}
{"id": "WebQTest-1040", "question": "what kind of language does colombia speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language does colombia speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.029296064749360085, -0.08276688307523727], "norm_scores": [0.5133644938468933, 0.4866354763507843]}}
{"id": "WebQTest-1041", "question": "what currency does england use now", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does england use now [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.013980102725327015, -0.2226543128490448], "norm_scores": [0.5519800782203674, 0.44801995158195496]}}
{"id": "WebQTest-1042", "question": "who was the original voice of meg griffin on family guy", "prediction": [["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the original voice of meg griffin on family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.03630328178405762, -0.08598088473081589], "norm_scores": [0.5124168395996094, 0.48758313059806824]}}
{"id": "WebQTest-1043", "question": "what kind of government is mali", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government is mali [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.027326131239533424, -0.07113352417945862], "norm_scores": [0.5109500885009766, 0.48904991149902344]}}
{"id": "WebQTest-1044", "question": "who did cristiano ronaldo play for in 2010", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"]], "ground_paths": [["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player.transfers", "soccer.football_player_transfer.purchasing_team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did cristiano ronaldo play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>"], "scores": [-0.049458567053079605, -0.05890488252043724], "norm_scores": [0.502361536026001, 0.49763843417167664]}}
{"id": "WebQTest-1045", "question": "what is the zip code for morgantown wv", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for morgantown wv [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0003010207146871835, -0.5939182043075562], "norm_scores": [0.6441947221755981, 0.35580533742904663]}}
{"id": "WebQTest-1046", "question": "what is jacksonville fl timezone", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is jacksonville fl timezone [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-4.519740105024539e-05, -0.8991928696632385], "norm_scores": [0.710774302482605, 0.28922566771507263]}}
{"id": "WebQTest-1047", "question": "who does amy stiller play in dodgeball", "prediction": [["film.actor.film", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.film_character.portrayed_in_films"]], "ground_paths": [["film.actor.film", "film.performance.character"], ["film.film.starring", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does amy stiller play in dodgeball [/INST]", "raw_output": {"paths": ["<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.06998808681964874, -0.0787951722741127], "norm_scores": [0.5022017359733582, 0.49779823422431946]}}
{"id": "WebQTest-1048", "question": "who were king henry vii children", "prediction": [["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were king henry vii children [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.069096140563488, -0.0972653180360794], "norm_scores": [0.5070418119430542, 0.4929581582546234]}}
{"id": "WebQTest-1050", "question": "who does mila kunis play on family guy", "prediction": [["tv.regular_tv_appearance.series", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"]], "ground_paths": [["tv.tv_program.regular_cast", "tv.regular_tv_appearance.character"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["tv.tv_program_creator.programs_created", "fictional_universe.fictional_character.character_created_by"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does mila kunis play on family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.character </PATH>"], "scores": [-0.060173310339450836, -0.0675913542509079], "norm_scores": [0.5018544793128967, 0.4981454908847809]}}
{"id": "WebQTest-1051", "question": "when did the san francisco earthquake occur", "prediction": [["time.event.locations"], ["event.disaster.areas_affected"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the san francisco earthquake occur [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> event.disaster.areas_affected </PATH>"], "scores": [-0.0173132736235857, -0.22384348511695862], "norm_scores": [0.5514497756958008, 0.44855019450187683]}}
{"id": "WebQTest-1052", "question": "who played damien in the omen 2006", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"]], "ground_paths": [["film.performance.film", "film.actor.film"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played damien in the omen 2006 [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>"], "scores": [-0.06704464554786682, -0.09326989948749542], "norm_scores": [0.50655597448349, 0.4934440553188324]}}
{"id": "WebQTest-1054", "question": "who played denver in four christmases", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"]], "ground_paths": [["film.film.language", "people.person.languages"], ["film.performance.film", "film.performance.actor"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played denver in four christmases [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>"], "scores": [-0.07235647737979889, -0.11445579677820206], "norm_scores": [0.51052325963974, 0.48947674036026]}}
{"id": "WebQTest-1055", "question": "what to do with kids in essex", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do with kids in essex [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.04142705351114273, -0.05734718590974808], "norm_scores": [0.5039799213409424, 0.49602001905441284]}}
{"id": "WebQTest-1056", "question": "who did the cleveland cavaliers draft", "prediction": [["sports.sports_league_draft_pick.team", "sports.drafted_athlete.drafted"], ["sports.sports_league_draft_pick.team", "sports.sports_league_draft_pick.player"]], "ground_paths": [["sports.sports_team_roster.team", "sports.sports_team_roster.player"], ["sports.professional_sports_team.draft_picks", "sports.drafted_athlete.drafted"], ["sports.sports_league_draft_pick.team", "sports.drafted_athlete.drafted"], ["sports.sports_team_roster.team", "sports.pro_athlete.teams"], ["sports.professional_sports_team.draft_picks", "sports.sports_league_draft_pick.player"], ["sports.sports_league_draft_pick.team", "sports.sports_league_draft_pick.player"], ["sports.sports_team.roster", "sports.pro_athlete.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did the cleveland cavaliers draft [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_league_draft_pick.team <SEP> sports.drafted_athlete.drafted </PATH>", "<PATH> sports.sports_league_draft_pick.team <SEP> sports.sports_league_draft_pick.player </PATH>"], "scores": [-0.050599776208400726, -0.05392193794250488], "norm_scores": [0.5008305311203003, 0.4991694688796997]}}
{"id": "WebQTest-1057", "question": "what concentration camp did anne frank died in", "prediction": [["base.fictionaluniverse.deceased_fictional_character.place_of_death"], ["book.written_work.subjects"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what concentration camp did anne frank died in [/INST]", "raw_output": {"paths": ["<PATH> base.fictionaluniverse.deceased_fictional_character.place_of_death </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.017749803140759468, -0.22636936604976654], "norm_scores": [0.5519665479660034, 0.4480334520339966]}}
{"id": "WebQTest-1059", "question": "what language does east timor speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.official_language"], ["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does east timor speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.03428727015852928, -0.1038147360086441], "norm_scores": [0.5173748731613159, 0.4826251268386841]}}
{"id": "WebQTest-1060", "question": "what is the currency in italy", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency in italy [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.027143359184265137, -0.12029728293418884], "norm_scores": [0.5232716202735901, 0.4767283499240875]}}
{"id": "WebQTest-1061", "question": "where does greek language come from", "prediction": [["language.human_language.language_family"], ["base.rosetta.languoid.parent"]], "ground_paths": [["base.rosetta.languoid.child"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does greek language come from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>"], "scores": [-0.03774154558777809, -0.09897687286138535], "norm_scores": [0.5153040885925293, 0.4846959412097931]}}
{"id": "WebQTest-1062", "question": "what is the name of the currency used in italy", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the currency used in italy [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.028619980439543724, -0.11634376645088196], "norm_scores": [0.5219168663024902, 0.4780831038951874]}}
{"id": "WebQTest-1063", "question": "where is audrey hepburn from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is audrey hepburn from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0048370081931352615, -0.23942750692367554], "norm_scores": [0.558380126953125, 0.441619873046875]}}
{"id": "WebQTest-1065", "question": "who does darth vaders voice", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does darth vaders voice [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.06171620264649391, -0.07730112969875336], "norm_scores": [0.5038961172103882, 0.49610382318496704]}}
{"id": "WebQTest-1067", "question": "what school did douglas macarthur go to", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did douglas macarthur go to [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.05075488239526749, -0.05361270159482956], "norm_scores": [0.5007144212722778, 0.4992855489253998]}}
{"id": "WebQTest-1068", "question": "what famous people are from barbados", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"]], "ground_paths": [["location.location.contains", "location.location.people_born_here"], ["people.person.place_of_birth"], ["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what famous people are from barbados [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.06837671995162964, -0.10910629481077194], "norm_scores": [0.5101810097694397, 0.4898190200328827]}}
{"id": "WebQTest-1069", "question": "what is the name of the currency used in china", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the currency used in china [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.03689496964216232, -0.09844490140676498], "norm_scores": [0.5153826475143433, 0.4846173822879791]}}
{"id": "WebQTest-1070", "question": "what are the names of michael jackson 's 3 children", "prediction": [["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the names of michael jackson 's 3 children [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.05688472464680672, -0.12063330411911011], "norm_scores": [0.5159317851066589, 0.48406824469566345]}}
{"id": "WebQTest-1072", "question": "who is the current governor of arizona 2010", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "ground_paths": [["location.country.first_level_divisions", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the current governor of arizona 2010 [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.05106421932578087, -0.05349847674369812], "norm_scores": [0.5006085634231567, 0.49939143657684326]}}
{"id": "WebQTest-1074", "question": "who is lamar odom playing for this year", "prediction": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"]], "ground_paths": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is lamar odom playing for this year [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>"], "scores": [-0.04482084885239601, -0.052556853741407394], "norm_scores": [0.5019339919090271, 0.4980660080909729]}}
{"id": "WebQTest-1075", "question": "where were the great pyramids of giza built", "prediction": [["architecture.building.building_complex"], ["location.location.containedby"]], "ground_paths": [["architecture.building.building_complex"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where were the great pyramids of giza built [/INST]", "raw_output": {"paths": ["<PATH> architecture.building.building_complex </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.03953710198402405, -0.12837138772010803], "norm_scores": [0.5221940279006958, 0.477806031703949]}}
{"id": "WebQTest-1077", "question": "what language do the russian speak", "prediction": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do the russian speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.06001773476600647, -0.08913668245077133], "norm_scores": [0.5072792172431946, 0.4927207827568054]}}
{"id": "WebQTest-1078", "question": "what language do australia people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do australia people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.035084012895822525, -0.07022590190172195], "norm_scores": [0.5087845921516418, 0.49121543765068054]}}
{"id": "WebQTest-1079", "question": "what country was king tut 's tomb found", "prediction": [["people.person.nationality"], ["people.deceased_person.place_of_death"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country was king tut 's tomb found [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>"], "scores": [-0.0876021683216095, -0.10637379437685013], "norm_scores": [0.504692792892456, 0.49530723690986633]}}
{"id": "WebQTest-1080", "question": "where did abraham and his descendants live", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"], ["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did abraham and his descendants live [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.08421380817890167, -0.18855351209640503], "norm_scores": [0.5260612964630127, 0.4739386737346649]}}
{"id": "WebQTest-1081", "question": "where did george washington carver go to collage", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did george washington carver go to collage [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.05743320658802986, -0.06171248480677605], "norm_scores": [0.5010697841644287, 0.4989301562309265]}}
{"id": "WebQTest-1082", "question": "what are the sports in spain", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"]], "ground_paths": [["sports.sports_team.location"], ["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the sports in spain [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>"], "scores": [-0.04675093665719032, -0.05667095258831978], "norm_scores": [0.5024799704551697, 0.4975200295448303]}}
{"id": "WebQTest-1083", "question": "where president obama was born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where president obama was born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.013955733738839626, -0.17216025292873383], "norm_scores": [0.5394688844680786, 0.4605311453342438]}}
{"id": "WebQTest-1084", "question": "what sports do people play in poland", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"]], "ground_paths": [["sports.sports_team.location"], ["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sports do people play in poland [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>"], "scores": [-0.04616035148501396, -0.05724012851715088], "norm_scores": [0.502769947052002, 0.49723008275032043]}}
{"id": "WebQTest-1085", "question": "who was demi lovato dating", "prediction": [["music.artist.genre", "music.artist.genre"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["music.recording.featured_artists", "music.album.featured_artists"], ["people.person.ethnicity", "people.person.ethnicity"], ["music.composer.compositions", "music.composer.compositions"], ["people.person.languages", "people.person.languages"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["music.recording.artist", "music.composer.compositions"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.nationality", "people.person.nationality"], ["music.album.artist", "music.album.artist"], ["music.composer.compositions", "music.composition.composer"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["people.person.profession", "people.person.profession"], ["music.recording.artist", "music.composition.composer"], ["music.artist.album", "music.composer.compositions"], ["music.artist.label", "music.artist.label"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["base.popstra.friendship.participant", "base.popstra.celebrity.friendship"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["music.artist.genre", "music.artist.genre"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was demi lovato dating [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.100132517516613, -0.14298571646213531], "norm_scores": [0.510711669921875, 0.489288330078125]}}
{"id": "WebQTest-1087", "question": "what films has scarlett johansson been in", "prediction": [["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["people.person.languages", "film.film.language"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.performance.character"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["people.person.place_of_birth", "film.film.featured_film_locations"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.film.starring"], ["award.award_winner.awards_won", "award.award_honor.honored_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what films has scarlett johansson been in [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>"], "scores": [-0.08013784885406494, -0.08583324402570724], "norm_scores": [0.5014238357543945, 0.4985761344432831]}}
{"id": "WebQTest-1088", "question": "who originally voiced meg on family guy", "prediction": [["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_program.regular_cast", "tv.tv_actor.starring_roles"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who originally voiced meg on family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.057824376970529556, -0.07296588271856308], "norm_scores": [0.5037853121757507, 0.49621471762657166]}}
{"id": "WebQTest-1089", "question": "who played amy squirrel in bad teacher", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.film.language", "people.person.languages"], ["film.film.starring", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played amy squirrel in bad teacher [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.10518229007720947, -0.13563166558742523], "norm_scores": [0.5076117515563965, 0.49238821864128113]}}
{"id": "WebQTest-1090", "question": "where are boeing headquarters", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are boeing headquarters [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>"], "scores": [-0.014423451386392117, -0.11278186738491058], "norm_scores": [0.5245698094367981, 0.4754302203655243]}}
{"id": "WebQTest-1091", "question": "what type of religions are in the united states", "prediction": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of religions are in the united states [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>"], "scores": [-0.021182717755436897, -0.05512325093150139], "norm_scores": [0.5084843039512634, 0.4915156960487366]}}
{"id": "WebQTest-1092", "question": "who did viggo mortensen play in lord of the rings", "prediction": [["film.actor.film", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.film_character.portrayed_in_films"]], "ground_paths": [["film.film.story_by", "fictional_universe.fictional_character.character_created_by"], ["media_common.adaptation.adapted_from", "book.book_character.appears_in_book"], ["film.film.starring", "film.performance.character"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"], ["media_common.adapted_work.adaptations", "book.book.characters"], ["film.film_series.films_in_series", "book.book.characters"], ["film.performance.film", "film.performance.character"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["music.featured_artist.recordings", "book.book_character.appears_in_book"], ["film.actor.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did viggo mortensen play in lord of the rings [/INST]", "raw_output": {"paths": ["<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.06752627342939377, -0.08307811617851257], "norm_scores": [0.5038878917694092, 0.4961121082305908]}}
{"id": "WebQTest-1094", "question": "who was isaac newton and what did he do", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was isaac newton and what did he do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0040265158750116825, -0.3370039761066437], "norm_scores": [0.5824836492538452, 0.4175163507461548]}}
{"id": "WebQTest-1095", "question": "who did william shakespeare marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.children"]], "ground_paths": [["people.place_of_interment.interred_here", "people.deceased_person.place_of_burial"], ["people.person.parents", "people.person.children"], ["freebase.valuenotation.has_value", "freebase.valuenotation.has_value"], ["people.person.place_of_birth", "people.deceased_person.place_of_death"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"], ["book.book_subject.works", "book.book_subject.works"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did william shakespeare marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.0442960262298584, -0.18006089329719543], "norm_scores": [0.5338891744613647, 0.46611082553863525]}}
{"id": "WebQTest-1096", "question": "what movies queen latifah is in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"]], "ground_paths": [["people.person.languages", "film.film.language"], ["film.film.produced_by"], ["film.person_or_entity_appearing_in_film.films", "film.personal_film_appearance.film"], ["film.film.executive_produced_by"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["film.actor.film", "film.film.starring"], ["film.producer.film"], ["film.producer.films_executive_produced"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.personal_film_appearance.person", "film.personal_film_appearance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["music.recording.artist"], ["film.producer.film", "film.film.sequel"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies queen latifah is in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>"], "scores": [-0.09166470915079117, -0.10512140393257141], "norm_scores": [0.5033641457557678, 0.49663588404655457]}}
{"id": "WebQTest-1097", "question": "where did michelle obama graduated from", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did michelle obama graduated from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.05326233431696892, -0.05894327163696289], "norm_scores": [0.5014201998710632, 0.4985797703266144]}}
{"id": "WebQTest-1098", "question": "what kind of music did franz liszt compose", "prediction": [["music.artist.genre"], ["music.genre.artists"]], "ground_paths": [["music.genre.artists"], ["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of music did franz liszt compose [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>"], "scores": [-0.027276670560240746, -0.17521321773529053], "norm_scores": [0.5369167923927307, 0.4630831778049469]}}
{"id": "WebQTest-1099", "question": "what country does david luiz play for", "prediction": [["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"]], "ground_paths": [["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country does david luiz play for [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_team.player_statistics </PATH>"], "scores": [-0.08620928972959518, -0.09208795428276062], "norm_scores": [0.5014696717262268, 0.4985303282737732]}}
{"id": "WebQTest-1100", "question": "what document did thomas jefferson wrote", "prediction": [["book.written_work.author"], ["book.author.book_editions_published"]], "ground_paths": [["book.book_subject.works"], ["book.written_work.author"], ["book.written_work.subjects"], ["book.author.book_editions_published"], ["book.book_edition.author_editor"], []], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what document did thomas jefferson wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.book_editions_published </PATH>"], "scores": [-0.05815098434686661, -0.11060941219329834], "norm_scores": [0.5131115913391113, 0.4868883788585663]}}
{"id": "WebQTest-1101", "question": "who is martin luther king jr facts", "prediction": [["people.person.profession"], ["symbols.namesake.named_after"]], "ground_paths": [["base.activism.activist.area_of_activism"], ["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is martin luther king jr facts [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.010209674015641212, -0.31305769085884094], "norm_scores": [0.5751385688781738, 0.42486143112182617]}}
{"id": "WebQTest-1102", "question": "where is victoria british columbia", "prediction": [["location.location.containedby"], ["location.province.capital"]], "ground_paths": [["base.aareas.schema.administrative_area.capital"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is victoria british columbia [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.province.capital </PATH>"], "scores": [-0.05103699490427971, -0.10652178525924683], "norm_scores": [0.5138676166534424, 0.48613235354423523]}}
{"id": "WebQTest-1103", "question": "how many mary mary sisters", "prediction": [["music.artist.genre", "music.artist.genre"], ["people.person.parents", "people.person.children"]], "ground_paths": [["music.musical_group.member", "music.group_member.membership"], ["music.album.artist", "music.composition.composer"], ["music.artist.origin", "people.person.place_of_birth"], ["music.album.artist", "music.composer.compositions"], ["music.group_membership.group", "music.group_membership.member"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how many mary mary sisters [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.13367217779159546, -0.18386982381343842], "norm_scores": [0.5125467777252197, 0.4874532222747803]}}
{"id": "WebQTest-1104", "question": "what do we call the currency of france", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do we call the currency of france [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.03968100622296333, -0.09897138923406601], "norm_scores": [0.5148182511329651, 0.4851817190647125]}}
{"id": "WebQTest-1106", "question": "what team does david beckham play for in 2012", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["soccer.football_player_loan.player", "soccer.football_player_loan.lending_team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["book.written_work.subjects", "book.written_work.subjects"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does david beckham play for in 2012 [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.07423409819602966, -0.09018641710281372], "norm_scores": [0.5039879679679871, 0.49601200222969055]}}
{"id": "WebQTest-1107", "question": "what movies did nick cannon star in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["tv.tv_producer.programs_produced", "tv.tv_program.tv_producer"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.film.starring"], ["music.artist.track", "music.recording.releases"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did nick cannon star in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.06263811141252518, -0.12942633032798767], "norm_scores": [0.5166908502578735, 0.4833091199398041]}}
{"id": "WebQTest-1108", "question": "what radio station is npr on in nyc", "prediction": [["broadcast.radio_station.owner"], ["broadcast.radio_station_owner.radio_stations"]], "ground_paths": [["award.award_winner.awards_won", "award.award_honor.award_winner"], ["broadcast.producer.produces", "broadcast.producer.produces"], ["broadcast.producer.location", "broadcast.broadcast.area_served"], ["broadcast.radio_network.affiliates", "broadcast.radio_affiliation_duration.station"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what radio station is npr on in nyc [/INST]", "raw_output": {"paths": ["<PATH> broadcast.radio_station.owner </PATH>", "<PATH> broadcast.radio_station_owner.radio_stations </PATH>"], "scores": [-0.08901729434728622, -0.12782849371433258], "norm_scores": [0.5097015500068665, 0.49029842019081116]}}
{"id": "WebQTest-1109", "question": "who did tom landry play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did tom landry play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.028068622574210167, -0.03171636536717415], "norm_scores": [0.5009119510650635, 0.4990880787372589]}}
{"id": "WebQTest-1110", "question": "where was john jay born", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was john jay born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.04615640267729759, -0.06711581349372864], "norm_scores": [0.5052396655082703, 0.49476033449172974]}}
{"id": "WebQTest-1111", "question": "where is mount fuji located", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.contains"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mount fuji located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.059451472014188766, -0.13873356580734253], "norm_scores": [0.519810140132904, 0.48018985986709595]}}
{"id": "WebQTest-1113", "question": "where was toussaint l'ouverture born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was toussaint l'ouverture born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.018052784726023674, -0.14072009921073914], "norm_scores": [0.5306283831596375, 0.4693715572357178]}}
{"id": "WebQTest-1114", "question": "what countries makes up the uk", "prediction": [["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_area_type"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries makes up the uk [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>"], "scores": [-0.05374915897846222, -0.0929010733962059], "norm_scores": [0.5097867250442505, 0.4902132749557495]}}
{"id": "WebQTest-1115", "question": "what is lawton ok zip code", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is lawton ok zip code [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0008040457614697516, -0.49941617250442505], "norm_scores": [0.6221331357955933, 0.37786686420440674]}}
{"id": "WebQTest-1116", "question": "what is the capital city of spain wiki answers", "prediction": [["location.country.capital"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital city of spain wiki answers [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.041568364948034286, -0.17921558022499084], "norm_scores": [0.5343575477600098, 0.46564242243766785]}}
{"id": "WebQTest-1117", "question": "where do they speak german", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do they speak german [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.05974559113383293, -0.10519125312566757], "norm_scores": [0.511359453201294, 0.48864054679870605]}}
{"id": "WebQTest-1118", "question": "when did mark mcgwire retired", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.season"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.season"]], "ground_paths": [["baseball.lifetime_batting_statistics.player", "baseball.lifetime_batting_statistics.ending_season"], ["baseball.batting_statistics.player", "baseball.batting_statistics.season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did mark mcgwire retired [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.season </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.season </PATH>"], "scores": [-0.036275263875722885, -0.03917321935296059], "norm_scores": [0.5007244944572449, 0.4992755055427551]}}
{"id": "WebQTest-1119", "question": "what is colorado technical university", "prediction": [["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"], ["location.location.containedby"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is colorado technical university [/INST]", "raw_output": {"paths": ["<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0713682472705841, -0.12689054012298584], "norm_scores": [0.5138769745826721, 0.4861229956150055]}}
{"id": "WebQTest-1120", "question": "what was the currency used in france before the euro", "prediction": [["location.country.currency_formerly_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the currency used in france before the euro [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.02649669162929058, -0.10268562287092209], "norm_scores": [0.5190380215644836, 0.48096197843551636]}}
{"id": "WebQTest-1121", "question": "what is kentucky state slogan", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"]], "ground_paths": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is kentucky state slogan [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.02748953178524971, -0.04822180047631264], "norm_scores": [0.5051828622817993, 0.4948171377182007]}}
{"id": "WebQTest-1124", "question": "who played samantha stephens in bewitched", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"]], "ground_paths": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_honor.honored_for", "award.award_honor.award_winner"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["tv.tv_program.regular_cast", "tv.tv_actor.starring_roles"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played samantha stephens in bewitched [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.04971785843372345, -0.09463164955377579], "norm_scores": [0.5112265348434448, 0.4887734353542328]}}
{"id": "WebQTest-1125", "question": "what area of science did sir isaac newton study", "prediction": [["people.person.profession"], ["education.education.student", "education.education.major_field_of_study"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what area of science did sir isaac newton study [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>"], "scores": [-0.034512266516685486, -0.0846991017460823], "norm_scores": [0.5125440955162048, 0.48745590448379517]}}
{"id": "WebQTest-1126", "question": "what county is san diego in", "prediction": [["location.us_county.hud_county_place"], ["location.hud_county_place.county"]], "ground_paths": [["location.location.containedby"], ["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is san diego in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.06535860151052475, -0.08466093242168427], "norm_scores": [0.5048254132270813, 0.4951745569705963]}}
{"id": "WebQTest-1127", "question": "which country in north america is divided into provinces", "prediction": [["location.location.containedby"], ["base.locations.countries.continent"]], "ground_paths": [["base.locations.continents.countries_within"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country in north america is divided into provinces [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.08841099590063095, -0.09970970451831818], "norm_scores": [0.5028246641159058, 0.4971753656864166]}}
{"id": "WebQTest-1130", "question": "what is real name of santa claus", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["fictional_universe.fictional_character.character_created_by"]], "ground_paths": [["fictional_universe.fictional_character.based_on"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is real name of santa claus [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>"], "scores": [-0.01568666100502014, -0.12175153195858002], "norm_scores": [0.5264913439750671, 0.4735085964202881]}}
{"id": "WebQTest-1131", "question": "when did kelly slater go pro", "prediction": [["sports.sports_award_winner.awards", "sports.sports_award.season"], ["sports.sports_award.award_winner", "sports.sports_award.season"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did kelly slater go pro [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.season </PATH>"], "scores": [-0.04481929540634155, -0.09741277247667313], "norm_scores": [0.5131453275680542, 0.4868546724319458]}}
{"id": "WebQTest-1132", "question": "what did coretta scott king died from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did coretta scott king died from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.004884045105427504, -0.18044689297676086], "norm_scores": [0.5437783002853394, 0.45622169971466064]}}
{"id": "WebQTest-1133", "question": "what does wh smith stand for", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does wh smith stand for [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.11035051941871643, -0.18542762100696564], "norm_scores": [0.5187605023384094, 0.48123952746391296]}}
{"id": "WebQTest-1135", "question": "who was vp for nixon", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"]], "ground_paths": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vp for nixon [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>"], "scores": [-0.02053002640604973, -0.07467308640480042], "norm_scores": [0.5135324597358704, 0.48646751046180725]}}
{"id": "WebQTest-1136", "question": "what religion did adolf hitler follow", "prediction": [["people.person.religion"], ["religion.religion.notable_figures"]], "ground_paths": [["people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religion did adolf hitler follow [/INST]", "raw_output": {"paths": ["<PATH> people.person.religion </PATH>", "<PATH> religion.religion.notable_figures </PATH>"], "scores": [-0.006431073881685734, -0.21780917048454285], "norm_scores": [0.5526486039161682, 0.4473513662815094]}}
{"id": "WebQTest-1137", "question": "where is the illinois river on a map", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the illinois river on a map [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.04858817532658577, -0.13130132853984833], "norm_scores": [0.5206664800643921, 0.47933346033096313]}}
{"id": "WebQTest-1138", "question": "where honduras in the world", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where honduras in the world [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0854206383228302, -0.11851052939891815], "norm_scores": [0.5082716941833496, 0.491728276014328]}}
{"id": "WebQTest-1142", "question": "who does ronaldinho play for now 2011", "prediction": [["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "ground_paths": [["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does ronaldinho play for now 2011 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_player_stats.team </PATH>"], "scores": [-0.07573912292718887, -0.08521125465631485], "norm_scores": [0.5023680329322815, 0.4976319670677185]}}
{"id": "WebQTest-1143", "question": "what galileo galilei was famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["symbols.name_source.namesakes", "astronomy.star_system_body.star_system"], ["influence.influence_node.influenced", "base.argumentmaps.innovator.original_ideas"], ["astronomy.astronomical_discovery.discoverer", "astronomy.star_system_body.star_system"], ["astronomy.astronomer.astronomical_objects_discovered", "astronomy.star_system_body.star_system"], ["book.book_edition.author_editor", "book.written_work.subjects"], ["book.written_work.author", "book.book_subject.works"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what galileo galilei was famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.007563120685517788, -0.32095664739608765], "norm_scores": [0.5777133703231812, 0.42228662967681885]}}
{"id": "WebQTest-1144", "question": "what are the major trading partners of germany", "prediction": [["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"], ["location.statistical_region.places_exported_to", "location.imports_and_exports.exported_to"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the major trading partners of germany [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.places_exported_to <SEP> location.statistical_region.places_imported_from </PATH>", "<PATH> location.statistical_region.places_exported_to <SEP> location.imports_and_exports.exported_to </PATH>"], "scores": [-0.0943186953663826, -0.10217906534671783], "norm_scores": [0.5019651055335999, 0.49803492426872253]}}
{"id": "WebQTest-1145", "question": "where did romney graduated college", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.religion", "religion.religion.organizations"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did romney graduated college [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.04776856675744057, -0.06167170777916908], "norm_scores": [0.5034757256507874, 0.49652427434921265]}}
{"id": "WebQTest-1146", "question": "who did johnny depp play in corpse bride", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["film.performance.film", "film.performance.character"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did johnny depp play in corpse bride [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.05431979522109032, -0.0985519140958786], "norm_scores": [0.5110561847686768, 0.48894378542900085]}}
{"id": "WebQTest-1147", "question": "what is the zip code for jonesboro ar", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for jonesboro ar [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.00036009051837027073, -0.5795441269874573], "norm_scores": [0.6408796310424805, 0.35912033915519714]}}
{"id": "WebQTest-1148", "question": "what college did se hinton go", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did se hinton go [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.0582905113697052, -0.058517370373010635], "norm_scores": [0.5000567436218262, 0.4999432861804962]}}
{"id": "WebQTest-1149", "question": "when was taylor swift fearless tour", "prediction": [["music.concert_tour.artist"], ["time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was taylor swift fearless tour [/INST]", "raw_output": {"paths": ["<PATH> music.concert_tour.artist </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.018399817869067192, -0.27970805764198303], "norm_scores": [0.564957857131958, 0.435042142868042]}}
{"id": "WebQTest-1151", "question": "where did charles darwin and his wife live", "prediction": [["people.person.nationality", "location.location.containedby"], ["book.written_work.subjects", "location.location.contains"]], "ground_paths": [["book.written_work.subjects", "location.location.contains"], ["people.deceased_person.place_of_burial", "travel.travel_destination.tourist_attractions"], ["influence.influence_node.influenced", "people.person.place_of_birth"], ["book.written_work.subjects"], ["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["people.person.children", "people.deceased_person.place_of_death"], ["people.person.nationality", "location.country.capital"], ["book.written_work.subjects", "location.country.capital"], ["people.person.places_lived", "people.place_lived.location"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["book.book_subject.works", "book.written_work.subjects"], ["people.person.nationality", "location.location.containedby"], ["book.written_work.subjects", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did charles darwin and his wife live [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.contains </PATH>"], "scores": [-0.11827544122934341, -0.14469310641288757], "norm_scores": [0.506604015827179, 0.49339595437049866]}}
{"id": "WebQTest-1152", "question": "who is the voice of kitt", "prediction": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_guest_role.character", "tv.tv_guest_role.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the voice of kitt [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.0962323397397995, -0.10901235044002533], "norm_scores": [0.5031949281692505, 0.49680501222610474]}}
{"id": "WebQTest-1153", "question": "what is eminem 's real full name", "prediction": [["people.person.nationality", "people.person.nationality"], ["music.artist.album", "music.composer.compositions"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is eminem 's real full name [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> music.artist.album <SEP> music.composer.compositions </PATH>"], "scores": [-0.11609254777431488, -0.15207155048847198], "norm_scores": [0.5089937448501587, 0.4910062253475189]}}
{"id": "WebQTest-1154", "question": "who is amy grant 's first husband", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.marriage.spouse"]], "ground_paths": [["people.person.parents", "people.person.children"], ["music.artist.origin", "music.artist.origin"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"], ["common.topic.notable_types", "common.topic.notable_types"], ["music.artist.genre", "music.artist.genre"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.parents", "people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is amy grant 's first husband [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>"], "scores": [-0.0635286495089531, -0.10992992669343948], "norm_scores": [0.5115982294082642, 0.48840177059173584]}}
{"id": "WebQTest-1155", "question": "what are the seven nations of central america", "prediction": [["location.location.contains"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the seven nations of central america [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.057843249291181564, -0.13690581917762756], "norm_scores": [0.5197553634643555, 0.4802446663379669]}}
{"id": "WebQTest-1156", "question": "what is jamie spears baby name", "prediction": [["people.person.children"], ["people.person.parents"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is jamie spears baby name [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.05614640936255455, -0.11586781591176987], "norm_scores": [0.5149258971214294, 0.48507410287857056]}}
{"id": "WebQTest-1160", "question": "who is the mother of prince michael jackson", "prediction": [["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the mother of prince michael jackson [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.04402313753962517, -0.2097003310918808], "norm_scores": [0.54132479429245, 0.45867517590522766]}}
{"id": "WebQTest-1161", "question": "what is the clemson mascot name", "prediction": [["education.school_mascot.school"], ["education.educational_institution.mascot"]], "ground_paths": [["education.school_mascot.school"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the clemson mascot name [/INST]", "raw_output": {"paths": ["<PATH> education.school_mascot.school </PATH>", "<PATH> education.educational_institution.mascot </PATH>"], "scores": [-0.05019237846136093, -0.050594717264175415], "norm_scores": [0.5001005530357361, 0.49989941716194153]}}
{"id": "WebQTest-1163", "question": "what rainforest is in south america", "prediction": [["location.location.containedby"], ["common.topic.notable_types"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what rainforest is in south america [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.019168710336089134, -0.1961081176996231], "norm_scores": [0.5441198348999023, 0.45588019490242004]}}
{"id": "WebQTest-1164", "question": "what type of art does claude monet do", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art does claude monet do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.017740463837981224, -0.0884658694267273], "norm_scores": [0.5176739692687988, 0.48232603073120117]}}
{"id": "WebQTest-1166", "question": "when did we start war with iraq", "prediction": [["base.culturalevent.event.entity_involved"], ["time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did we start war with iraq [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.08916550874710083, -0.1615593135356903], "norm_scores": [0.518090546131134, 0.4819094240665436]}}
{"id": "WebQTest-1168", "question": "what guitar did kurt cobain", "prediction": [["music.group_member.instruments_played"], ["music.guitarist.guitars_played"]], "ground_paths": [["music.guitarist.guitars_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what guitar did kurt cobain [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.guitarist.guitars_played </PATH>"], "scores": [-0.04688110202550888, -0.13997767865657806], "norm_scores": [0.5232573747634888, 0.4767426550388336]}}
{"id": "WebQTest-1169", "question": "who did jennifer lawrence play in x men", "prediction": [["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["film.performance.actor", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.film_character.portrayed_in_films"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did jennifer lawrence play in x men [/INST]", "raw_output": {"paths": ["<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.11155527085065842, -0.11655867844820023], "norm_scores": [0.5012508630752563, 0.49874913692474365]}}
{"id": "WebQTest-1170", "question": "who is the governor of virginia 2011", "prediction": [["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the governor of virginia 2011 [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> people.person.nationality </PATH>"], "scores": [-0.03711461275815964, -0.04892497882246971], "norm_scores": [0.5029525756835938, 0.49704742431640625]}}
{"id": "WebQTest-1171", "question": "who did darth vaders voice", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did darth vaders voice [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.06513748317956924, -0.0803360790014267], "norm_scores": [0.5037996172904968, 0.49620041251182556]}}
{"id": "WebQTest-1173", "question": "where is walton county ga", "prediction": [["location.location.containedby"], ["location.us_county.hud_county_place"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is walton county ga [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.hud_county_place </PATH>"], "scores": [-0.045056335628032684, -0.10593640804290771], "norm_scores": [0.5152153372764587, 0.48478472232818604]}}
{"id": "WebQTest-1174", "question": "what cancer did jackie kennedy die of", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what cancer did jackie kennedy die of [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.04521877318620682, -0.04899033531546593], "norm_scores": [0.5009428858757019, 0.4990571141242981]}}
{"id": "WebQTest-1175", "question": "what does lauren conrad major in", "prediction": [["people.person.education", "education.education.major_field_of_study"], ["education.education.student", "education.education.major_field_of_study"]], "ground_paths": [["people.person.education", "education.field_of_study.students_majoring"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does lauren conrad major in [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>"], "scores": [-0.022936765104532242, -0.051827069371938705], "norm_scores": [0.507222056388855, 0.49277791380882263]}}
{"id": "WebQTest-1176", "question": "what time zone is pennsylvania in right now", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is pennsylvania in right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>"], "scores": [-8.079963299678639e-05, -0.4107367992401123], "norm_scores": [0.6012451648712158, 0.3987548351287842]}}
{"id": "WebQTest-1177", "question": "where did mario lopez go to school", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did mario lopez go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.04269711673259735, -0.0786035880446434], "norm_scores": [0.5089756846427917, 0.49102434515953064]}}
{"id": "WebQTest-1178", "question": "where is tupac from originally", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is tupac from originally [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.019178936257958412, -0.14611589908599854], "norm_scores": [0.5316916704177856, 0.46830829977989197]}}
{"id": "WebQTest-1179", "question": "what role did george lucas play in star wars", "prediction": [["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "ground_paths": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.film_character.portrayed_in_films"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what role did george lucas play in star wars [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>"], "scores": [-0.07132892310619354, -0.1552547812461853], "norm_scores": [0.5209691524505615, 0.4790308475494385]}}
{"id": "WebQTest-1180", "question": "what sarah dessen books are movies", "prediction": [["film.film_story_contributor.film_story_credits"], ["book.author.works_written"]], "ground_paths": [["film.film.story_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sarah dessen books are movies [/INST]", "raw_output": {"paths": ["<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.0839989110827446, -0.1712179034948349], "norm_scores": [0.5217909216880798, 0.4782090485095978]}}
{"id": "WebQTest-1181", "question": "what is the current time in kauai hawaii", "prediction": [["location.location.time_zones"], ["geography.island.island_group", "location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the current time in kauai hawaii [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> geography.island.island_group <SEP> location.location.time_zones </PATH>"], "scores": [-0.001249563181772828, -0.2571665346622467], "norm_scores": [0.5636323690414429, 0.4363676905632019]}}
{"id": "WebQTest-1183", "question": "where was farrah fawcett buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"]], "ground_paths": [["people.deceased_person.place_of_burial"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was farrah fawcett buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>"], "scores": [-0.008939760737121105, -0.14120297133922577], "norm_scores": [0.5330176949501038, 0.46698233485221863]}}
{"id": "WebQTest-1185", "question": "what inspired steinbeck", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "ground_paths": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inspired steinbeck [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.03526439890265465, -0.04698904976248741], "norm_scores": [0.5029311180114746, 0.4970688819885254]}}
{"id": "WebQTest-1186", "question": "what kind of government does cuba have today", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does cuba have today [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.04176852107048035, -0.04849652573466301], "norm_scores": [0.5016820430755615, 0.49831801652908325]}}
{"id": "WebQTest-1187", "question": "where are english speakers distributed", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are english speakers distributed [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.04998178780078888, -0.10789263993501663], "norm_scores": [0.5144736170768738, 0.48552632331848145]}}
{"id": "WebQTest-1189", "question": "what is the capital of italy today", "prediction": [["location.country.capital"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of italy today [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.07460974901914597, -0.14180797338485718], "norm_scores": [0.5167932510375977, 0.48320674896240234]}}
{"id": "WebQTest-1191", "question": "what language do people speak in the bahamas", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people speak in the bahamas [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.022983940318226814, -0.12524999678134918], "norm_scores": [0.5255442261695862, 0.47445571422576904]}}
{"id": "WebQTest-1192", "question": "where did george michael go to school", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did george michael go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.04897362366318703, -0.05453949794173241], "norm_scores": [0.5013914704322815, 0.4986084997653961]}}
{"id": "WebQTest-1194", "question": "what club does cristiano ronaldo play for in 2010", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"]], "ground_paths": [["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player.transfers", "soccer.football_player_transfer.purchasing_team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what club does cristiano ronaldo play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>"], "scores": [-0.05200985446572304, -0.058356452733278275], "norm_scores": [0.5015866160392761, 0.4984133541584015]}}
{"id": "WebQTest-1195", "question": "what time zone is in the bahamas", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is in the bahamas [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>"], "scores": [-4.054966848343611e-05, -0.526746392250061], "norm_scores": [0.6287145018577576, 0.37128549814224243]}}
{"id": "WebQTest-1197", "question": "what language do people speak in brazil", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people speak in brazil [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.002684241160750389, -0.35662323236465454], "norm_scores": [0.587572455406189, 0.41242754459381104]}}
{"id": "WebQTest-1198", "question": "who influenced samuel taylor coleridge", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who influenced samuel taylor coleridge [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.023559480905532837, -0.06895691901445389], "norm_scores": [0.5113474130630493, 0.48865261673927307]}}
{"id": "WebQTest-1199", "question": "what does kim kardashian work as", "prediction": [["people.person.profession"], ["freebase.valuenotation.is_reviewed", "common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does kim kardashian work as [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> common.topic.notable_types </PATH>"], "scores": [-0.005638774950057268, -0.20744651556015015], "norm_scores": [0.5502814054489136, 0.4497185945510864]}}
{"id": "WebQTest-1200", "question": "where did the mississippi river end", "prediction": [["geography.river.mouth"], ["location.location.containedby"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the mississippi river end [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.008011627942323685, -0.31522226333618164], "norm_scores": [0.576204240322113, 0.4237957298755646]}}
{"id": "WebQTest-1201", "question": "who played carlton lassiter on psych", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played carlton lassiter on psych [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.0595904141664505, -0.06714778393507004], "norm_scores": [0.5018893480300903, 0.49811065196990967]}}
{"id": "WebQTest-1203", "question": "who is president of israel", "prediction": [["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is president of israel [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.039910029619932175, -0.050076670944690704], "norm_scores": [0.5025416612625122, 0.4974583685398102]}}
{"id": "WebQTest-1204", "question": "where did the anasazi indians live", "prediction": [["people.ethnicity.geographic_distribution"], ["location.location.contains"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the anasazi indians live [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0032283540349453688, -0.5246323347091675], "norm_scores": [0.6274759769439697, 0.3725239932537079]}}
{"id": "WebQTest-1205", "question": "who plays harley quinn", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["cvg.game_performance.character", "cvg.game_performance.voice_actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays harley quinn [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.04460978880524635, -0.08496389538049698], "norm_scores": [0.5100871920585632, 0.48991286754608154]}}
{"id": "WebQTest-1206", "question": "what was francis bacon contributions", "prediction": [["people.person.profession"], ["law.invention.inventor"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was francis bacon contributions [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> law.invention.inventor </PATH>"], "scores": [-0.033557821065187454, -0.20429633557796478], "norm_scores": [0.5425812005996704, 0.4574187695980072]}}
{"id": "WebQTest-1209", "question": "when does ziva start on ncis", "prediction": [["tv.tv_series_season.series"], ["tv.tv_program.seasons"]], "ground_paths": [["tv.tv_program.seasons"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when does ziva start on ncis [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_series_season.series </PATH>", "<PATH> tv.tv_program.seasons </PATH>"], "scores": [-0.05674705281853676, -0.17319227755069733], "norm_scores": [0.529078483581543, 0.4709215462207794]}}
{"id": "WebQTest-1210", "question": "what is the name of the national anthem of canada ( 2 words )", "prediction": [["music.composition.language"], ["music.composition.language", "music.composition.language"]], "ground_paths": [["location.country.official_language", "music.composition.language"], ["location.country.national_anthem", "government.national_anthem.national_anthem_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the national anthem of canada ( 2 words ) [/INST]", "raw_output": {"paths": ["<PATH> music.composition.language </PATH>", "<PATH> music.composition.language <SEP> music.composition.language </PATH>"], "scores": [-0.07422631978988647, -0.08370278775691986], "norm_scores": [0.5023690462112427, 0.49763089418411255]}}
{"id": "WebQTest-1212", "question": "what did mark zuckerberg study", "prediction": [["people.person.education", "education.education.major_field_of_study"], ["education.education.student", "education.education.major_field_of_study"]], "ground_paths": [["people.person.profession", "base.skills.skill.performed_by"], ["education.education.student", "education.education.major_field_of_study"], ["people.person.education", "education.field_of_study.students_majoring"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did mark zuckerberg study [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>"], "scores": [-0.025649210438132286, -0.04695053771138191], "norm_scores": [0.5053250789642334, 0.4946748614311218]}}
{"id": "WebQTest-1213", "question": "which country was michael jackson born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country was michael jackson born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.05620608106255531, -0.07739586383104324], "norm_scores": [0.5052972435951233, 0.4947027564048767]}}
{"id": "WebQTest-1215", "question": "who was stephen r covey", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was stephen r covey [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.005740309599786997, -0.2878335118293762], "norm_scores": [0.5700592994689941, 0.42994070053100586]}}
{"id": "WebQTest-1216", "question": "what country was stalin in charge of", "prediction": [["government.government_position_held.office_holder", "government.governmental_jurisdiction.governing_officials"], ["people.person.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country was stalin in charge of [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.04526539146900177, -0.07825703918933868], "norm_scores": [0.5082471370697021, 0.49175283312797546]}}
{"id": "WebQTest-1217", "question": "what is monta ellis career high points", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"]], "ground_paths": [["sports.sports_award_winner.awards", "sports.sports_award.award"], ["award.award_honor.award_winner", "award.award_honor.award"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is monta ellis career high points [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.season </PATH>"], "scores": [-0.03144891560077667, -0.044094834476709366], "norm_scores": [0.5031614303588867, 0.4968385398387909]}}
{"id": "WebQTest-1219", "question": "what nationality is rihanna", "prediction": [["people.person.nationality"], ["people.ethnicity.people"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what nationality is rihanna [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.ethnicity.people </PATH>"], "scores": [-0.058396756649017334, -0.15129652619361877], "norm_scores": [0.5232082009315491, 0.47679173946380615]}}
{"id": "WebQTest-1220", "question": "what has shannen doherty played in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["people.person.languages", "film.film.language"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.languages", "tv.tv_program.languages"], ["film.actor.film", "film.film.starring"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has shannen doherty played in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.06137536093592644, -0.12172187864780426], "norm_scores": [0.515082061290741, 0.4849179685115814]}}
{"id": "WebQTest-1221", "question": "what college did joakim noah", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did joakim noah [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.05593525245785713, -0.07201822847127914], "norm_scores": [0.5040206909179688, 0.49597933888435364]}}
{"id": "WebQTest-1223", "question": "who plays billy elliot", "prediction": [["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "ground_paths": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["film.performance.film", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["media_common.netflix_title.netflix_genres", "people.person.nationality"], ["award.award_honor.honored_for", "award.award_winner.awards_won"], ["award.award_honor.honored_for", "award.award_honor.award_winner"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["film.film.language", "people.person.languages"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays billy elliot [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.07462606579065323, -0.08114787936210632], "norm_scores": [0.5016304850578308, 0.4983695447444916]}}
{"id": "WebQTest-1225", "question": "what kourtney kardashian boyfriends name", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["people.person.parents", "people.person.children"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kourtney kardashian boyfriends name [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.05090170353651047, -0.0952872484922409], "norm_scores": [0.5110945701599121, 0.4889054298400879]}}
{"id": "WebQTest-1226", "question": "what form of government does australia use", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does australia use [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>"], "scores": [-0.029650766402482986, -0.06393807381391525], "norm_scores": [0.5085709691047668, 0.49142903089523315]}}
{"id": "WebQTest-1227", "question": "what team did howie long", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did howie long [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.029583461582660675, -0.031907882541418076], "norm_scores": [0.5005810856819153, 0.49941888451576233]}}
{"id": "WebQTest-1228", "question": "where did the greek alphabet come from", "prediction": [["language.human_language.language_family"], ["language.language_family.languages"]], "ground_paths": [["language.language_writing_system.child_writing_systems"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the greek alphabet come from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> language.language_family.languages </PATH>"], "scores": [-0.09416400641202927, -0.10385061055421829], "norm_scores": [0.5024216175079346, 0.49757835268974304]}}
{"id": "WebQTest-1229", "question": "what timezone is elizabethtown kentucky", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is elizabethtown kentucky [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-4.7364679630845785e-05, -0.9006156325340271], "norm_scores": [0.7110662460327148, 0.28893375396728516]}}
{"id": "WebQTest-1230", "question": "what to see in christchurch new zealand", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "ground_paths": [["location.location.contains"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to see in christchurch new zealand [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>"], "scores": [-0.04187791422009468, -0.04333079978823662], "norm_scores": [0.5003631711006165, 0.49963676929473877]}}
{"id": "WebQTest-1231", "question": "where is chris paul from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is chris paul from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.04427862539887428, -0.0752202644944191], "norm_scores": [0.5077347755432129, 0.4922651946544647]}}
{"id": "WebQTest-1232", "question": "what year was albert pujols rookie year", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.season"], ["baseball.batting_statistics.player", "baseball.batting_statistics.season"]], "ground_paths": [["sports.sports_award.award_winner", "sports.sports_league_season.awards"], ["baseball.lifetime_batting_statistics.player", "baseball.lifetime_batting_statistics.starting_season"], ["baseball.batting_statistics.player", "baseball.batting_statistics.season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was albert pujols rookie year [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.season </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.season </PATH>"], "scores": [-0.03644329681992531, -0.038271401077508926], "norm_scores": [0.5004570484161377, 0.4995429813861847]}}
{"id": "WebQTest-1233", "question": "what are the colors of the ny giants", "prediction": [["sports.sports_team.colors"], ["sports.professional_sports_team.owner_s", "sports.sports_team.colors"]], "ground_paths": [["sports.sports_team.colors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the colors of the ny giants [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.colors </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.colors </PATH>"], "scores": [-0.00019799846631940454, -0.27845776081085205], "norm_scores": [0.5691195130348206, 0.43088048696517944]}}
{"id": "WebQTest-1234", "question": "what business did andrew carnegie dominate", "prediction": [["people.person.profession"], ["book.written_work.subjects"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what business did andrew carnegie dominate [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.06213800609111786, -0.21641196310520172], "norm_scores": [0.5384922027587891, 0.4615078270435333]}}
{"id": "WebQTest-1235", "question": "what was sir isaac newton 's inventions", "prediction": [["law.invention.inventor"], ["law.inventor.inventions"]], "ground_paths": [["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was sir isaac newton 's inventions [/INST]", "raw_output": {"paths": ["<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.inventions </PATH>"], "scores": [-0.02227639965713024, -0.15225398540496826], "norm_scores": [0.5324487090110779, 0.46755126118659973]}}
{"id": "WebQTest-1237", "question": "who did shaq first play for", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"]], "ground_paths": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did shaq first play for [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>"], "scores": [-0.04955616965889931, -0.05153884366154671], "norm_scores": [0.5004956722259521, 0.49950435757637024]}}
{"id": "WebQTest-1238", "question": "who was the artist of mickey mouse", "prediction": [["fictional_universe.fictional_character.character_created_by"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character_creator.fictional_characters_created"], ["fictional_universe.fictional_character.character_created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the artist of mickey mouse [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.002909485250711441, -0.20259208977222443], "norm_scores": [0.5497553944587708, 0.4502445459365845]}}
{"id": "WebQTest-1239", "question": "what is the zip code for concord", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for concord [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.000643744133412838, -0.5285605788230896], "norm_scores": [0.6289970874786377, 0.3710028827190399]}}
{"id": "WebQTest-1240", "question": "what football team is michael vick on", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what football team is michael vick on [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> american_football.football_player.rushing <SEP> american_football.player_rushing_statistics.team </PATH>"], "scores": [-0.04830440878868103, -0.09335502982139587], "norm_scores": [0.5112607479095459, 0.4887392818927765]}}
{"id": "WebQTest-1241", "question": "where does the columbia river start", "prediction": [["geography.lake.outflow"], ["geography.river.origin"]], "ground_paths": [["geography.lake.outflow"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the columbia river start [/INST]", "raw_output": {"paths": ["<PATH> geography.lake.outflow </PATH>", "<PATH> geography.river.origin </PATH>"], "scores": [-0.03427494317293167, -0.166059672832489], "norm_scores": [0.5328986048698425, 0.46710142493247986]}}
{"id": "WebQTest-1242", "question": "what are all the names of the harry potter books", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes"], ["book.book_character.appears_in_book"]], "ground_paths": [["fictional_universe.fictional_universe.works_set_here"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes"], ["book.book_character.appears_in_book"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are all the names of the harry potter books [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes </PATH>", "<PATH> book.book_character.appears_in_book </PATH>"], "scores": [-0.020578615367412567, -0.10670638084411621], "norm_scores": [0.5215186476707458, 0.47848135232925415]}}
{"id": "WebQTest-1244", "question": "who was sir walter raleigh and what did he do", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was sir walter raleigh and what did he do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.005309758707880974, -0.31704390048980713], "norm_scores": [0.5773084759712219, 0.4226915240287781]}}
{"id": "WebQTest-1245", "question": "what countries are in the euro dollar", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are in the euro dollar [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.06010545417666435, -0.08385387808084488], "norm_scores": [0.5059368014335632, 0.4940631687641144]}}
{"id": "WebQTest-1246", "question": "what currency can you use in barbados", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency can you use in barbados [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.06278757750988007, -0.06394906342029572], "norm_scores": [0.5002903938293457, 0.4997096359729767]}}
{"id": "WebQTest-1247", "question": "who is angelina jolie", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is angelina jolie [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.011085833422839642, -0.24998660385608673], "norm_scores": [0.5594427585601807, 0.4405572712421417]}}
{"id": "WebQTest-1248", "question": "where is nineveh city", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is nineveh city [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.003833891125395894, -0.44621074199676514], "norm_scores": [0.6088252663612366, 0.3911747634410858]}}
{"id": "WebQTest-1249", "question": "what is the state flower of new mexico", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "ground_paths": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the state flower of new mexico [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>"], "scores": [-0.02728908322751522, -0.03537578135728836], "norm_scores": [0.5020216703414917, 0.4979783296585083]}}
{"id": "WebQTest-1250", "question": "where is the national ffa foundation located today", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the national ffa foundation located today [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>"], "scores": [-0.02339739352464676, -0.09657767415046692], "norm_scores": [0.518286943435669, 0.48171308636665344]}}
{"id": "WebQTest-1251", "question": "which countries speak germanic languages", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries speak germanic languages [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.053858377039432526, -0.1442139595746994], "norm_scores": [0.5225735306739807, 0.4774264693260193]}}
{"id": "WebQTest-1252", "question": "who plays eowyn in lord of the rings", "prediction": [["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "ground_paths": [["award.award_honor.honored_for", "award.award_winner.awards_won"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_honor.honored_for", "award.award_honor.award_winner"], ["film.film.starring", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays eowyn in lord of the rings [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.07668131589889526, -0.0818430557847023], "norm_scores": [0.5012904405593872, 0.4987095594406128]}}
{"id": "WebQTest-1254", "question": "where is augusta masters 2012", "prediction": [["time.event.locations"], ["time.recurring_event.instances"]], "ground_paths": [["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is augusta masters 2012 [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> time.recurring_event.instances </PATH>"], "scores": [-0.013009751215577126, -0.23187024891376495], "norm_scores": [0.5544977188110352, 0.44550225138664246]}}
{"id": "WebQTest-1255", "question": "what type of cancer did eva peron die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of cancer did eva peron die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.028192810714244843, -0.08603302389383316], "norm_scores": [0.514456033706665, 0.48554399609565735]}}
{"id": "WebQTest-1256", "question": "who did terrell owens play for in 2010", "prediction": [["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"]], "ground_paths": [["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did terrell owens play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>"], "scores": [-0.04637356847524643, -0.05859140679240227], "norm_scores": [0.5030544400215149, 0.4969455897808075]}}
{"id": "WebQTest-1257", "question": "where is denmark south carolina", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"], ["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is denmark south carolina [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0696222260594368, -0.1319456547498703], "norm_scores": [0.5155758261680603, 0.4844241738319397]}}
{"id": "WebQTest-1259", "question": "what books did ernest hemingway write", "prediction": [["book.written_work.author"], ["book.book_edition.author_editor"]], "ground_paths": [["film.film.story_by"], ["film.film_story_contributor.film_story_credits"], ["book.written_work.author"], ["book.book_edition.author_editor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what books did ernest hemingway write [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.08739256113767624, -0.09721873700618744], "norm_scores": [0.5024564862251282, 0.49754345417022705]}}
{"id": "WebQTest-1260", "question": "what year did baltimore ravens win super bowl", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did baltimore ravens win super bowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>"], "scores": [-0.05179166421294212, -0.054086003452539444], "norm_scores": [0.5005735754966736, 0.49942639470100403]}}
{"id": "WebQTest-1261", "question": "where did the battle of passchendaele happen", "prediction": [["time.event.locations"], ["location.location.events"]], "ground_paths": [["location.location.events"], ["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the battle of passchendaele happen [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.07916168868541718, -0.08713655918836594], "norm_scores": [0.501993715763092, 0.49800625443458557]}}
{"id": "WebQTest-1262", "question": "what part of the country is ohio considered", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part of the country is ohio considered [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.06556140631437302, -0.10160820931196213], "norm_scores": [0.5090107321739197, 0.4909892976284027]}}
{"id": "WebQTest-1263", "question": "what team is cristiano ronaldo play for", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"]], "ground_paths": [["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player.transfers", "soccer.football_player_transfer.purchasing_team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is cristiano ronaldo play for [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>"], "scores": [-0.05067306384444237, -0.0614171177148819], "norm_scores": [0.5026859641075134, 0.4973140060901642]}}
{"id": "WebQTest-1264", "question": "what standard time is michigan", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what standard time is michigan [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>"], "scores": [-7.199432729976252e-05, -0.44199061393737793], "norm_scores": [0.6087160706520081, 0.39128386974334717]}}
{"id": "WebQTest-1265", "question": "what was john tyler", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was john tyler [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.018382001668214798, -0.08385668694972992], "norm_scores": [0.5163628458976746, 0.48363715410232544]}}
{"id": "WebQTest-1266", "question": "what did romo do", "prediction": [["american_football.football_player.position_s"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.position"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did romo do [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.position_s </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.position </PATH>"], "scores": [-0.049954235553741455, -0.08654183894395828], "norm_scores": [0.5091458559036255, 0.4908541142940521]}}
{"id": "WebQTest-1267", "question": "what currency can you use in jamaica", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency can you use in jamaica [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.0035139438696205616, -0.28287068009376526], "norm_scores": [0.5693885087966919, 0.4306114912033081]}}
{"id": "WebQTest-1268", "question": "where did jay moriarty die", "prediction": [["people.deceased_person.place_of_death"], ["people.person.place_of_birth"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did jay moriarty die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.0009378643007948995, -0.4604344964027405], "norm_scores": [0.6128947138786316, 0.38710522651672363]}}
{"id": "WebQTest-1269", "question": "where was the boston tea party 1773", "prediction": [["time.event.locations"], ["location.location.events"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the boston tea party 1773 [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.04034925252199173, -0.17116817831993103], "norm_scores": [0.5326582193374634, 0.4673418402671814]}}
{"id": "WebQTest-1270", "question": "what drugs lindsay lohan abuse", "prediction": [["base.popstra.substance_abuse.abuser", "base.popstra.substance_abuse.substance"], ["base.popstra.celebrity.substance_abuse", "base.popstra.substance_abuse.substance"]], "ground_paths": [["base.popstra.rehab_stay.patient", "base.popstra.rehab_stay.substance_abused"], ["base.popstra.substance_abuse.abuser", "base.popstra.substance_abuse.substance"], ["base.popstra.celebrity.substance_abuse", "base.popstra.substance_abuse.substance"], ["celebrities.celebrity.substance_abuse_problems", "celebrities.abused_substance.abused_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what drugs lindsay lohan abuse [/INST]", "raw_output": {"paths": ["<PATH> base.popstra.substance_abuse.abuser <SEP> base.popstra.substance_abuse.substance </PATH>", "<PATH> base.popstra.celebrity.substance_abuse <SEP> base.popstra.substance_abuse.substance </PATH>"], "scores": [-0.017076222226023674, -0.05993419885635376], "norm_scores": [0.5107128620147705, 0.4892871677875519]}}
{"id": "WebQTest-1271", "question": "when did carolina panthers go to superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "ground_paths": [["tv.tv_actor.guest_roles", "tv.tv_guest_role.episodes_appeared_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did carolina panthers go to superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.04107082635164261, -0.06708819419145584], "norm_scores": [0.5065039992332458, 0.49349603056907654]}}
{"id": "WebQTest-1272", "question": "when did the burma cyclone happen", "prediction": [["meteorology.tropical_cyclone.affected_areas"], ["meteorology.cyclone_affected_area.cyclones"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the burma cyclone happen [/INST]", "raw_output": {"paths": ["<PATH> meteorology.tropical_cyclone.affected_areas </PATH>", "<PATH> meteorology.cyclone_affected_area.cyclones </PATH>"], "scores": [-0.026901405304670334, -0.06908273696899414], "norm_scores": [0.5105437636375427, 0.48945626616477966]}}
{"id": "WebQTest-1274", "question": "who won the governor election in texas", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"]], "ground_paths": [["location.location.events", "base.culturalevent.event.entity_involved"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["location.administrative_division.country", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.appointed_by"], ["film.film_location.featured_in_films", "film.film.subjects"], ["location.location.containedby", "symbols.name_source.namesakes"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["base.biblioness.bibs_location.state", "symbols.namesake.named_after"], ["people.deceased_person.place_of_death"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.political_appointer.appointees"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["location.us_state.capital", "people.deceased_person.place_of_death"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won the governor election in texas [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.04933277517557144, -0.049580324441194534], "norm_scores": [0.5000618696212769, 0.49993813037872314]}}
{"id": "WebQTest-1275", "question": "what does helen flanagan do", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does helen flanagan do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.00605008564889431, -0.27992987632751465], "norm_scores": [0.5680451393127441, 0.43195489048957825]}}
{"id": "WebQTest-1276", "question": "when are the summer and winter olympics held", "prediction": [["time.recurring_event.instances"], ["time.event.instance_of_recurring_event"]], "ground_paths": [["time.recurring_event.current_frequency"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when are the summer and winter olympics held [/INST]", "raw_output": {"paths": ["<PATH> time.recurring_event.instances </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>"], "scores": [-0.03719809651374817, -0.06425128877162933], "norm_scores": [0.5067628622055054, 0.49323713779449463]}}
{"id": "WebQTest-1277", "question": "where is tennessee river", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "ground_paths": [["location.location.partially_contains"], ["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is tennessee river [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.019052984192967415, -0.15462155640125275], "norm_scores": [0.5338403582572937, 0.4661597013473511]}}
{"id": "WebQTest-1278", "question": "what are some major events in venezuela", "prediction": [["time.event.locations"], ["location.location.events"]], "ground_paths": [["location.location.events"], ["time.event.locations"], ["base.culturalevent.event.entity_involved"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some major events in venezuela [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.09551379084587097, -0.15776246786117554], "norm_scores": [0.5155571103096008, 0.4844428598880768]}}
{"id": "WebQTest-1279", "question": "what guitar does corey taylor play", "prediction": [["music.group_member.instruments_played"], ["music.instrument.family"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what guitar does corey taylor play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.family </PATH>"], "scores": [-0.029740724712610245, -0.18171326816082], "norm_scores": [0.5379201769828796, 0.46207982301712036]}}
{"id": "WebQTest-1280", "question": "who was president after franklin d. roosevelt", "prediction": [["government.us_president.vice_president"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was president after franklin d. roosevelt [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.07597292214632034, -0.08295142650604248], "norm_scores": [0.5017446279525757, 0.49825534224510193]}}
{"id": "WebQTest-1281", "question": "where did kate gosselin grew up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kate gosselin grew up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.009782559238374233, -0.1819661408662796], "norm_scores": [0.5429399013519287, 0.45706015825271606]}}
{"id": "WebQTest-1282", "question": "under what political system does the united states government operate", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: under what political system does the united states government operate [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.017093904316425323, -0.09886423498392105], "norm_scores": [0.5204312205314636, 0.47956880927085876]}}
{"id": "WebQTest-1284", "question": "what to do in paris in 1 day", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["location.location.containedby"], ["base.arthist.helynevek.intezmeny"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do in paris in 1 day [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.02923198603093624, -0.06716807186603546], "norm_scores": [0.5094828605651855, 0.49051713943481445]}}
{"id": "WebQTest-1285", "question": "what city is mt lassen in", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city is mt lassen in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.028031662106513977, -0.18538884818553925], "norm_scores": [0.5392583608627319, 0.46074166893959045]}}
{"id": "WebQTest-1287", "question": "what is the state motto of ohio", "prediction": [["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.location_symbol_relationship.symbol"]], "ground_paths": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the state motto of ohio [/INST]", "raw_output": {"paths": ["<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.046019233763217926, -0.05479865148663521], "norm_scores": [0.5021948218345642, 0.4978051483631134]}}
{"id": "WebQTest-1289", "question": "what type of music did mozart composed", "prediction": [["music.artist.genre"], ["music.genre.artists"]], "ground_paths": [["music.genre.artists"], ["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did mozart composed [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>"], "scores": [-0.029176846146583557, -0.16243988275527954], "norm_scores": [0.533266544342041, 0.466733455657959]}}
{"id": "WebQTest-1291", "question": "how many kids were there in the kennedy family", "prediction": [["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how many kids were there in the kennedy family [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.058185987174510956, -0.11623889207839966], "norm_scores": [0.5145091414451599, 0.4854908585548401]}}
{"id": "WebQTest-1292", "question": "which barcelona airport is closest to the port", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"]], "ground_paths": [["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which barcelona airport is closest to the port [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>"], "scores": [-0.029236339032649994, -0.14781799912452698], "norm_scores": [0.5296106934547424, 0.4703892767429352]}}
{"id": "WebQTest-1293", "question": "where has the english language evolved from", "prediction": [["language.human_language.language_family"], ["base.rosetta.languoid.parent"]], "ground_paths": [["language.human_language.language_family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where has the english language evolved from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>"], "scores": [-0.027692722156643867, -0.10526305437088013], "norm_scores": [0.519382894039154, 0.48061713576316833]}}
{"id": "WebQTest-1294", "question": "what country did hitler invade that started ww2", "prediction": [["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did hitler invade that started ww2 [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.07659024745225906, -0.10688097774982452], "norm_scores": [0.5075721144676208, 0.49242791533470154]}}
{"id": "WebQTest-1295", "question": "what does matt damon play in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"]], "ground_paths": [["people.person.languages", "film.film.language"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["film.actor.film", "film.film.starring"], ["film.producer.film"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["common.topic.notable_types", "common.topic.notable_types"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["tv.tv_program.program_creator"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["film.film.written_by"], ["film.person_or_entity_appearing_in_film.films", "film.film.personal_appearances"], ["music.recording.featured_artists", "music.recording.releases"], ["people.person.languages", "tv.tv_program.languages"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does matt damon play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>"], "scores": [-0.08097606152296066, -0.08454879373311996], "norm_scores": [0.5008931756019592, 0.49910682439804077]}}
{"id": "WebQTest-1296", "question": "what was scottie pippen known for", "prediction": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.player"], ["basketball.basketball_player.position_s"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was scottie pippen known for [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.player </PATH>", "<PATH> basketball.basketball_player.position_s </PATH>"], "scores": [-0.03995395079255104, -0.10944746434688568], "norm_scores": [0.5173664093017578, 0.4826336205005646]}}
{"id": "WebQTest-1297", "question": "what continent does canada belong to", "prediction": [["base.locations.countries.continent"], ["location.location.containedby"]], "ground_paths": [["location.location.contains"], ["base.locations.continents.countries_within"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what continent does canada belong to [/INST]", "raw_output": {"paths": ["<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.06946877390146255, -0.09948821365833282], "norm_scores": [0.5075043439865112, 0.49249571561813354]}}
{"id": "WebQTest-1298", "question": "what does george wilson do for a living", "prediction": [["people.person.profession"], ["fictional_universe.fictional_character.occupation"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does george wilson do for a living [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>"], "scores": [-0.028388241305947304, -0.11396843940019608], "norm_scores": [0.5213820338249207, 0.4786180257797241]}}
{"id": "WebQTest-1300", "question": "what city does fulham play in", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.location"]], "ground_paths": [["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city does fulham play in [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>"], "scores": [-0.08754635602235794, -0.10500361770391464], "norm_scores": [0.5043642520904541, 0.4956358075141907]}}
{"id": "WebQTest-1301", "question": "what are fun things to do in sacramento ca", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are fun things to do in sacramento ca [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.05151652544736862, -0.054563652724027634], "norm_scores": [0.5007617473602295, 0.4992382228374481]}}
{"id": "WebQTest-1303", "question": "what other movies is josh hutcherson in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["film.actor.film", "film.film.starring"], ["film.producer.films_executive_produced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other movies is josh hutcherson in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.06699270755052567, -0.13963523507118225], "norm_scores": [0.51815265417099, 0.48184734582901]}}
{"id": "WebQTest-1304", "question": "what was hitler the leader of", "prediction": [["organization.organization_founder.organizations_founded"], ["base.activism.activist.area_of_activism", "government.governmental_jurisdiction.government_positions"]], "ground_paths": [["symbols.name_source.namesakes"], ["organization.organization.founders"], ["organization.organization_founder.organizations_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was hitler the leader of [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> government.governmental_jurisdiction.government_positions </PATH>"], "scores": [-0.06045644357800484, -0.09340168535709381], "norm_scores": [0.5082355737686157, 0.49176445603370667]}}
{"id": "WebQTest-1305", "question": "what awards did marilyn monroe won", "prediction": [["award.award_nomination.award_nominee", "award.award_nomination.award"], ["award.award_honor.award_winner", "award.award_honor.award"]], "ground_paths": [["award.award_winner.awards_won", "award.award_honor.award"], ["award.award_honor.award_winner", "award.award_category.winners"], ["people.person.profession", "award.award_discipline.awards_in_this_discipline"], ["award.award_nominee.award_nominations", "award.award_nomination.award"], ["award.award_nomination.award_nominee", "award.award_category.nominees"], ["award.award_nominee.award_nominations", "award.award_category.nominees"], ["award.award_honor.award_winner", "award.award_honor.award"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what awards did marilyn monroe won [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_honor.award </PATH>"], "scores": [-0.04792151227593422, -0.06497816741466522], "norm_scores": [0.5042640566825867, 0.49573594331741333]}}
{"id": "WebQTest-1306", "question": "what kind of government does iran have after 1979", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does iran have after 1979 [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.01617470011115074, -0.10223813354969025], "norm_scores": [0.5215026140213013, 0.4784974455833435]}}
{"id": "WebQTest-1307", "question": "where did harper lee attend high school", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did harper lee attend high school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.039928097277879715, -0.0618995800614357], "norm_scores": [0.505492627620697, 0.4945073127746582]}}
{"id": "WebQTest-1308", "question": "what college did jrr tolkien go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.employment_history", "business.employment_tenure.company"], ["education.education.student", "education.education.institution"], ["business.employment_tenure.person", "business.employment_tenure.company"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did jrr tolkien go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.049066390842199326, -0.07875869423151016], "norm_scores": [0.5074225664138794, 0.492577463388443]}}
{"id": "WebQTest-1309", "question": "what art did wassily kandinsky do", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what art did wassily kandinsky do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.03180466592311859, -0.09401162713766098], "norm_scores": [0.5155467391014099, 0.4844532608985901]}}
{"id": "WebQTest-1310", "question": "who won the league cup in 2002", "prediction": [["sports.sports_award_type.winners", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"]], "ground_paths": [["time.event.instance_of_recurring_event", "sports.sports_championship_event.champion"], ["sports.sports_award.award", "sports.sports_award_winner.awards"], ["sports.sports_championship_event.championship", "sports.sports_team.championships"], ["sports.sports_championship.events", "sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won the league cup in 2002 [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_type.winners <SEP> sports.sports_award.award_winner </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.053075939416885376, -0.05364714562892914], "norm_scores": [0.5001428127288818, 0.49985721707344055]}}
{"id": "WebQTest-1311", "question": "what was antoni gaudi inspired by", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was antoni gaudi inspired by [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.03518565744161606, -0.05029410496354103], "norm_scores": [0.503777027130127, 0.49622294306755066]}}
{"id": "WebQTest-1312", "question": "in what country do people speak danish", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: in what country do people speak danish [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.0375671423971653, -0.08667858690023422], "norm_scores": [0.5122753977775574, 0.4877246022224426]}}
{"id": "WebQTest-1313", "question": "who were demeter 's siblings", "prediction": [["fictional_universe.fictional_character.siblings", "fictional_universe.fictional_character.siblings"], ["fictional_universe.fictional_character.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"]], "ground_paths": [["fictional_universe.fictional_character.children", "fictional_universe.fictional_character.parents"], ["fictional_universe.romantic_involvement.partner", "fictional_universe.fictional_character.romantically_involved_with"], ["fictional_universe.fictional_character.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.parents"], ["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character.siblings", "fictional_universe.fictional_character.siblings"], ["common.topic.notable_types", "fictional_universe.fictional_character.species"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were demeter 's siblings [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.siblings <SEP> fictional_universe.fictional_character.siblings </PATH>", "<PATH> fictional_universe.fictional_character.siblings <SEP> fictional_universe.sibling_relationship_of_fictional_characters.siblings </PATH>"], "scores": [-0.04607122018933296, -0.04892848804593086], "norm_scores": [0.5007143020629883, 0.4992856979370117]}}
{"id": "WebQTest-1314", "question": "where should a family stay in paris", "prediction": [["travel.travel_destination.accommodation"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["travel.travel_destination.accommodation"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where should a family stay in paris [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.accommodation </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.013982241041958332, -0.17071008682250977], "norm_scores": [0.5391019582748413, 0.4608980417251587]}}
{"id": "WebQTest-1316", "question": "what was the name of benjamin franklin wife", "prediction": [["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.place_of_interment.interred_here", "people.place_of_interment.interred_here"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the name of benjamin franklin wife [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.10854198783636093, -0.14196595549583435], "norm_scores": [0.5083552002906799, 0.4916447699069977]}}
{"id": "WebQTest-1317", "question": "what the largest city in spain", "prediction": [["location.country.capital"], ["location.location.containedby"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the largest city in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.1150377169251442, -0.1367356777191162], "norm_scores": [0.5054242610931396, 0.49457573890686035]}}
{"id": "WebQTest-1319", "question": "who has pudge rodriguez played for", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"]], "ground_paths": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has pudge rodriguez played for [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>"], "scores": [-0.031988147646188736, -0.0495491586625576], "norm_scores": [0.5043901205062866, 0.495609849691391]}}
{"id": "WebQTest-1320", "question": "what is the milwaukee brewers stadium called", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"]], "ground_paths": [["sports.sports_facility.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the milwaukee brewers stadium called [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>"], "scores": [-0.011360695585608482, -0.13142657279968262], "norm_scores": [0.529980480670929, 0.47001951932907104]}}
{"id": "WebQTest-1321", "question": "where did the gallipoli war take place", "prediction": [["time.event.locations"], ["location.location.events"]], "ground_paths": [["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the gallipoli war take place [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.04586652293801308, -0.13948433101177216], "norm_scores": [0.5233873724937439, 0.4766126573085785]}}
{"id": "WebQTest-1322", "question": "what other movies has tom hardy played in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["film.actor.film", "film.film.starring"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other movies has tom hardy played in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.08508622646331787, -0.13169685006141663], "norm_scores": [0.511650562286377, 0.48834946751594543]}}
{"id": "WebQTest-1323", "question": "what to visit in atlanta", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "ground_paths": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to visit in atlanta [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>"], "scores": [-0.048459041863679886, -0.06069786101579666], "norm_scores": [0.5030596852302551, 0.49694034457206726]}}
{"id": "WebQTest-1324", "question": "what college did john nash go to", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.employment_history", "business.employment_tenure.company"], ["book.written_work.author", "education.dissertation.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did john nash go to [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.04985218867659569, -0.05913938581943512], "norm_scores": [0.5023217797279358, 0.4976782202720642]}}
{"id": "WebQTest-1325", "question": "what language do maltese speak", "prediction": [["location.country.official_language"], ["language.human_language.main_country"]], "ground_paths": [["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do maltese speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.0737362802028656, -0.14582574367523193], "norm_scores": [0.5180146098136902, 0.4819854497909546]}}
{"id": "WebQTest-1326", "question": "what did thomas edison do for the world", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"]], "ground_paths": [["law.inventor.us_patents"], ["law.invention.inventor"], ["law.inventor.inventions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did thomas edison do for the world [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>"], "scores": [-0.06364130228757858, -0.10218210518360138], "norm_scores": [0.5096339583396912, 0.49036598205566406]}}
{"id": "WebQTest-1327", "question": "where is shakira from", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is shakira from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.05835037678480148, -0.07493261247873306], "norm_scores": [0.5041454434394836, 0.49585452675819397]}}
{"id": "WebQTest-1328", "question": "what did hayes do", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did hayes do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.032746993005275726, -0.21699021756649017], "norm_scores": [0.5459309816360474, 0.454069048166275]}}
{"id": "WebQTest-1329", "question": "who played bilbo in the fellowship of the ring", "prediction": [["film.performance.character", "film.actor.film"], ["film.performance.film", "film.actor.film"]], "ground_paths": [["film.film.starring", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played bilbo in the fellowship of the ring [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.film <SEP> film.actor.film </PATH>"], "scores": [-0.13989803194999695, -0.1751595288515091], "norm_scores": [0.5088144540786743, 0.4911855161190033]}}
{"id": "WebQTest-1330", "question": "what is the most populated state in united states", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.administrative_division.first_level_division_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the most populated state in united states [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.046623073518276215, -0.08112858980894089], "norm_scores": [0.5086255073547363, 0.49137449264526367]}}
{"id": "WebQTest-1331", "question": "who is my state senator arizona", "prediction": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "ground_paths": [["location.country.first_level_divisions", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.capital", "people.person.place_of_birth"], ["base.aareas.schema.administrative_area.capital", "people.deceased_person.place_of_death"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["location.location.containedby", "people.person.place_of_birth"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.political_district.representatives", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is my state senator arizona [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.04792691394686699, -0.06114862486720085], "norm_scores": [0.5033053755760193, 0.4966946542263031]}}
{"id": "WebQTest-1332", "question": "when does the vietnam war end", "prediction": [["time.event.included_in_event"], ["time.event.includes_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when does the vietnam war end [/INST]", "raw_output": {"paths": ["<PATH> time.event.included_in_event </PATH>", "<PATH> time.event.includes_event </PATH>"], "scores": [-0.06404633074998856, -0.16608092188835144], "norm_scores": [0.5254865884780884, 0.4745134711265564]}}
{"id": "WebQTest-1333", "question": "where does the tennessee river go", "prediction": [["geography.river.mouth"], ["location.location.partially_containedby"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the tennessee river go [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.06909637153148651, -0.13970592617988586], "norm_scores": [0.5176450610160828, 0.4823549687862396]}}
{"id": "WebQTest-1334", "question": "who did tom hanks play in apollo 13", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.film", "film.performance.character"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["people.person.profession", "people.person.profession"], ["spaceflight.astronaut.missions"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did tom hanks play in apollo 13 [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>"], "scores": [-0.07119694352149963, -0.11158762127161026], "norm_scores": [0.5100963115692139, 0.4899037480354309]}}
{"id": "WebQTest-1335", "question": "what language do they in ghana", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they in ghana [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.01948179304599762, -0.15484027564525604], "norm_scores": [0.5337880253791809, 0.4662119448184967]}}
{"id": "WebQTest-1336", "question": "what is utah 's state capitol", "prediction": [["base.aareas.schema.administrative_area.capital"], ["location.location.containedby"]], "ground_paths": [["base.biblioness.bibs_location.city"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is utah 's state capitol [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0653328225016594, -0.13314557075500488], "norm_scores": [0.5169467329978943, 0.4830533266067505]}}
{"id": "WebQTest-1337", "question": "what all did ben franklin invent", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"]], "ground_paths": [["law.inventor.inventions"], ["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what all did ben franklin invent [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>"], "scores": [-0.029275963082909584, -0.12394711375236511], "norm_scores": [0.5236501097679138, 0.4763498604297638]}}
{"id": "WebQTest-1339", "question": "what team does jeremy lin play for 2013", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team.roster"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does jeremy lin play for 2013 [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.0354502908885479, -0.0453474186360836], "norm_scores": [0.5024742484092712, 0.49752575159072876]}}
{"id": "WebQTest-1340", "question": "what is steve nash nationality", "prediction": [["people.person.nationality"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is steve nash nationality [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.011245420202612877, -0.15503929555416107], "norm_scores": [0.5358866453170776, 0.46411335468292236]}}
{"id": "WebQTest-1341", "question": "who are the u s senators of pennsylvania", "prediction": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.place_of_birth"], ["base.aareas.schema.administrative_area.administrative_children", "people.deceased_person.place_of_burial"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.administrative_children", "people.deceased_person.place_of_death"], ["base.aareas.schema.administrative_area.administrative_parent", "people.deceased_person.place_of_death"], ["base.aareas.schema.administrative_area.administrative_children", "people.person.place_of_birth"], ["location.location.containedby", "people.deceased_person.place_of_burial"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["location.location.containedby", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["location.location.containedby", "people.person.place_of_birth"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["government.political_district.representatives", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the u s senators of pennsylvania [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.05092036724090576, -0.060533713549375534], "norm_scores": [0.5024033188819885, 0.4975966513156891]}}
{"id": "WebQTest-1342", "question": "what kind of language does france speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language does france speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.063536636531353, -0.07898256182670593], "norm_scores": [0.5038613677024841, 0.4961385726928711]}}
{"id": "WebQTest-1343", "question": "what degree did martin luther king earn", "prediction": [["education.education.student", "education.education.degree"], ["people.person.education", "education.education.degree"]], "ground_paths": [["people.person.education", "education.education.degree"], ["education.education.student", "education.education.degree"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what degree did martin luther king earn [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> people.person.education <SEP> education.education.degree </PATH>"], "scores": [-0.014240030199289322, -0.07162890583276749], "norm_scores": [0.51434326171875, 0.48565673828125]}}
{"id": "WebQTest-1344", "question": "what two states is st louis in", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what two states is st louis in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.019143227487802505, -0.3659958243370056], "norm_scores": [0.5858541131019592, 0.4141458570957184]}}
{"id": "WebQTest-1348", "question": "what team did peyton manning 's dad play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did peyton manning 's dad play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.023101938888430595, -0.04610131308436394], "norm_scores": [0.5057495832443237, 0.49425041675567627]}}
{"id": "WebQTest-1350", "question": "what did jesse owens won", "prediction": [["olympics.olympic_athlete.medals_won", "olympics.olympic_medal_honor.event"], ["olympics.olympic_medal_honor.medalist", "olympics.olympic_medal_honor.event"]], "ground_paths": [["award.award_honor.award_winner", "award.award_category.winners"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did jesse owens won [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_medal_honor.event </PATH>", "<PATH> olympics.olympic_medal_honor.medalist <SEP> olympics.olympic_medal_honor.event </PATH>"], "scores": [-0.04035305604338646, -0.042959123849868774], "norm_scores": [0.500651478767395, 0.4993484914302826]}}
{"id": "WebQTest-1351", "question": "what is the name of the currency used in peru", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the currency used in peru [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.05060495808720589, -0.07744448632001877], "norm_scores": [0.5067095160484314, 0.493290513753891]}}
{"id": "WebQTest-1353", "question": "what are the four nations of the united kingdom", "prediction": [["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the four nations of the united kingdom [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.03343142569065094, -0.08563783019781113], "norm_scores": [0.5130486488342285, 0.4869513511657715]}}
{"id": "WebQTest-1354", "question": "who is judy garland father", "prediction": [["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is judy garland father [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.041227467358112335, -0.14795087277889252], "norm_scores": [0.5266555547714233, 0.47334444522857666]}}
{"id": "WebQTest-1355", "question": "what school did robert f kennedy go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did robert f kennedy go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.05317782238125801, -0.06758764386177063], "norm_scores": [0.5036023855209351, 0.49639761447906494]}}
{"id": "WebQTest-1356", "question": "what drug killed marilyn monroe", "prediction": [["people.cause_of_death.people"], ["people.deceased_person.cause_of_death"]], "ground_paths": [["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what drug killed marilyn monroe [/INST]", "raw_output": {"paths": ["<PATH> people.cause_of_death.people </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.0280295517295599, -0.07898902893066406], "norm_scores": [0.5127370953559875, 0.48726290464401245]}}
{"id": "WebQTest-1357", "question": "where is the fukushima daiichi nuclear power station", "prediction": [["location.location.containedby"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the fukushima daiichi nuclear power station [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.028661280870437622, -0.14834825694561005], "norm_scores": [0.5298860669136047, 0.47011393308639526]}}
{"id": "WebQTest-1358", "question": "what is los angeles california time zone", "prediction": [["location.location.time_zones"], ["location.location.containedby"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is los angeles california time zone [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-5.6771168601699173e-05, -0.8815776109695435], "norm_scores": [0.7071372270584106, 0.2928627133369446]}}
{"id": "WebQTest-1359", "question": "what did nicolas cage name his son", "prediction": [["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did nicolas cage name his son [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.06824395805597305, -0.10020671039819717], "norm_scores": [0.5079900026321411, 0.4920099973678589]}}
{"id": "WebQTest-1361", "question": "what currency should take to mexico", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should take to mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.03996976464986801, -0.14284458756446838], "norm_scores": [0.5256960391998291, 0.4743039608001709]}}
{"id": "WebQTest-1362", "question": "who is the state governor of florida", "prediction": [["location.administrative_division.first_level_division_of", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"], ["location.administrative_division.first_level_division_of", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the state governor of florida [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.first_level_division_of <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.03633122891187668, -0.0462413914501667], "norm_scores": [0.5024775266647339, 0.4975224733352661]}}
{"id": "WebQTest-1363", "question": "who was jessica simpson married too", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["people.person.parents", "people.person.children"], ["people.person.spouse_s", "people.marriage.spouse"], ["base.saturdaynightlive.snl_episode.host", "base.saturdaynightlive.snl_host.episodes_hosted"], ["music.artist.album", "music.album.featured_artists"], ["music.artist.genre", "music.artist.genre"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.profession", "people.person.profession"], ["people.person.languages", "people.person.languages"], ["common.topic.notable_types", "common.topic.notable_types"], ["broadcast.artist.content", "broadcast.artist.content"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.celebrity.infidelity_participant", "base.popstra.celebrity.infidelity_victim"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was jessica simpson married too [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.05626871809363365, -0.11993447691202164], "norm_scores": [0.5159110426902771, 0.4840889573097229]}}
{"id": "WebQTest-1364", "question": "what do they speak in scotland", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do they speak in scotland [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.05644066631793976, -0.0666051134467125], "norm_scores": [0.5025411248207092, 0.49745893478393555]}}
{"id": "WebQTest-1365", "question": "what did john dalton study", "prediction": [["people.person.education", "education.education.major_field_of_study"], ["people.person.profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did john dalton study [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.06068417802453041, -0.07082317024469376], "norm_scores": [0.5025346875190735, 0.4974652826786041]}}
{"id": "WebQTest-1366", "question": "what songs do zac brown band sing", "prediction": [["music.recording.artist"], ["music.artist.album"]], "ground_paths": [["music.recording.artist"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["music.album.artist"], ["music.recording.featured_artists"], ["music.artist.album"], ["music.featured_artist.recordings"], ["music.music_video.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what songs do zac brown band sing [/INST]", "raw_output": {"paths": ["<PATH> music.recording.artist </PATH>", "<PATH> music.artist.album </PATH>"], "scores": [-0.0737108439207077, -0.16890259087085724], "norm_scores": [0.5237799882888794, 0.476220041513443]}}
{"id": "WebQTest-1367", "question": "where is glastonbury england", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is glastonbury england [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0589725598692894, -0.10692276060581207], "norm_scores": [0.5119852423667908, 0.48801475763320923]}}
{"id": "WebQTest-1368", "question": "who formed the american federation of labor", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who formed the american federation of labor [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.007664513308554888, -0.17534108459949493], "norm_scores": [0.5418211817741394, 0.4581787884235382]}}
{"id": "WebQTest-1369", "question": "what to visit in denver", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to visit in denver [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.04216059297323227, -0.12312036752700806], "norm_scores": [0.520228922367096, 0.47977110743522644]}}
{"id": "WebQTest-1370", "question": "where did thomas hobbes live", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did thomas hobbes live [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.08670499920845032, -0.1348293423652649], "norm_scores": [0.5120287537574768, 0.4879712164402008]}}
{"id": "WebQTest-1373", "question": "where was john f kennedy when he got shot", "prediction": [["people.deceased_person.place_of_death"], ["people.person.place_of_birth"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was john f kennedy when he got shot [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.015279722400009632, -0.23749586939811707], "norm_scores": [0.5553265810012817, 0.44467344880104065]}}
{"id": "WebQTest-1374", "question": "what type of government does italy have 2011", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does italy have 2011 [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.03357504680752754, -0.05974876508116722], "norm_scores": [0.5065430402755737, 0.4934569299221039]}}
{"id": "WebQTest-1376", "question": "what province is montreal canada located", "prediction": [["base.biblioness.bibs_location.state"], ["location.province.capital"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what province is montreal canada located [/INST]", "raw_output": {"paths": ["<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> location.province.capital </PATH>"], "scores": [-0.11010654270648956, -0.126540407538414], "norm_scores": [0.5041083693504333, 0.49589163064956665]}}
{"id": "WebQTest-1378", "question": "who does kellan lutz play in prom night", "prediction": [["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.film_character.portrayed_in_films"]], "ground_paths": [["film.actor.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does kellan lutz play in prom night [/INST]", "raw_output": {"paths": ["<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.06929351389408112, -0.07332703471183777], "norm_scores": [0.5010083913803101, 0.49899163842201233]}}
{"id": "WebQTest-1379", "question": "what did franz liszt do to earn a living", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did franz liszt do to earn a living [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.006261749658733606, -0.31231316924095154], "norm_scores": [0.5759211778640747, 0.4240788221359253]}}
{"id": "WebQTest-1380", "question": "what was nikola tesla known for", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was nikola tesla known for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.013459624722599983, -0.24886323511600494], "norm_scores": [0.5585806369781494, 0.4414193630218506]}}
{"id": "WebQTest-1381", "question": "what is the zip code for princeton nj", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"]], "ground_paths": [["location.location.contains"], ["location.citytown.postal_codes"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for princeton nj [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0014997640391811728, -0.42214441299438477], "norm_scores": [0.6036375164985657, 0.3963625133037567]}}
{"id": "WebQTest-1382", "question": "what other countries border argentina", "prediction": [["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.partially_containedby", "location.location.partially_contains"]], "ground_paths": [["time.event.locations", "time.event.locations"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["location.location.containedby", "location.location.containedby"], ["time.event.locations", "location.location.events"], ["biology.animal_breed.place_of_origin", "biology.animal_breed.place_of_origin"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["location.country.form_of_government", "location.country.form_of_government"], ["location.location.contains", "location.location.containedby"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["base.uncommon.topic.exceptions", "base.uncommon.topic.exceptions"], ["location.location.partially_containedby", "location.location.partially_contains"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["film.film.featured_film_locations", "film.film.country"], ["location.location.primarily_containedby", "location.location.containedby"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["book.book_subject.works", "book.book_subject.works"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["location.location.events", "time.event.locations"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.country.languages_spoken", "location.country.official_language"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["government.form_of_government.countries", "location.country.form_of_government"], ["location.location.contains", "location.location.contains"], ["base.locations.continents.counties_within", "base.locations.counties_parishes_and_boroughs.coninent"], ["location.location.events", "location.location.events"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["sports.multi_event_tournament.participating_countries", "sports.multi_event_tournament.participating_countries"], ["location.location.partially_contains", "location.location.partially_contains"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["geography.river.basin_countries", "location.location.partially_contains"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["organization.organization_founder.organizations_founded", "organization.organization_founder.organizations_founded"], ["location.location.partially_containedby", "location.location.containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["language.human_language.main_country", "language.language_family.geographic_distribution"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.location.containedby", "location.location.contains_major_portion_of"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["location.country.form_of_government", "government.form_of_government.countries"], ["film.film.featured_film_locations", "media_common.netflix_title.netflix_genres"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["organization.organization.founders", "organization.organization.founders"], ["location.location.partially_contains", "location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other countries border argentina [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>"], "scores": [-0.10601735860109329, -0.11920664459466934], "norm_scores": [0.5032972693443298, 0.49670273065567017]}}
{"id": "WebQTest-1383", "question": "what language does people in iceland speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does people in iceland speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.05840938538312912, -0.07748428732156754], "norm_scores": [0.5047686100006104, 0.49523138999938965]}}
{"id": "WebQTest-1384", "question": "what is the predominant religion in israel", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "ground_paths": [["people.person.quotations", "people.person.religion"], ["film.film_location.featured_in_films", "media_common.netflix_title.netflix_genres"], ["people.person.nationality", "people.person.religion"], ["location.country.administrative_divisions", "religion.religion.sacred_sites"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["symbols.namesake.named_after", "religion.religion.notable_figures"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the predominant religion in israel [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>"], "scores": [-0.01386244036257267, -0.07376277446746826], "norm_scores": [0.514970600605011, 0.485029399394989]}}
{"id": "WebQTest-1385", "question": "who has coached the carolina panthers", "prediction": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["sports.sports_team.sport", "sports.sports_team_coach.sports_coached"]], "ground_paths": [["american_football.football_team.historical_coaching_staff", "american_football.football_coach.coaching_history"], ["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["sports.sports_team.sport", "sports.sports_team_coach.sports_coached"], ["sports.sports_team.coaches", "sports.sports_team_coach.teams_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has coached the carolina panthers [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sports_team_coach.sports_coached </PATH>"], "scores": [-0.014556880109012127, -0.08497566729784012], "norm_scores": [0.5175974369049072, 0.4824025630950928]}}
{"id": "WebQTest-1387", "question": "what time zone is toronto gmt", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is toronto gmt [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>"], "scores": [-0.0010400194441899657, -0.241705060005188], "norm_scores": [0.5598775148391724, 0.44012248516082764]}}
{"id": "WebQTest-1388", "question": "what country did toussaint l'ouverture help free", "prediction": [["base.culturalevent.event.entity_involved"], ["people.person.nationality"]], "ground_paths": [["military.military_service.military_person", "military.armed_force.personnel"], ["military.military_service.military_person", "military.military_service.military_force"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did toussaint l'ouverture help free [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.09661310911178589, -0.1008637547492981], "norm_scores": [0.5010626316070557, 0.49893733859062195]}}
{"id": "WebQTest-1389", "question": "what jobs did robert burns have", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what jobs did robert burns have [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.013636194169521332, -0.30852529406547546], "norm_scores": [0.5731926560401917, 0.42680737376213074]}}
{"id": "WebQTest-1390", "question": "who won utah attorney general", "prediction": [["base.aareas.schema.administrative_area.administrative_children", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.city", "people.person.place_of_birth"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won utah attorney general [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_children <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.03735974058508873, -0.057798776775598526], "norm_scores": [0.5051096081733704, 0.494890421628952]}}
{"id": "WebQTest-1392", "question": "what the language spoken in indonesia", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the language spoken in indonesia [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.06713324040174484, -0.08580256253480911], "norm_scores": [0.5046672224998474, 0.495332807302475]}}
{"id": "WebQTest-1394", "question": "who coaches the carolina panthers", "prediction": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["american_football.football_historical_coach_position.team", "american_football.football_coach.coaching_history"]], "ground_paths": [["american_football.football_coach.current_team_head_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who coaches the carolina panthers [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_coach.coaching_history </PATH>"], "scores": [-0.013622409664094448, -0.0994236171245575], "norm_scores": [0.5214371681213379, 0.4785628616809845]}}
{"id": "WebQTest-1395", "question": "what other countries does south africa share borders with", "prediction": [["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.partially_contains", "location.location.partially_contains"]], "ground_paths": [["sports.sport_country.multi_event_tournaments_participated_in", "sports.sport_country.multi_event_tournaments_participated_in"], ["time.event.locations", "time.event.locations"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["location.location.containedby", "location.location.containedby"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["time.event.locations", "location.location.events"], ["location.location.containedby", "base.locations.countries.continent"], ["location.country.form_of_government", "location.country.form_of_government"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.containedby", "base.locations.continents.countries_within"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["location.country.languages_spoken", "language.human_language.main_country"], ["location.location.partially_containedby", "location.location.partially_contains"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["location.country.official_language", "language.human_language.main_country"], ["film.film.featured_film_locations", "film.film.subjects"], ["base.uncommon.topic.exceptions", "base.uncommon.topic.exceptions"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.statistical_region.places_imported_from", "location.statistical_region.places_exported_to"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.country.official_language", "location.country.official_language"], ["government.form_of_government.countries", "location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.official_language"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.multi_event_tournament.participating_countries"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["location.imports_and_exports.exported_to", "location.imports_and_exports.imported_from"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["film.film.featured_film_locations", "film.film_location.featured_in_films"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["location.country.official_language", "language.human_language.countries_spoken_in"], ["location.country.currency_used", "location.country.currency_used"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["organization.organization.founders", "organization.organization.founders"], ["location.location.partially_contains", "location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other countries does south africa share borders with [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>"], "scores": [-0.13959015905857086, -0.14042949676513672], "norm_scores": [0.5002098083496094, 0.4997901916503906]}}
{"id": "WebQTest-1396", "question": "who played darth vader at the end of return of the jedi", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played darth vader at the end of return of the jedi [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.08305671066045761, -0.09144828468561172], "norm_scores": [0.5020979046821594, 0.49790212512016296]}}
{"id": "WebQTest-1398", "question": "what movies did joan crawford play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.character"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.person_or_entity_appearing_in_film.films", "film.personal_film_appearance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["common.topic.notable_types", "common.topic.notable_types"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.film.starring"], ["film.producer.film"], ["people.deceased_person.place_of_death", "film.film.featured_film_locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did joan crawford play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.08956211805343628, -0.13069480657577515], "norm_scores": [0.5102817416191101, 0.48971831798553467]}}
{"id": "WebQTest-1399", "question": "what products does the united states export", "prediction": [["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.statistical_region.major_exports", "location.imports_exports_by_industry.industry"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what products does the united states export [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> location.statistical_region.major_exports <SEP> location.imports_exports_by_industry.industry </PATH>"], "scores": [-0.04998547583818436, -0.05699886381626129], "norm_scores": [0.5017533302307129, 0.4982466399669647]}}
{"id": "WebQTest-1402", "question": "what is the kentucky state bird", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"]], "ground_paths": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the kentucky state bird [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.02816559560596943, -0.06703393906354904], "norm_scores": [0.5097158551216125, 0.49028414487838745]}}
{"id": "WebQTest-1403", "question": "who was canadian prime minister in 1993", "prediction": [["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"]], "ground_paths": [["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["base.firsts.first_achievement.achievement", "base.firsts.first_achievement.first"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was canadian prime minister in 1993 [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.019085125997662544, -0.03691975027322769], "norm_scores": [0.5044585466384888, 0.4955414831638336]}}
{"id": "WebQTest-1405", "question": "what there to do in salt lake city utah", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what there to do in salt lake city utah [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>"], "scores": [-0.04789276048541069, -0.05055040493607521], "norm_scores": [0.5006644129753113, 0.49933555722236633]}}
{"id": "WebQTest-1408", "question": "who is the current ohio state senator", "prediction": [["government.political_district.representatives", "government.government_position_held.office_holder"], ["government.government_position_held.governmental_body", "government.government_position_held.office_holder"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the current ohio state senator [/INST]", "raw_output": {"paths": ["<PATH> government.political_district.representatives <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.05460174009203911, -0.05727357789874077], "norm_scores": [0.500667929649353, 0.4993320405483246]}}
{"id": "WebQTest-1409", "question": "what are the major languages spoken in the united kingdom", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the major languages spoken in the united kingdom [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.0798768475651741, -0.08607194572687149], "norm_scores": [0.5015487670898438, 0.49845123291015625]}}
{"id": "WebQTest-1410", "question": "what did st. nicholas do in his life", "prediction": [["people.person.profession"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did st. nicholas do in his life [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.10258063673973083, -0.1709965020418167], "norm_scores": [0.5170972943305969, 0.48290273547172546]}}
{"id": "WebQTest-1414", "question": "what happened to pope john paul ii", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what happened to pope john paul ii [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.0022037294693291187, -0.27171123027801514], "norm_scores": [0.566972017288208, 0.4330280125141144]}}
{"id": "WebQTest-1416", "question": "what was franklin pierce famous for", "prediction": [["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["people.person.profession"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was franklin pierce famous for [/INST]", "raw_output": {"paths": ["<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.06538984924554825, -0.06599225103855133], "norm_scores": [0.5001506209373474, 0.4998493790626526]}}
{"id": "WebQTest-1417", "question": "when was the last time the boston bruins went to the stanley cup", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_championship_event.runner_up"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the boston bruins went to the stanley cup [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.05094674974679947, -0.059336256235837936], "norm_scores": [0.5020973682403564, 0.49790263175964355]}}
{"id": "WebQTest-1418", "question": "who is meredith gray married to in real life", "prediction": [["fictional_universe.fictional_character.married_to", "fictional_universe.marriage_of_fictional_characters.spouses"], ["fictional_universe.fictional_character.married_to", "fictional_universe.fictional_character.married_to"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is meredith gray married to in real life [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.married_to <SEP> fictional_universe.marriage_of_fictional_characters.spouses </PATH>", "<PATH> fictional_universe.fictional_character.married_to <SEP> fictional_universe.fictional_character.married_to </PATH>"], "scores": [-0.04295526444911957, -0.058560363948345184], "norm_scores": [0.5039011836051941, 0.4960988163948059]}}
{"id": "WebQTest-1419", "question": "what inspired michelangelo buonarroti", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "ground_paths": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inspired michelangelo buonarroti [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.02138805016875267, -0.06981240212917328], "norm_scores": [0.5121037364006042, 0.48789626359939575]}}
{"id": "WebQTest-1420", "question": "what does monsanto own", "prediction": [["business.business_operation.industry", "business.business_operation.industry"], ["organization.organization.parent", "organization.organization.child"]], "ground_paths": [["business.industry.companies"], ["business.business_operation.industry"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does monsanto own [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> organization.organization.parent <SEP> organization.organization.child </PATH>"], "scores": [-0.09075037389993668, -0.12814465165138245], "norm_scores": [0.5093474984169006, 0.49065253138542175]}}
{"id": "WebQTest-1421", "question": "who did woody harrelson play on cheers", "prediction": [["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"]], "ground_paths": [["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.character"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["people.person.profession", "fictional_universe.fictional_character.occupation"], ["film.actor.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did woody harrelson play on cheers [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>"], "scores": [-0.06483833491802216, -0.06619710475206375], "norm_scores": [0.500339686870575, 0.49966031312942505]}}
{"id": "WebQTest-1422", "question": "what language do they speak in sweden yahoo", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they speak in sweden yahoo [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.06013718247413635, -0.07233600318431854], "norm_scores": [0.5030496716499329, 0.4969503581523895]}}
{"id": "WebQTest-1423", "question": "what language did they speak in ghana", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language did they speak in ghana [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.016035860404372215, -0.168222114443779], "norm_scores": [0.5379732847213745, 0.4620266854763031]}}
{"id": "WebQTest-1424", "question": "what form of government do we have in the philippines", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government do we have in the philippines [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.032765574753284454, -0.060991767793893814], "norm_scores": [0.5070560574531555, 0.4929439127445221]}}
{"id": "WebQTest-1425", "question": "what colleges did mitt romney attend", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.religion", "religion.religion.organizations"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.nationality", "base.biblioness.bibs_location.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what colleges did mitt romney attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.06013605371117592, -0.07009541988372803], "norm_scores": [0.5024898052215576, 0.49751016497612]}}
{"id": "WebQTest-1426", "question": "where is ibrahim afellay from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is ibrahim afellay from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.021115001291036606, -0.17344237864017487], "norm_scores": [0.5380083322525024, 0.4619916081428528]}}
{"id": "WebQTest-1427", "question": "who was the secretary of state when andrew jackson was president", "prediction": [["government.government_position_held.appointed_by", "government.government_position_held.office_holder"], ["government.government_position_held.appointed_by", "government.politician.government_positions_held"]], "ground_paths": [["people.person.profession", "people.person.profession"], ["people.person.profession", "common.topic.notable_types"], ["government.government_position_held.appointed_by", "government.government_position_held.office_holder"], ["people.person.nationality", "people.person.nationality"], ["government.us_vice_president.to_president"], ["government.political_appointer.appointees", "government.government_position_held.office_holder"], ["people.person.gender", "people.person.gender"], ["government.government_position_held.appointed_by", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the secretary of state when andrew jackson was president [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.appointed_by <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.appointed_by <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.055372804403305054, -0.06523863971233368], "norm_scores": [0.5024664402008057, 0.49753355979919434]}}
{"id": "WebQTest-1428", "question": "who does ron stoppable 's voice", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does ron stoppable 's voice [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.1110825464129448, -0.11684596538543701], "norm_scores": [0.5014408230781555, 0.4985591471195221]}}
{"id": "WebQTest-1429", "question": "what currency is accepted in new zealand", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency is accepted in new zealand [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.03617843613028526, -0.10586364567279816], "norm_scores": [0.5174142718315125, 0.48258575797080994]}}
{"id": "WebQTest-1432", "question": "what places are in the west midlands", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.location.containedby"], ["location.location.containedby", "common.topic.image"], ["location.location.contains"], []], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what places are in the west midlands [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.02838229201734066, -0.23079408705234528], "norm_scores": [0.5504308938980103, 0.44956910610198975]}}
{"id": "WebQTest-1433", "question": "what to do with kids in asheville nc", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do with kids in asheville nc [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.02824247255921364, -0.06952384859323502], "norm_scores": [0.5103188753128052, 0.4896811246871948]}}
{"id": "WebQTest-1434", "question": "who is in the american league in baseball", "prediction": [["baseball.baseball_team.league"], ["baseball.baseball_league.teams"]], "ground_paths": [["sports.sports_league_participation.league", "sports.sports_team.league"], ["sports.sports_league_participation.league", "sports.sports_league_participation.team"], ["sports.sports_league.teams", "sports.sports_league_participation.team"], ["sports.sports_league.sport", "sports.defunct_sports_team.sport"], ["baseball.baseball_league.teams", "sports.defunct_sports_team.later_known_as"], ["baseball.baseball_team.league"], ["baseball.baseball_league.teams", "sports.sports_team.previously_known_as"], ["sports.sports_league.sport", "sports.sports_team.sport"], ["baseball.baseball_league.teams"], ["sports.sports_league.teams", "sports.sports_team.league"], ["baseball.baseball_team.league", "sports.sports_team.previously_known_as"], ["baseball.baseball_team.league", "sports.defunct_sports_team.later_known_as"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is in the american league in baseball [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_team.league </PATH>", "<PATH> baseball.baseball_league.teams </PATH>"], "scores": [-0.04799215495586395, -0.10874243825674057], "norm_scores": [0.5151829123497009, 0.48481711745262146]}}
{"id": "WebQTest-1435", "question": "who did kurt warner play 4", "prediction": [["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team.roster"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.football_player.passing", "american_football.player_passing_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.player_passing_statistics.player", "american_football.player_passing_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did kurt warner play 4 [/INST]", "raw_output": {"paths": ["<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>"], "scores": [-0.06815505027770996, -0.08471615612506866], "norm_scores": [0.5041401386260986, 0.4958598017692566]}}
{"id": "WebQTest-1436", "question": "who is dirk nowitzki wife", "prediction": [["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.parents"]], "ground_paths": [["people.person.spouse_s", "people.person.spouse_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is dirk nowitzki wife [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>"], "scores": [-0.1432035118341446, -0.15293747186660767], "norm_scores": [0.5024334788322449, 0.4975665211677551]}}
{"id": "WebQTest-1441", "question": "where all did picasso live", "prediction": [["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["influence.influence_node.influenced", "people.person.nationality"], ["influence.influence_node.influenced_by", "people.person.nationality"], ["symbols.namesake.named_after", "film.film.country"], ["symbols.name_source.namesakes", "location.location.containedby"], ["people.person.places_lived", "people.place_lived.location"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["people.person.parents", "people.person.nationality"], ["people.person.nationality", "location.location.containedby"], ["book.written_work.subjects", "book.written_work.subjects"], ["film.film_subject.films", "film.film.country"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where all did picasso live [/INST]", "raw_output": {"paths": ["<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.0714704692363739, -0.08758309483528137], "norm_scores": [0.5040280818939209, 0.4959719181060791]}}
{"id": "WebQTest-1442", "question": "what currency shall i take to croatia", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency shall i take to croatia [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.04429306834936142, -0.08878941088914871], "norm_scores": [0.5111222863197327, 0.4888777732849121]}}
{"id": "WebQTest-1443", "question": "what team is raul ibanez on", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"]], "ground_paths": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is raul ibanez on [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>"], "scores": [-0.041796859353780746, -0.05627123638987541], "norm_scores": [0.5036185383796692, 0.4963814914226532]}}
{"id": "WebQTest-1445", "question": "what tv series did mark harmon star in", "prediction": [["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.languages", "tv.tv_program.languages"]], "ground_paths": [["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"], ["people.person.languages", "film.film.language"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.languages", "tv.tv_program.languages"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what tv series did mark harmon star in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> people.person.languages <SEP> tv.tv_program.languages </PATH>"], "scores": [-0.06082569435238838, -0.08442752063274384], "norm_scores": [0.5059001445770264, 0.49409982562065125]}}
{"id": "WebQTest-1446", "question": "what year did jimmy the rev sullivan die", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.cause_of_death"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did jimmy the rev sullivan die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.0027766916900873184, -0.2265712171792984], "norm_scores": [0.5557162761688232, 0.44428372383117676]}}
{"id": "WebQTest-1447", "question": "who plays anakin skywalker in star wars 1", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["film.film.country", "people.person.nationality"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["film.film.starring", "film.performance.actor"], ["film.film.language", "people.person.languages"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays anakin skywalker in star wars 1 [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.04929741472005844, -0.06978805363178253], "norm_scores": [0.5051224827766418, 0.49487751722335815]}}
{"id": "WebQTest-1448", "question": "where does michelle pfeiffer live now", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["people.person.place_of_birth", "location.us_county.county_seat"], ["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does michelle pfeiffer live now [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.053820572793483734, -0.10168123245239258], "norm_scores": [0.511962890625, 0.4880370795726776]}}
{"id": "WebQTest-1449", "question": "what was laura ingalls wilder famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was laura ingalls wilder famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.020596109330654144, -0.2329835295677185], "norm_scores": [0.5528981685638428, 0.4471018314361572]}}
{"id": "WebQTest-1452", "question": "where is dwight howard now", "prediction": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is dwight howard now [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.04973375052213669, -0.05806705355644226], "norm_scores": [0.5020833015441895, 0.49791666865348816]}}
{"id": "WebQTest-1453", "question": "who is nick lachey married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["people.person.parents", "people.person.children"], ["people.marriage.spouse", "people.marriage.spouse"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["people.person.profession", "people.person.profession"], ["people.person.languages", "people.person.languages"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is nick lachey married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.049663517624139786, -0.10559267550706863], "norm_scores": [0.5139786005020142, 0.48602133989334106]}}
{"id": "WebQTest-1454", "question": "where did madonna grew up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did madonna grew up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0318397618830204, -0.14965157210826874], "norm_scores": [0.5294189453125, 0.4705810844898224]}}
{"id": "WebQTest-1455", "question": "what capital city of brazil", "prediction": [["location.country.capital"], ["location.location.containedby"]], "ground_paths": [["base.biblioness.bibs_location.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what capital city of brazil [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.02215077355504036, -0.21231326460838318], "norm_scores": [0.5473978519439697, 0.4526021182537079]}}
{"id": "WebQTest-1456", "question": "what college did john stockton go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did john stockton go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.0433046892285347, -0.061460915952920914], "norm_scores": [0.5045389533042908, 0.4954610764980316]}}
{"id": "WebQTest-1457", "question": "what kind of currency does mexico use", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of currency does mexico use [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.013117330148816109, -0.20162074267864227], "norm_scores": [0.5469868183135986, 0.45301318168640137]}}
{"id": "WebQTest-1458", "question": "who owns chrysler corporation 2011", "prediction": [["business.business_operation.industry", "business.business_operation.industry"], ["organization.organization.parent", "organization.organization.child"]], "ground_paths": [["business.business_operation.industry", "business.business_operation.industry"], ["organization.organization.parent", "organization.organization_relationship.parent"], ["common.topic.notable_types", "common.topic.notable_types"], ["organization.organization_relationship.child", "organization.organization_relationship.parent"], ["organization.organization.parent", "organization.organization.child"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who owns chrysler corporation 2011 [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> organization.organization.parent <SEP> organization.organization.child </PATH>"], "scores": [-0.049326296895742416, -0.10083911567926407], "norm_scores": [0.5128753781318665, 0.48712462186813354]}}
{"id": "WebQTest-1459", "question": "what are some books that mark twain wrote", "prediction": [["book.written_work.author"], ["book.author.works_written"]], "ground_paths": [["book.book_subject.works"], ["book.written_work.author"], ["book.written_work.subjects"], ["book.book_edition.author_editor"], ["film.film.story_by"], ["film.film_story_contributor.film_story_credits"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some books that mark twain wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.0614616684615612, -0.130148246884346], "norm_scores": [0.5171648859977722, 0.4828351140022278]}}
{"id": "WebQTest-1460", "question": "what was gregor mendel contribution", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was gregor mendel contribution [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.050734590739011765, -0.2163347601890564], "norm_scores": [0.5413057208061218, 0.45869433879852295]}}
{"id": "WebQTest-1461", "question": "who has ray allen dated", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["people.person.parents", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.profession", "people.person.profession"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has ray allen dated [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.04329632967710495, -0.09879080951213837], "norm_scores": [0.5138700604438782, 0.48612990975379944]}}
{"id": "WebQTest-1462", "question": "who is running for vice president with barack obama 2012", "prediction": [["government.election_campaign.election", "government.election_campaign.candidate"], ["government.election_campaign.election", "government.politician.election_campaigns"]], "ground_paths": [["government.us_vice_president.to_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is running for vice president with barack obama 2012 [/INST]", "raw_output": {"paths": ["<PATH> government.election_campaign.election <SEP> government.election_campaign.candidate </PATH>", "<PATH> government.election_campaign.election <SEP> government.politician.election_campaigns </PATH>"], "scores": [-0.043751321732997894, -0.08285211771726608], "norm_scores": [0.5097739696502686, 0.49022606015205383]}}
{"id": "WebQTest-1463", "question": "what language do people from the uk speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people from the uk speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.05626111477613449, -0.07751088589429855], "norm_scores": [0.5053122043609619, 0.4946877658367157]}}
{"id": "WebQTest-1464", "question": "what to do with my kids in toronto", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "ground_paths": [["location.location.containedby"], ["film.film_festival.location"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do with my kids in toronto [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>"], "scores": [-0.04705045372247696, -0.04792556166648865], "norm_scores": [0.5002188086509705, 0.4997812509536743]}}
{"id": "WebQTest-1465", "question": "what kind of government is south korea", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government is south korea [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.034421827644109726, -0.05854162946343422], "norm_scores": [0.5060296654701233, 0.4939703345298767]}}
{"id": "WebQTest-1467", "question": "who did ben franklin marry", "prediction": [["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.place_of_interment.interred_here", "people.place_of_interment.interred_here"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did ben franklin marry [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.11944062262773514, -0.1290869414806366], "norm_scores": [0.5024115443229675, 0.49758845567703247]}}
{"id": "WebQTest-1468", "question": "what sri lanka capital", "prediction": [["location.country.capital"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sri lanka capital [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.05041064694523811, -0.13515035808086395], "norm_scores": [0.5211722254753113, 0.47882774472236633]}}
{"id": "WebQTest-1470", "question": "what kind of government does poland have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does poland have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>"], "scores": [-0.04323297739028931, -0.0466289259493351], "norm_scores": [0.5008490085601807, 0.4991510212421417]}}
{"id": "WebQTest-1471", "question": "what famous people came from delaware", "prediction": [["people.person.place_of_birth"], ["location.location.containedby", "people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what famous people came from delaware [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>"], "scores": [-0.05450103059411049, -0.13180944323539734], "norm_scores": [0.5193175077438354, 0.4806825518608093]}}
{"id": "WebQTest-1472", "question": "when did romney become governor", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did romney become governor [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.0408792644739151, -0.06425424665212631], "norm_scores": [0.5058435201644897, 0.49415653944015503]}}
{"id": "WebQTest-1473", "question": "what currency does cyprus use", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does cyprus use [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.012478833086788654, -0.1991654336452484], "norm_scores": [0.5465365648269653, 0.45346343517303467]}}
{"id": "WebQTest-1475", "question": "what type of guitar does johnny depp play", "prediction": [["music.group_member.instruments_played"], ["music.guitar.guitarists"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of guitar does johnny depp play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.guitar.guitarists </PATH>"], "scores": [-0.05420161783695221, -0.18258430063724518], "norm_scores": [0.532051682472229, 0.4679483473300934]}}
{"id": "WebQTest-1477", "question": "what episode of how i met your mother is carrie underwood on", "prediction": [["people.person.nationality", "tv.tv_program.country_of_origin"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"]], "ground_paths": [["tv.tv_guest_role.actor", "tv.tv_guest_role.episodes_appeared_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what episode of how i met your mother is carrie underwood on [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>"], "scores": [-0.05722504481673241, -0.07227256149053574], "norm_scores": [0.5037617683410645, 0.49623817205429077]}}
{"id": "WebQTest-1478", "question": "who are the 9 justices of the supreme court 2011", "prediction": [["law.court.jurisdiction", "people.person.nationality"], ["book.written_work.subjects", "book.written_work.author"]], "ground_paths": [["law.court.jurisdiction", "people.person.nationality"], ["book.written_work.subjects", "book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the 9 justices of the supreme court 2011 [/INST]", "raw_output": {"paths": ["<PATH> law.court.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> book.written_work.subjects <SEP> book.written_work.author </PATH>"], "scores": [-0.0291751716285944, -0.1220824122428894], "norm_scores": [0.5232101082801819, 0.4767898917198181]}}
{"id": "WebQTest-1479", "question": "who is dustin johnson the golfer", "prediction": [["people.person.profession"], ["sports.pro_athlete.sports_played_professionally", "sports.pro_sports_played.athlete"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is dustin johnson the golfer [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> sports.pro_athlete.sports_played_professionally <SEP> sports.pro_sports_played.athlete </PATH>"], "scores": [-0.013964846730232239, -0.15784664452075958], "norm_scores": [0.5359085202217102, 0.4640914499759674]}}
{"id": "WebQTest-1480", "question": "who did cher have a son with", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["music.artist.concert_tours", "music.concert_tour.artist"], ["people.person.parents", "people.person.children"], ["people.marriage.spouse", "people.marriage.spouse"], ["base.popstra.celebrity.breakup", "base.popstra.breakup.participant"], ["music.artist.label", "music.artist.label"], ["award.award_winner.awards_won", "award.award_honor.award_winner"], ["people.person.nationality", "people.person.nationality"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.profession", "people.person.profession"], ["music.composer.compositions", "music.composer.compositions"], ["award.award_nomination.award_nominee", "award.award_nominee.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominee.award_nominations"], ["people.person.children", "people.person.children"], ["base.americancomedy.double_act.comic", "base.americancomedy.double_act.straight_man"], ["music.artist.genre", "music.artist.genre"], ["music.album.artist", "music.album.artist"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.celebrity.breakup", "base.popstra.celebrity.breakup"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did cher have a son with [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.05112733319401741, -0.11717433482408524], "norm_scores": [0.5165057182312012, 0.48349425196647644]}}
{"id": "WebQTest-1481", "question": "what city is the university of oregon state in", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city is the university of oregon state in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.014535288326442242, -0.3586037755012512], "norm_scores": [0.5851784348487854, 0.4148215055465698]}}
{"id": "WebQTest-1482", "question": "who played carmen cortez in spy kids", "prediction": [["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.performance.film", "film.performance.actor"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played carmen cortez in spy kids [/INST]", "raw_output": {"paths": ["<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.12550976872444153, -0.130612313747406], "norm_scores": [0.5012755990028381, 0.4987243711948395]}}
{"id": "WebQTest-1483", "question": "what is the political system in germany", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the political system in germany [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.03479347750544548, -0.057876456528902054], "norm_scores": [0.5057705044746399, 0.4942295253276825]}}
{"id": "WebQTest-1484", "question": "what prompted shakespeare to write poetry", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced_by"]], "ground_paths": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what prompted shakespeare to write poetry [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced_by </PATH>"], "scores": [-0.056020233780145645, -0.05904068052768707], "norm_scores": [0.5007551312446594, 0.49924489855766296]}}
{"id": "WebQTest-1485", "question": "what is the currency in turkey called", "prediction": [["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency in turkey called [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.07406000792980194, -0.07489404082298279], "norm_scores": [0.5002084970474243, 0.4997915029525757]}}
{"id": "WebQTest-1486", "question": "what school did mark zuckerberg attend", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did mark zuckerberg attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.04613080248236656, -0.06111447513103485], "norm_scores": [0.5037458539009094, 0.4962541460990906]}}
{"id": "WebQTest-1487", "question": "where does kate middleton live 2012", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does kate middleton live 2012 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.03977322578430176, -0.07097087055444717], "norm_scores": [0.5077987313270569, 0.49220120906829834]}}
{"id": "WebQTest-1488", "question": "what type of art does marc chagall do", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"], ["book.school_or_movement.associated_authors"], ["visual_art.art_period_movement.associated_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art does marc chagall do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.004770309664309025, -0.12664686143398285], "norm_scores": [0.5304314494132996, 0.46956852078437805]}}
{"id": "WebQTest-1489", "question": "what character did billy d williams play in star wars", "prediction": [["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what character did billy d williams play in star wars [/INST]", "raw_output": {"paths": ["<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.06647218763828278, -0.11334189772605896], "norm_scores": [0.511715292930603, 0.488284707069397]}}
{"id": "WebQTest-1490", "question": "what language is spoken in argentina", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is spoken in argentina [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.03298911824822426, -0.10117162764072418], "norm_scores": [0.5170390605926514, 0.482960969209671]}}
{"id": "WebQTest-1493", "question": "what school did wilfred owen go to", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did wilfred owen go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.045954447239637375, -0.07029960304498672], "norm_scores": [0.506085991859436, 0.49391400814056396]}}
{"id": "WebQTest-1494", "question": "who is kevin jonas married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.person.spouse_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is kevin jonas married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.06831538677215576, -0.09708652645349503], "norm_scores": [0.5071923136711121, 0.4928077161312103]}}
{"id": "WebQTest-1495", "question": "where does airtran airways fly", "prediction": [["aviation.airline.airports_served", "aviation.airline_airport_presence.airport"], ["aviation.airline_airport_presence.airline", "aviation.airline_airport_presence.airport"]], "ground_paths": [["aviation.airline.focus_cities"], ["aviation.airline.airports_served", "aviation.airline_airport_presence.airport"], ["aviation.airport.focus_city_for"], ["aviation.airline_airport_presence.airline", "aviation.airport.airlines"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does airtran airways fly [/INST]", "raw_output": {"paths": ["<PATH> aviation.airline.airports_served <SEP> aviation.airline_airport_presence.airport </PATH>", "<PATH> aviation.airline_airport_presence.airline <SEP> aviation.airline_airport_presence.airport </PATH>"], "scores": [-0.046586774289608, -0.05656903237104416], "norm_scores": [0.502495527267456, 0.49750444293022156]}}
{"id": "WebQTest-1496", "question": "where does the city of paris get its name", "prediction": [["symbols.namesake.named_after"], ["symbols.name_source.namesakes"]], "ground_paths": [["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the city of paris get its name [/INST]", "raw_output": {"paths": ["<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.0521651916205883, -0.06912393122911453], "norm_scores": [0.504239559173584, 0.49576041102409363]}}
{"id": "WebQTest-1499", "question": "who did sandra bullock marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["people.marriage.spouse", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.popstra.infidelity.victim", "base.popstra.celebrity.infidelity_perpetrator"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.infidelity.participant", "base.popstra.infidelity.perpetrator"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did sandra bullock marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.04293544590473175, -0.10668187588453293], "norm_scores": [0.5159311890602112, 0.48406878113746643]}}
{"id": "WebQTest-1500", "question": "where is olympic national park located", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "ground_paths": [["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is olympic national park located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>"], "scores": [-0.057106416672468185, -0.08032480627298355], "norm_scores": [0.5058043003082275, 0.4941956400871277]}}
{"id": "WebQTest-1501", "question": "where is the un based", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization_founder.organizations_founded", "location.location.containedby"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization_founder.organizations_founded", "location.location.containedby"], ["book.book_subject.works", "base.reviews.review.mentions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the un based [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.location.containedby </PATH>"], "scores": [-0.03788251429796219, -0.11971062421798706], "norm_scores": [0.5204456448554993, 0.47955435514450073]}}
{"id": "WebQTest-1502", "question": "what city and state is yale located in", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city and state is yale located in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.009228669106960297, -0.37316659092903137], "norm_scores": [0.5899933576583862, 0.4100066125392914]}}
{"id": "WebQTest-1503", "question": "where did thomas paine die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people_deceased_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did thomas paine die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>"], "scores": [-0.001288590021431446, -0.3247981071472168], "norm_scores": [0.5801792740821838, 0.4198206961154938]}}
{"id": "WebQTest-1504", "question": "where was barack hussein obama sr born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was barack hussein obama sr born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0033066249452531338, -0.296939879655838], "norm_scores": [0.5728853344917297, 0.4271146357059479]}}
{"id": "WebQTest-1506", "question": "where is the home stadium of the new england patriots", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"]], "ground_paths": [["sports.sports_facility.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the home stadium of the new england patriots [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>"], "scores": [-0.013713154941797256, -0.11965073645114899], "norm_scores": [0.5264596343040466, 0.47354036569595337]}}
{"id": "WebQTest-1507", "question": "who was kate chopin", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was kate chopin [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0027340599335730076, -0.3754672706127167], "norm_scores": [0.5921192765235901, 0.4078807830810547]}}
{"id": "WebQTest-1508", "question": "when did bill clinton go to college", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did bill clinton go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.04444081708788872, -0.06843794137239456], "norm_scores": [0.505998969078064, 0.49400103092193604]}}
{"id": "WebQTest-1510", "question": "what are staffordshire terriers", "prediction": [["biology.animal_breed.breed_of"], ["biology.breed_of_breed.breed_of"]], "ground_paths": [["biology.animal_breed.breed_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are staffordshire terriers [/INST]", "raw_output": {"paths": ["<PATH> biology.animal_breed.breed_of </PATH>", "<PATH> biology.breed_of_breed.breed_of </PATH>"], "scores": [-0.025742057710886, -0.1289496272802353], "norm_scores": [0.5257790088653564, 0.47422099113464355]}}
{"id": "WebQTest-1513", "question": "what to do downtown sacramento", "prediction": [["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do downtown sacramento [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.06463044881820679, -0.08872018754482269], "norm_scores": [0.5060220956802368, 0.4939778447151184]}}
{"id": "WebQTest-1515", "question": "who is michael j fox parents", "prediction": [["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is michael j fox parents [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.06732004135847092, -0.09917780756950378], "norm_scores": [0.5079637765884399, 0.49203625321388245]}}
{"id": "WebQTest-1516", "question": "where did sanjay gupta go to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["education.honorary_degree_recipient.honorary_degrees", "education.honorary_degree.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did sanjay gupta go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.05226469784975052, -0.06694719940423965], "norm_scores": [0.5036705732345581, 0.4963294267654419]}}
{"id": "WebQTest-1518", "question": "what countries are involved in the nafta", "prediction": [["organization.membership_organization.members", "organization.organization_member.member_of"], ["organization.organization_membership.organization", "organization.organization_member.member_of"]], "ground_paths": [["organization.organization_founder.organizations_founded", "people.person.nationality"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["organization.membership_organization.members", "organization.organization_member.member_of"], ["organization.membership_organization.members", "organization.organization_membership.member"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are involved in the nafta [/INST]", "raw_output": {"paths": ["<PATH> organization.membership_organization.members <SEP> organization.organization_member.member_of </PATH>", "<PATH> organization.organization_membership.organization <SEP> organization.organization_member.member_of </PATH>"], "scores": [-0.030131015926599503, -0.06505229324102402], "norm_scores": [0.5087294578552246, 0.4912705719470978]}}
{"id": "WebQTest-1519", "question": "what continent is cayman islands located", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what continent is cayman islands located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.04697403684258461, -0.13486628234386444], "norm_scores": [0.5219589471817017, 0.4780410826206207]}}
{"id": "WebQTest-1520", "question": "what language speak in brazil", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language speak in brazil [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.0046875691041350365, -0.3119337856769562], "norm_scores": [0.5762129426002502, 0.42378702759742737]}}
{"id": "WebQTest-1522", "question": "who does kroy biermann play football for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"]], "ground_paths": [["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does kroy biermann play football for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>"], "scores": [-0.05393436178565025, -0.07394161075353622], "norm_scores": [0.5050016641616821, 0.49499836564064026]}}
{"id": "WebQTest-1523", "question": "when is the last time the pittsburgh steelers won a superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is the last time the pittsburgh steelers won a superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.04839937761425972, -0.062414828687906265], "norm_scores": [0.5035037994384766, 0.49649620056152344]}}
{"id": "WebQTest-1524", "question": "who invented islamic religion", "prediction": [["religion.religion.notable_figures"], ["religion.religion.founding_figures"]], "ground_paths": [["religion.religion.notable_figures"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who invented islamic religion [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.notable_figures </PATH>", "<PATH> religion.religion.founding_figures </PATH>"], "scores": [-0.008804328739643097, -0.1562475711107254], "norm_scores": [0.5367941856384277, 0.46320584416389465]}}
{"id": "WebQTest-1525", "question": "who were we fighting in the gulf war", "prediction": [["military.military_combatant_group.conflict", "military.military_combatant.military_conflicts"], ["military.military_combatant_group.conflict", "military.military_combatant_group.combatants"]], "ground_paths": [["military.military_person.participated_in_conflicts", "people.person.nationality"], ["film.film.subjects", "film.film.country"], ["film.film_subject.films", "film.film.country"], ["book.book_subject.works", "media_common.netflix_title.netflix_genres"], ["military.casualties.military_conflict", "military.military_combatant.casualties"], ["time.event.includes_event", "base.culturalevent.event.entity_involved"], ["time.event.included_in_event", "base.culturalevent.event.entity_involved"], ["film.film.genre", "film.film.country"], ["film.film_genre.films_in_this_genre", "film.film.country"], ["military.military_conflict.casualties", "military.military_combatant.casualties"], ["time.event.locations"], ["base.culturalevent.event.entity_involved", "people.person.place_of_birth"], ["military.military_conflict.combatants", "military.military_combatant_group.combatants"], ["base.culturalevent.event.entity_involved", "people.person.nationality"], ["film.film_subject.films", "tv.tv_program.country_of_origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were we fighting in the gulf war [/INST]", "raw_output": {"paths": ["<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant.military_conflicts </PATH>", "<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant_group.combatants </PATH>"], "scores": [-0.051769454032182693, -0.06322570145130157], "norm_scores": [0.5028640627861023, 0.4971359968185425]}}
{"id": "WebQTest-1526", "question": "how many languages do they speak in switzerland", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how many languages do they speak in switzerland [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.05575307086110115, -0.10291805118322372], "norm_scores": [0.511789083480835, 0.4882109463214874]}}
{"id": "WebQTest-1527", "question": "what political party does barack obama represent", "prediction": [["government.politician.party", "government.political_party_tenure.party"], ["government.election_campaign.candidate", "government.election_campaign.party"]], "ground_paths": [["government.politician.election_campaigns", "government.election_campaign.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"], ["government.election.winner", "government.primary_election.party"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party does barack obama represent [/INST]", "raw_output": {"paths": ["<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.party </PATH>"], "scores": [-0.051938481628894806, -0.05321943759918213], "norm_scores": [0.5003201961517334, 0.4996797442436218]}}
{"id": "WebQTest-1528", "question": "what movies does logan lerman play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["tv.tv_guest_role.actor", "tv.tv_guest_role.episodes_appeared_in"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.film.starring"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies does logan lerman play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.06181997433304787, -0.14395448565483093], "norm_scores": [0.5205220580101013, 0.4794778823852539]}}
{"id": "WebQTest-1529", "question": "where did emperor constantine die", "prediction": [["people.deceased_person.place_of_death"], ["location.location.people_born_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did emperor constantine die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.002306785201653838, -0.3641456663608551], "norm_scores": [0.5894855260848999, 0.4105145037174225]}}
{"id": "WebQTest-1531", "question": "where is mali located", "prediction": [["location.location.containedby"], ["base.locations.countries.continent"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mali located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.08290766179561615, -0.08897438645362854], "norm_scores": [0.5015166997909546, 0.4984833300113678]}}
{"id": "WebQTest-1533", "question": "what planet did luke skywalker came from", "prediction": [["fictional_universe.fictional_setting.characters_that_have_lived_here"], ["fictional_universe.fictional_character.place_of_birth"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what planet did luke skywalker came from [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_setting.characters_that_have_lived_here </PATH>", "<PATH> fictional_universe.fictional_character.place_of_birth </PATH>"], "scores": [-0.03271644562482834, -0.057255819439888], "norm_scores": [0.5061345100402832, 0.4938654601573944]}}
{"id": "WebQTest-1534", "question": "who are the coaches for the oakland raiders", "prediction": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["american_football.football_team.current_head_coach"]], "ground_paths": [["american_football.football_coach.current_team_head_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the coaches for the oakland raiders [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> american_football.football_team.current_head_coach </PATH>"], "scores": [-0.03557831048965454, -0.07714841514825821], "norm_scores": [0.5103909969329834, 0.4896089732646942]}}
{"id": "WebQTest-1535", "question": "where was the first microsoft headquarters located", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"]], "ground_paths": [["base.engineering1.rfid_company.location", "base.biblioness.bibs_location.country"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the first microsoft headquarters located [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>"], "scores": [-0.053769804537296295, -0.13268503546714783], "norm_scores": [0.519718587398529, 0.48028144240379333]}}
{"id": "WebQTest-1536", "question": "where did john steinbeck go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "base.biblioness.bibs_location.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did john steinbeck go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.05639626085758209, -0.056503210216760635], "norm_scores": [0.5000267624855042, 0.49997326731681824]}}
{"id": "WebQTest-1537", "question": "what did niels bohr discover about the atomic structure", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"]], "ground_paths": [["base.nobelprizes.nobel_honor.nobel_prize_winner", "base.nobelprizes.nobel_honor.subject_area"], ["education.academic.advisees", "book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did niels bohr discover about the atomic structure [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.05791715160012245, -0.11912045627832413], "norm_scores": [0.5152960419654846, 0.4847039580345154]}}
{"id": "WebQTest-1538", "question": "what books did louis sachar wrote", "prediction": [["book.book_edition.author_editor"], ["book.author.works_written"]], "ground_paths": [["book.book_edition.author_editor"], ["book.written_work.author"], ["book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what books did louis sachar wrote [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.05334002152085304, -0.13853515684604645], "norm_scores": [0.521285891532898, 0.47871407866477966]}}
{"id": "WebQTest-1539", "question": "who is ryan reynolds married to 2012", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["people.person.spouse_s", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.profession", "people.person.profession"], ["common.topic.notable_types", "common.topic.notable_types"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is ryan reynolds married to 2012 [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.04550424590706825, -0.12073964625597], "norm_scores": [0.5187999606132507, 0.48120003938674927]}}
{"id": "WebQTest-1540", "question": "what is newt gingrich 's wife 's name", "prediction": [["people.person.nationality", "people.person.nationality"], ["people.person.children", "people.person.parents"]], "ground_paths": [["people.marriage.spouse", "people.person.spouse_s"], ["people.person.profession", "people.person.profession"], ["people.person.religion", "people.person.religion"], ["people.person.profession", "common.topic.notable_types"], ["film.film.executive_produced_by", "film.film.executive_produced_by"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is newt gingrich 's wife 's name [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>"], "scores": [-0.08525485545396805, -0.14815951883792877], "norm_scores": [0.5157209634780884, 0.4842790365219116]}}
{"id": "WebQTest-1541", "question": "what type of music did antonio vivaldi play", "prediction": [["music.artist.genre"], ["music.genre.artists"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did antonio vivaldi play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>"], "scores": [-0.013928540050983429, -0.21096470952033997], "norm_scores": [0.5491002798080444, 0.4508996903896332]}}
{"id": "WebQTest-1542", "question": "what school does james franco go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school does james franco go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.0496344119310379, -0.05022410675883293], "norm_scores": [0.5001474022865295, 0.49985259771347046]}}
{"id": "WebQTest-1543", "question": "what is samuel langhorne clemens well known for", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is samuel langhorne clemens well known for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.009054907597601414, -0.24606555700302124], "norm_scores": [0.5589768290519714, 0.44102320075035095]}}
{"id": "WebQTest-1544", "question": "who was president after william henry harrison", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"]], "ground_paths": [["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["freebase.type_profile.equivalent_topic", "common.topic.notable_types"], ["government.government_office_or_title.jurisdiction", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was president after william henry harrison [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>"], "scores": [-0.05231393501162529, -0.08472476154565811], "norm_scores": [0.5081019997596741, 0.4918980002403259]}}
{"id": "WebQTest-1546", "question": "where did rick santorum attend high school", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did rick santorum attend high school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.040886882692575455, -0.06381115317344666], "norm_scores": [0.5057308077812195, 0.4942691922187805]}}
{"id": "WebQTest-1547", "question": "what is the title of george bush 's new book", "prediction": [["book.written_work.author"], ["book.author.works_written"]], "ground_paths": [["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the title of george bush 's new book [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.0542292520403862, -0.13305260241031647], "norm_scores": [0.5196956396102905, 0.48030439019203186]}}
{"id": "WebQTest-1548", "question": "where did antoine lavoisier die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people_deceased_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did antoine lavoisier die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>"], "scores": [-0.0008395581389777362, -0.34817028045654297], "norm_scores": [0.5859701633453369, 0.41402989625930786]}}
{"id": "WebQTest-1550", "question": "what is the name of king george vi wife", "prediction": [["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.parents"]], "ground_paths": [["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.religion", "people.person.religion"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["people.family.members", "people.family.members"], ["film.film.subjects", "film.film_subject.films"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of king george vi wife [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>"], "scores": [-0.13837231695652008, -0.1437768042087555], "norm_scores": [0.5013511180877686, 0.49864891171455383]}}
{"id": "WebQTest-1551", "question": "what year did seattle seahawks go to the super bowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did seattle seahawks go to the super bowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.08133905380964279, -0.08433117717504501], "norm_scores": [0.5007480382919312, 0.49925196170806885]}}
{"id": "WebQTest-1553", "question": "where does sami yusuf live", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does sami yusuf live [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.081619992852211, -0.12130594998598099], "norm_scores": [0.5099201798439026, 0.490079790353775]}}
{"id": "WebQTest-1554", "question": "what is the official language of brazil now", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the official language of brazil now [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.00238114595413208, -0.3619741201400757], "norm_scores": [0.5889418721199036, 0.41105806827545166]}}
{"id": "WebQTest-1555", "question": "what is the parliament of nepal called", "prediction": [["government.governmental_jurisdiction.governing_officials", "government.governmental_body.members"], ["government.governmental_jurisdiction.government_bodies"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the parliament of nepal called [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.governmental_body.members </PATH>", "<PATH> government.governmental_jurisdiction.government_bodies </PATH>"], "scores": [-0.0394480898976326, -0.046142082661390305], "norm_scores": [0.5016735196113586, 0.49832653999328613]}}
{"id": "WebQTest-1556", "question": "when 's the last time the mets won the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when 's the last time the mets won the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>"], "scores": [-0.04437008500099182, -0.04722457379102707], "norm_scores": [0.5007136464118958, 0.49928638339042664]}}
{"id": "WebQTest-1557", "question": "where do florida panthers play", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"]], "ground_paths": [["sports.sports_facility.teams"], ["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do florida panthers play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>"], "scores": [-0.024741703644394875, -0.09566891938447952], "norm_scores": [0.517724335193634, 0.4822756052017212]}}
{"id": "WebQTest-1558", "question": "who is john cena married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "ground_paths": [["people.person.nationality", "people.person.nationality"], ["people.person.spouse_s", "people.person.spouse_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is john cena married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>"], "scores": [-0.04867265000939369, -0.08469473570585251], "norm_scores": [0.5090045928955078, 0.49099546670913696]}}
{"id": "WebQTest-1559", "question": "what to do san jose ca", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "ground_paths": [["film.film_festival.location"], ["travel.tourist_attraction.near_travel_destination"], ["location.neighborhood.neighborhood_of"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do san jose ca [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>"], "scores": [-0.05166308581829071, -0.053659871220588684], "norm_scores": [0.5004991888999939, 0.4995008111000061]}}
{"id": "WebQTest-1560", "question": "who founded new york university", "prediction": [["education.educational_institution.subsidiary_or_constituent_schools", "symbols.namesake.named_after"], ["education.educational_institution.parent_institution", "people.person.parents"]], "ground_paths": [["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who founded new york university [/INST]", "raw_output": {"paths": ["<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> symbols.namesake.named_after </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> people.person.parents </PATH>"], "scores": [-0.08907962590456009, -0.09289629012346268], "norm_scores": [0.5009541511535645, 0.49904584884643555]}}
{"id": "WebQTest-1561", "question": "who created arthur", "prediction": [["tv.tv_program_creator.programs_created"], ["tv.tv_program.program_creator"]], "ground_paths": [["tv.tv_program.program_creator"], ["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who created arthur [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program_creator.programs_created </PATH>", "<PATH> tv.tv_program.program_creator </PATH>"], "scores": [-0.02390495501458645, -0.14671117067337036], "norm_scores": [0.530663013458252, 0.46933698654174805]}}
{"id": "WebQTest-1562", "question": "what countries constitute the eu", "prediction": [["organization.organization_membership.organization", "organization.organization_member.member_of"], ["organization.organization.geographic_scope", "base.locations.continents.countries_within"]], "ground_paths": [["base.industrystandards.standard_organization.standards", "base.aareas.coding_scheme.geo_scope"], ["organization.organization.geographic_scope", "base.locations.continents.countries_within"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["organization.organization_founder.organizations_founded"], ["government.government_position_held.jurisdiction_of_office", "government.political_district.representatives"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.district_represented"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["organization.organization.geographic_scope", "location.location.containedby"], ["organization.membership_organization.members", "organization.organization_member.member_of"], ["organization.organization.geographic_scope", "base.locations.countries.continent"], ["organization.organization_membership.organization", "organization.organization_membership.member"], ["government.governmental_jurisdiction.governing_officials", "government.political_district.representatives"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.district_represented"], ["organization.organization.geographic_scope", "location.location.primarily_containedby"], ["organization.organization.founders"], ["organization.membership_organization.members", "organization.organization_membership.member"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries constitute the eu [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_membership.organization <SEP> organization.organization_member.member_of </PATH>", "<PATH> organization.organization.geographic_scope <SEP> base.locations.continents.countries_within </PATH>"], "scores": [-0.07339522987604141, -0.0781257227063179], "norm_scores": [0.5011826157569885, 0.4988173544406891]}}
{"id": "WebQTest-1563", "question": "what are all the countries in north africa", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are all the countries in north africa [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.02709609642624855, -0.21181999146938324], "norm_scores": [0.5460500717163086, 0.453949898481369]}}
{"id": "WebQTest-1564", "question": "what did bobby darin die of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did bobby darin die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.0049474663101136684, -0.18050146102905273], "norm_scores": [0.5437760949134827, 0.45622387528419495]}}
{"id": "WebQTest-1565", "question": "where does houston dynamo play", "prediction": [["sports.sports_facility.teams"], ["sports.sports_team.arena_stadium"]], "ground_paths": [["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does houston dynamo play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.arena_stadium </PATH>"], "scores": [-0.047822028398513794, -0.048880256712436676], "norm_scores": [0.5002645254135132, 0.49973544478416443]}}
{"id": "WebQTest-1566", "question": "what kind of guitar john mayer", "prediction": [["music.guitarist.guitars_played"], ["music.guitar.guitarists"]], "ground_paths": [["music.guitar.guitarists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of guitar john mayer [/INST]", "raw_output": {"paths": ["<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.guitar.guitarists </PATH>"], "scores": [-0.060561347752809525, -0.07015946507453918], "norm_scores": [0.5023995041847229, 0.4976004660129547]}}
{"id": "WebQTest-1567", "question": "what cancer did patrick swayze", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"]], "ground_paths": [["medicine.disease.notable_people_with_this_condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what cancer did patrick swayze [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>"], "scores": [-0.004937293473631144, -0.14270153641700745], "norm_scores": [0.5343866944313049, 0.46561333537101746]}}
{"id": "WebQTest-1568", "question": "what university did obama graduated from", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what university did obama graduated from [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.04157455265522003, -0.044930350035429], "norm_scores": [0.5008389949798584, 0.4991610646247864]}}
{"id": "WebQTest-1569", "question": "what jamaican money called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what jamaican money called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.002609862480312586, -0.3145945370197296], "norm_scores": [0.5773696303367615, 0.4226303994655609]}}
{"id": "WebQTest-1570", "question": "where is the arizona cardinals stadium located", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"]], "ground_paths": [["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the arizona cardinals stadium located [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>"], "scores": [-0.022803669795393944, -0.10684549063444138], "norm_scores": [0.5209981203079224, 0.4790019094944]}}
{"id": "WebQTest-1571", "question": "what is sir isaac newton theory", "prediction": [["symbols.namesake.named_after"], ["symbols.name_source.namesakes"]], "ground_paths": [["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is sir isaac newton theory [/INST]", "raw_output": {"paths": ["<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.03405352309346199, -0.10701621323823929], "norm_scores": [0.5182325839996338, 0.4817674160003662]}}
{"id": "WebQTest-1572", "question": "what should i do today in san francisco", "prediction": [["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.neighborhood.neighborhood_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what should i do today in san francisco [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.06609263271093369, -0.08306299149990082], "norm_scores": [0.504242479801178, 0.49575749039649963]}}
{"id": "WebQTest-1573", "question": "who influenced picasso art", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who influenced picasso art [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.041167374700307846, -0.04139197617769241], "norm_scores": [0.5000561475753784, 0.49994388222694397]}}
{"id": "WebQTest-1574", "question": "what is my timezone in pennsylvania", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is my timezone in pennsylvania [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>"], "scores": [-7.056265894789249e-05, -0.42357510328292847], "norm_scores": [0.6043215394020081, 0.39567846059799194]}}
{"id": "WebQTest-1575", "question": "what products and\\/or services does google offer customers", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization.sectors"]], "ground_paths": [["computer.software.developer"], ["projects.project_participation.participant", "projects.project_participation.project"], ["computer.computer_manufacturer_brand.computer_models"], ["business.consumer_company.products", "business.consumer_product.company"], ["business.company_product_relationship.company", "business.consumer_product.company"], ["computer.software_developer.software"], ["internet.website.owner"], ["business.consumer_company.products", "business.company_product_relationship.consumer_product"], ["business.company_product_relationship.company", "business.company_product_relationship.consumer_product"], ["business.consumer_company.products"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what products and\\/or services does google offer customers [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.sectors </PATH>"], "scores": [-0.1799120008945465, -0.21859431266784668], "norm_scores": [0.5096693634986877, 0.49033063650131226]}}
{"id": "WebQTest-1577", "question": "who did cliff lee play for last year", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did cliff lee play for last year [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>"], "scores": [-0.03888457268476486, -0.04140776768326759], "norm_scores": [0.500630795955658, 0.49936920404434204]}}
{"id": "WebQTest-1580", "question": "what type of democracy is the united states", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of democracy is the united states [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.03128267079591751, -0.06365079432725906], "norm_scores": [0.5080913305282593, 0.4919086694717407]}}
{"id": "WebQTest-1582", "question": "what time in china hong kong", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["time.time_zone.locations_in_this_time_zone"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time in china hong kong [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>"], "scores": [-0.026849843561649323, -0.07960948348045349], "norm_scores": [0.5131868720054626, 0.48681315779685974]}}
{"id": "WebQTest-1583", "question": "what all movies has taylor lautner been in", "prediction": [["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_winner.awards_won", "award.award_honor.honored_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what all movies has taylor lautner been in [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.06993407756090164, -0.0731668472290039], "norm_scores": [0.5008081793785095, 0.4991917908191681]}}
{"id": "WebQTest-1584", "question": "what disease did bernie mac die of", "prediction": [["people.deceased_person.cause_of_death"], ["medicine.disease.notable_people_with_this_condition"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what disease did bernie mac die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>"], "scores": [-0.015585232526063919, -0.10562609136104584], "norm_scores": [0.5224949717521667, 0.4775049686431885]}}
{"id": "WebQTest-1585", "question": "who plays spike spiegel", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["base.schemastaging.tv_character_extra.regular_dubbing_performances", "base.schemastaging.tv_star_dubbing_performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays spike spiegel [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.04572700336575508, -0.12409142404794693], "norm_scores": [0.5195810794830322, 0.4804189205169678]}}
{"id": "WebQTest-1586", "question": "who was queen isabella 's mother", "prediction": [["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was queen isabella 's mother [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.0507500022649765, -0.12741124629974365], "norm_scores": [0.5191559195518494, 0.48084408044815063]}}
{"id": "WebQTest-1587", "question": "who does chris hemsworth have a baby with", "prediction": [["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.parents"]], "ground_paths": [["celebrities.friendship.friend", "celebrities.friendship.friend"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.profession", "people.person.profession"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.children", "people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.parents", "people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does chris hemsworth have a baby with [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>"], "scores": [-0.08342985808849335, -0.09844305366277695], "norm_scores": [0.5037532448768616, 0.4962467849254608]}}
{"id": "WebQTest-1589", "question": "where is the mts iceplex in winnipeg", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the mts iceplex in winnipeg [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0784638524055481, -0.20345014333724976], "norm_scores": [0.531205952167511, 0.468794047832489]}}
{"id": "WebQTest-1590", "question": "what are the most popular religions in israel", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "ground_paths": [["people.person.quotations", "people.person.religion"], ["film.film_location.featured_in_films", "media_common.netflix_title.netflix_genres"], ["people.person.nationality", "people.person.religion"], ["location.country.administrative_divisions", "religion.religion.sacred_sites"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["symbols.namesake.named_after", "religion.religion.notable_figures"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the most popular religions in israel [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>"], "scores": [-0.010680201463401318, -0.07971446961164474], "norm_scores": [0.5172516703605652, 0.48274827003479004]}}
{"id": "WebQTest-1591", "question": "what was obama 's former occupation", "prediction": [["people.person.profession"], ["people.person.profession", "people.profession.specializations"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was obama 's former occupation [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.person.profession <SEP> people.profession.specializations </PATH>"], "scores": [-0.04424405097961426, -0.13822905719280243], "norm_scores": [0.5234789848327637, 0.4765210449695587]}}
{"id": "WebQTest-1592", "question": "what are all the songs justin bieber wrote", "prediction": [["music.composer.compositions"], ["music.recording.artist"]], "ground_paths": [["music.recording.artist"], ["music.composer.compositions"], ["music.album.artist"], ["music.lyricist.lyrics_written"], ["music.recording.featured_artists"], ["music.artist.album"], ["music.composition.lyricist"], ["music.composition.composer"], ["music.featured_artist.recordings"], ["music.featured_artist.albums"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are all the songs justin bieber wrote [/INST]", "raw_output": {"paths": ["<PATH> music.composer.compositions </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.09651555120944977, -0.12118754535913467], "norm_scores": [0.5061677098274231, 0.4938323199748993]}}
{"id": "WebQTest-1593", "question": "where is the thames river source", "prediction": [["geography.river.origin"], ["geography.river.mouth"]], "ground_paths": [["geography.river.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the thames river source [/INST]", "raw_output": {"paths": ["<PATH> geography.river.origin </PATH>", "<PATH> geography.river.mouth </PATH>"], "scores": [-0.027826717123389244, -0.2969459593296051], "norm_scores": [0.5668766498565674, 0.4331233501434326]}}
{"id": "WebQTest-1595", "question": "what new movies is robert pattinson in", "prediction": [["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"]], "ground_paths": [["music.recording.artist", "music.recording.releases"], ["film.actor.film", "film.film.starring"], ["film.producer.films_executive_produced"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["music.artist.track", "music.recording.releases"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what new movies is robert pattinson in [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>"], "scores": [-0.06637708097696304, -0.0717257633805275], "norm_scores": [0.5013371706008911, 0.4986628293991089]}}
{"id": "WebQTest-1596", "question": "what language do they use in pakistan", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they use in pakistan [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.057799749076366425, -0.0798477903008461], "norm_scores": [0.5055117607116699, 0.4944881796836853]}}
{"id": "WebQTest-1597", "question": "what kind of products does nike sell", "prediction": [["business.business_operation.industry"], ["business.industry.companies"]], "ground_paths": [["business.industry.companies"], ["business.business_operation.industry"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of products does nike sell [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry </PATH>", "<PATH> business.industry.companies </PATH>"], "scores": [-0.04908769950270653, -0.13098689913749695], "norm_scores": [0.5204633474349976, 0.47953665256500244]}}
{"id": "WebQTest-1598", "question": "who did lbj marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "ground_paths": [["book.written_work.subjects", "people.person.nationality"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.profession", "common.topic.notable_types"], ["people.person.parents", "people.person.parents"], ["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_owner.pets_owned"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_ownership.owner"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_owner.pets_owned"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did lbj marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>"], "scores": [-0.06223185732960701, -0.09323107451200485], "norm_scores": [0.5077491998672485, 0.49225082993507385]}}
{"id": "WebQTest-1599", "question": "where did kim jong il die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kim jong il die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people </PATH>"], "scores": [-0.0012761078542098403, -0.46525779366493225], "norm_scores": [0.6139582991600037, 0.38604167103767395]}}
{"id": "WebQTest-1603", "question": "what is the sacred text of daoism", "prediction": [["religion.religion.texts"], ["religion.religious_text.religious_text_of"]], "ground_paths": [["religion.religion.texts"], ["religion.religion.notable_figures"], ["book.written_work.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the sacred text of daoism [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.texts </PATH>", "<PATH> religion.religious_text.religious_text_of </PATH>"], "scores": [-0.040541671216487885, -0.045687172561883926], "norm_scores": [0.5012863278388977, 0.4987136125564575]}}
{"id": "WebQTest-1604", "question": "what countries speak english natively", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries speak english natively [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.058863695710897446, -0.09414815902709961], "norm_scores": [0.5088201761245728, 0.49117979407310486]}}
{"id": "WebQTest-1607", "question": "who won fedex cup 2012", "prediction": [["award.competition.winner", "people.person.profession"], ["award.competitor.competitions", "people.person.profession"]], "ground_paths": [["sports.sports_award.award", "sports.sports_award.award_winner"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won fedex cup 2012 [/INST]", "raw_output": {"paths": ["<PATH> award.competition.winner <SEP> people.person.profession </PATH>", "<PATH> award.competitor.competitions <SEP> people.person.profession </PATH>"], "scores": [-0.05953044816851616, -0.09536014497280121], "norm_scores": [0.5089564323425293, 0.4910435080528259]}}
{"id": "WebQTest-1608", "question": "what language is mainly spoken in egypt", "prediction": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is mainly spoken in egypt [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.08495371043682098, -0.10152124613523483], "norm_scores": [0.5041418075561523, 0.49585822224617004]}}
{"id": "WebQTest-1609", "question": "who played alf on tv show", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_program_creator.programs_created"]], "ground_paths": [["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played alf on tv show [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.tv_program_creator.programs_created </PATH>"], "scores": [-0.05983520671725273, -0.06232042983174324], "norm_scores": [0.5006213188171387, 0.4993787109851837]}}
{"id": "WebQTest-1610", "question": "who was gerald ford vp", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was gerald ford vp [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>"], "scores": [-0.019032200798392296, -0.08202771097421646], "norm_scores": [0.5157436728477478, 0.4842563271522522]}}
{"id": "WebQTest-1611", "question": "what is armenian currency", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is armenian currency [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.03309997171163559, -0.11243128031492233], "norm_scores": [0.5198224186897278, 0.4801775813102722]}}
{"id": "WebQTest-1612", "question": "what type of art did raphael sanzio create", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_artist.associated_periods_or_movements"]], "ground_paths": [["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art did raphael sanzio create [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>"], "scores": [-0.019025973975658417, -0.06908522546291351], "norm_scores": [0.51251220703125, 0.48748779296875]}}
{"id": "WebQTest-1613", "question": "what language do they speak in malta", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they speak in malta [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.07843943685293198, -0.08735370635986328], "norm_scores": [0.5022285580635071, 0.49777141213417053]}}
{"id": "WebQTest-1614", "question": "what religion was king louis xiv", "prediction": [["people.person.religion"], ["royalty.monarch.kingdom", "people.person.religion"]], "ground_paths": [["people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religion was king louis xiv [/INST]", "raw_output": {"paths": ["<PATH> people.person.religion </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.religion </PATH>"], "scores": [-0.009183479472994804, -0.1669236421585083], "norm_scores": [0.5393534898757935, 0.46064651012420654]}}
{"id": "WebQTest-1615", "question": "what football team does mike wright play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what football team does mike wright play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.04004201665520668, -0.044554270803928375], "norm_scores": [0.501128077507019, 0.49887195229530334]}}
{"id": "WebQTest-1616", "question": "what did shannon hoon die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did shannon hoon die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.00820344127714634, -0.1445138156414032], "norm_scores": [0.5340248942375183, 0.4659750759601593]}}
{"id": "WebQTest-1617", "question": "what did nicholas copernicus discover", "prediction": [["symbols.namesake.named_after"], ["base.argumentmaps.original_idea.innovator"]], "ground_paths": [["base.argumentmaps.original_idea.innovator"], ["symbols.namesake.named_after"], ["base.argumentmaps.innovator.original_ideas"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did nicholas copernicus discover [/INST]", "raw_output": {"paths": ["<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.argumentmaps.original_idea.innovator </PATH>"], "scores": [-0.0701134130358696, -0.08809254318475723], "norm_scores": [0.5044946670532227, 0.49550533294677734]}}
{"id": "WebQTest-1618", "question": "what teams has manny ramirez played for", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team.roster"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_coach.teams_coached", "sports.sports_team.coaches"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what teams has manny ramirez played for [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.05010087788105011, -0.05895676091313362], "norm_scores": [0.5022139549255371, 0.4977860450744629]}}
{"id": "WebQTest-1619", "question": "where do the appalachian mountains run", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the appalachian mountains run [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.03583817929029465, -0.10059144347906113], "norm_scores": [0.5161826610565186, 0.48381733894348145]}}
{"id": "WebQTest-1620", "question": "when was president wilson in office", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["book.written_work.subjects", "base.culturalevent.event.entity_involved"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was president wilson in office [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> book.written_work.subjects <SEP> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.06688588857650757, -0.08082335442304611], "norm_scores": [0.503484308719635, 0.496515691280365]}}
{"id": "WebQTest-1621", "question": "what political party is jerry brown", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"]], "ground_paths": [["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party is jerry brown [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>"], "scores": [-0.04114585369825363, -0.049805764108896255], "norm_scores": [0.5021649599075317, 0.49783504009246826]}}
{"id": "WebQTest-1622", "question": "what ocean is around hawaii", "prediction": [["location.location.containedby", "location.location.containedby"], ["location.location.contains", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what ocean is around hawaii [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.contains <SEP> location.location.containedby </PATH>"], "scores": [-0.09678590297698975, -0.09951320290565491], "norm_scores": [0.5006818771362305, 0.4993181824684143]}}
{"id": "WebQTest-1623", "question": "what is the most common language spoken in argentina", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the most common language spoken in argentina [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.034110214561223984, -0.10485018789768219], "norm_scores": [0.5176776647567749, 0.4823223948478699]}}
{"id": "WebQTest-1624", "question": "what language tunisia", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language tunisia [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.058608993887901306, -0.07112845778465271], "norm_scores": [0.5031298398971558, 0.4968701899051666]}}
{"id": "WebQTest-1625", "question": "what are some inventions that leonardo da vinci invented", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"]], "ground_paths": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some inventions that leonardo da vinci invented [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>"], "scores": [-0.03554583340883255, -0.10120999068021774], "norm_scores": [0.5164101719856262, 0.48358985781669617]}}
{"id": "WebQTest-1626", "question": "who wrote st trinians", "prediction": [["fictional_universe.fictional_universe.locations"], ["film.film.written_by"]], "ground_paths": [["fictional_universe.fictional_universe.created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who wrote st trinians [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_universe.locations </PATH>", "<PATH> film.film.written_by </PATH>"], "scores": [-0.1083829402923584, -0.11160765588283539], "norm_scores": [0.5008061528205872, 0.49919381737709045]}}
{"id": "WebQTest-1627", "question": "what are malaysian coins called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are malaysian coins called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.034245818853378296, -0.12243365496397018], "norm_scores": [0.5220326781272888, 0.47796735167503357]}}
{"id": "WebQTest-1628", "question": "what type government does mexico have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type government does mexico have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.020330460742115974, -0.08831058442592621], "norm_scores": [0.5169885158538818, 0.48301151394844055]}}
{"id": "WebQTest-1629", "question": "what currency does dominican republic take", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does dominican republic take [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.0022890823893249035, -0.32342728972435], "norm_scores": [0.5796016454696655, 0.42039841413497925]}}
{"id": "WebQTest-1630", "question": "what inspired michael jackson to become a singer", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "ground_paths": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inspired michael jackson to become a singer [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.023268017917871475, -0.06844509392976761], "norm_scores": [0.5112923383712769, 0.48870766162872314]}}
{"id": "WebQTest-1631", "question": "who is the texas state senator", "prediction": [["location.administrative_division.country", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["location.administrative_division.country", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.political_district.elections", "government.election.winner"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.state", "symbols.namesake.named_after"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["location.us_state.capital", "people.deceased_person.place_of_death"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"], ["location.location.events", "military.military_person.participated_in_conflicts"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["government.political_district.representatives", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the texas state senator [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.country <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.05065622925758362, -0.053446948528289795], "norm_scores": [0.500697672367096, 0.49930232763290405]}}
{"id": "WebQTest-1632", "question": "what to visit in london city", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["metropolitan_transit.transit_system.area_served"], ["location.location.containedby"], ["organization.organization.geographic_scope"], ["travel.travel_destination.tourist_attractions"], ["transportation.bridge.locale"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to visit in london city [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>"], "scores": [-0.03953421115875244, -0.06776933372020721], "norm_scores": [0.5070582628250122, 0.492941677570343]}}
{"id": "WebQTest-1633", "question": "what genre of art is the mona lisa", "prediction": [["visual_art.artwork.art_form"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["visual_art.artwork.art_genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what genre of art is the mona lisa [/INST]", "raw_output": {"paths": ["<PATH> visual_art.artwork.art_form </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.08148082345724106, -0.1116984561085701], "norm_scores": [0.5075538158416748, 0.4924461543560028]}}
{"id": "WebQTest-1634", "question": "who does peter griffin voice", "prediction": [["fictional_universe.fictional_character.character_created_by"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["fictional_universe.fictional_character.character_created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does peter griffin voice [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.05340445041656494, -0.12695060670375824], "norm_scores": [0.5183782577514648, 0.48162174224853516]}}
{"id": "WebQTest-1635", "question": "when did tony romo got drafted", "prediction": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did tony romo got drafted [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>"], "scores": [-0.02867843024432659, -0.0354863703250885], "norm_scores": [0.5017019510269165, 0.4982980191707611]}}
{"id": "WebQTest-1637", "question": "what are the catholic holy days", "prediction": [["religion.religion.holidays"], ["time.holiday.featured_in_religions"]], "ground_paths": [["religion.religion.holidays"], ["time.holiday.featured_in_religions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the catholic holy days [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.holidays </PATH>", "<PATH> time.holiday.featured_in_religions </PATH>"], "scores": [-0.04522745683789253, -0.051629748195409775], "norm_scores": [0.5016005635261536, 0.49839943647384644]}}
{"id": "WebQTest-1638", "question": "what language is most commonly spoken in belgium", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is most commonly spoken in belgium [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.05155615508556366, -0.07370269298553467], "norm_scores": [0.5055363774299622, 0.49446359276771545]}}
{"id": "WebQTest-1639", "question": "where did frank baum die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people_deceased_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did frank baum die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>"], "scores": [-0.0011700651375576854, -0.31952032446861267], "norm_scores": [0.5789221525192261, 0.4210778474807739]}}
{"id": "WebQTest-1640", "question": "where is tunis on a map", "prediction": [["location.location.containedby"], ["base.locations.continents.countries_within"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_parent"], ["location.location.containedby"], ["location.administrative_division.country"], ["base.locations.countries.continent"], []], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is tunis on a map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.07025129348039627, -0.13302664458751678], "norm_scores": [0.5156887173652649, 0.4843113422393799]}}
{"id": "WebQTest-1641", "question": "who is nicolas cage married too", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["people.person.profession", "people.person.profession"], ["people.marriage.spouse", "people.marriage.spouse"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is nicolas cage married too [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.041964221745729446, -0.10681995004415512], "norm_scores": [0.5162082314491272, 0.4837917685508728]}}
{"id": "WebQTest-1643", "question": "who does the voice for chucky in child 's play", "prediction": [["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.film.starring", "film.actor.film"], ["film.performance.film", "film.actor.film"], ["film.performance.film", "film.performance.actor"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does the voice for chucky in child 's play [/INST]", "raw_output": {"paths": ["<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.11439913511276245, -0.11603784561157227], "norm_scores": [0.5004096627235413, 0.49959033727645874]}}
{"id": "WebQTest-1644", "question": "what did mary wollstonecraft fight for", "prediction": [["base.activism.activist.area_of_activism"], ["people.person.profession"]], "ground_paths": [["base.activism.activist.area_of_activism"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did mary wollstonecraft fight for [/INST]", "raw_output": {"paths": ["<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.07780014723539352, -0.10640807449817657], "norm_scores": [0.5071514844894409, 0.4928484857082367]}}
{"id": "WebQTest-1646", "question": "what year did the phillies begin", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the phillies begin [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.01583816111087799, -0.12323123961687088], "norm_scores": [0.5268224477767944, 0.4731774926185608]}}
{"id": "WebQTest-1648", "question": "what language does iceland speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does iceland speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.06339573860168457, -0.06345193833112717], "norm_scores": [0.500014066696167, 0.4999859631061554]}}
{"id": "WebQTest-1649", "question": "what was dr seuss real name", "prediction": [["book.book.interior_illustrations_by", "people.person.nationality"], ["book.book.interior_illustrations_by", "people.person.profession"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was dr seuss real name [/INST]", "raw_output": {"paths": ["<PATH> book.book.interior_illustrations_by <SEP> people.person.nationality </PATH>", "<PATH> book.book.interior_illustrations_by <SEP> people.person.profession </PATH>"], "scores": [-0.07736466079950333, -0.1362609565258026], "norm_scores": [0.5147197842597961, 0.48528018593788147]}}
{"id": "WebQTest-1650", "question": "what did gloria steinem founded", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did gloria steinem founded [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.026981867849826813, -0.08360663801431656], "norm_scores": [0.5141524076461792, 0.4858475923538208]}}
{"id": "WebQTest-1652", "question": "what language they speak in scotland", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language they speak in scotland [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.059359706938266754, -0.07580108195543289], "norm_scores": [0.5041102170944214, 0.49588972330093384]}}
{"id": "WebQTest-1653", "question": "what boarding school did mark zuckerberg go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what boarding school did mark zuckerberg go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.04527783393859863, -0.069505974650383], "norm_scores": [0.5060567259788513, 0.4939432740211487]}}
{"id": "WebQTest-1654", "question": "what are bigos", "prediction": [["food.dish.cuisine"], ["food.diet_and_soup.diet"]], "ground_paths": [["food.type_of_dish.dishes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are bigos [/INST]", "raw_output": {"paths": ["<PATH> food.dish.cuisine </PATH>", "<PATH> food.diet_and_soup.diet </PATH>"], "scores": [-0.13688820600509644, -0.2983521521091461], "norm_scores": [0.5402784943580627, 0.45972147583961487]}}
{"id": "WebQTest-1655", "question": "where are ike and tina turner 's children", "prediction": [["people.person.children"], ["people.person.parents"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are ike and tina turner 's children [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.05567162483930588, -0.12242453545331955], "norm_scores": [0.5166820287704468, 0.4833179712295532]}}
{"id": "WebQTest-1656", "question": "what are some of leonardo da vinci most famous inventions", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"]], "ground_paths": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some of leonardo da vinci most famous inventions [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>"], "scores": [-0.03444206714630127, -0.10426152497529984], "norm_scores": [0.5174477696418762, 0.4825522005558014]}}
{"id": "WebQTest-1657", "question": "when did shaq come into the nba", "prediction": [["sports.sports_award_winner.awards", "sports.sports_award.season"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"]], "ground_paths": [["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.draft"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did shaq come into the nba [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>"], "scores": [-0.04037173464894295, -0.05721037834882736], "norm_scores": [0.504209578037262, 0.49579042196273804]}}
{"id": "WebQTest-1658", "question": "where do they speak turkish", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do they speak turkish [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.050077978521585464, -0.0973191037774086], "norm_scores": [0.5118080973625183, 0.4881919026374817]}}
{"id": "WebQTest-1659", "question": "what is the zip code for walnut creek", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for walnut creek [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0003505180065985769, -0.5780695676803589], "norm_scores": [0.6405423879623413, 0.3594576120376587]}}
{"id": "WebQTest-1661", "question": "who was the voice of darth vader in the original star wars movie", "prediction": [["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the voice of darth vader in the original star wars movie [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.07124219089746475, -0.09597336500883102], "norm_scores": [0.5061824917793274, 0.4938175082206726]}}
{"id": "WebQTest-1662", "question": "where is english spoken around the world", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is english spoken around the world [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.046140898019075394, -0.11345864087343216], "norm_scores": [0.5168230533599854, 0.48317691683769226]}}
{"id": "WebQTest-1663", "question": "what music did louis armstrong play", "prediction": [["music.artist.genre"], ["music.genre.artists"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what music did louis armstrong play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>"], "scores": [-0.009170598350465298, -0.2615405321121216], "norm_scores": [0.5627597570419312, 0.4372403025627136]}}
{"id": "WebQTest-1664", "question": "what form of currency does germany use", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of currency does germany use [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.025948837399482727, -0.13264061510562897], "norm_scores": [0.526647686958313, 0.473352313041687]}}
{"id": "WebQTest-1665", "question": "where is the latin language from", "prediction": [["language.human_language.language_family"], ["base.rosetta.languoid.parent"]], "ground_paths": [["language.human_language.language_family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the latin language from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>"], "scores": [-0.008488547056913376, -0.2021132856607437], "norm_scores": [0.5482555627822876, 0.4517444968223572]}}
{"id": "WebQTest-1666", "question": "what are the top tourist attractions in rome", "prediction": [["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["base.arthist.int_zm_nyek.helynev"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the top tourist attractions in rome [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.06527131050825119, -0.09578139334917068], "norm_scores": [0.5076269507408142, 0.4923730790615082]}}
{"id": "WebQTest-1667", "question": "who were the children of king solomon", "prediction": [["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were the children of king solomon [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.07680138945579529, -0.08835754543542862], "norm_scores": [0.5028889775276184, 0.4971109926700592]}}
{"id": "WebQTest-1668", "question": "what did andy irons die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did andy irons die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.006667172536253929, -0.15928034484386444], "norm_scores": [0.5380794405937195, 0.4619205892086029]}}
{"id": "WebQTest-1669", "question": "where is auburn university located", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is auburn university located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.011954480782151222, -0.2786475419998169], "norm_scores": [0.5662809014320374, 0.43371912837028503]}}
{"id": "WebQTest-1670", "question": "who is tatyana ali parents", "prediction": [["people.person.children"], ["people.person.parents"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is tatyana ali parents [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.07405348122119904, -0.0901251882314682], "norm_scores": [0.5040178298950195, 0.49598217010498047]}}
{"id": "WebQTest-1671", "question": "where did leif ericson grow up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did leif ericson grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.007269677240401506, -0.22373001277446747], "norm_scores": [0.5539047718048096, 0.44609522819519043]}}
{"id": "WebQTest-1672", "question": "what language do the maasai speak", "prediction": [["people.ethnicity.languages_spoken"], ["location.country.languages_spoken"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do the maasai speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.01153483148664236, -0.22158436477184296], "norm_scores": [0.552320122718811, 0.4476798474788666]}}
{"id": "WebQTest-1673", "question": "what political party was hitler in", "prediction": [["base.activism.activist.area_of_activism", "government.political_party.ideology"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [["government.political_party_tenure.politician", "government.political_party.politicians_in_this_party"], ["base.activism.activist.area_of_activism", "government.political_party.ideology"], ["organization.organization_founder.organizations_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party was hitler in [/INST]", "raw_output": {"paths": ["<PATH> base.activism.activist.area_of_activism <SEP> government.political_party.ideology </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.043333832174539566, -0.07004984468221664], "norm_scores": [0.506678581237793, 0.49332138895988464]}}
{"id": "WebQTest-1674", "question": "who is the leader of north korea today", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the leader of north korea today [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.023423368111252785, -0.12867042422294617], "norm_scores": [0.5262874960899353, 0.4737124741077423]}}
{"id": "WebQTest-1675", "question": "where do the appalachian mountains run through", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "ground_paths": [["location.location.partially_contains"], ["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the appalachian mountains run through [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.0332646518945694, -0.10180992633104324], "norm_scores": [0.51712965965271, 0.4828703999519348]}}
{"id": "WebQTest-1676", "question": "what money does spain use", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money does spain use [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.02657610923051834, -0.1303309053182602], "norm_scores": [0.5259154438972473, 0.4740845263004303]}}
{"id": "WebQTest-1677", "question": "where was terry waite born", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was terry waite born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.04283883795142174, -0.07115422934293747], "norm_scores": [0.5070784091949463, 0.4929216504096985]}}
{"id": "WebQTest-1678", "question": "what country did the battle of san juan hill take place", "prediction": [["location.location.events"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did the battle of san juan hill take place [/INST]", "raw_output": {"paths": ["<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.043265633285045624, -0.10060373693704605], "norm_scores": [0.5143306255340576, 0.48566940426826477]}}
{"id": "WebQTest-1679", "question": "what kind of government does the united states have currently", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does the united states have currently [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.02764017879962921, -0.07040034979581833], "norm_scores": [0.5106884241104126, 0.4893116056919098]}}
{"id": "WebQTest-1680", "question": "when was the last time the orioles won the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the orioles won the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>"], "scores": [-0.034542664885520935, -0.05847107991576195], "norm_scores": [0.5059818029403687, 0.49401816725730896]}}
{"id": "WebQTest-1682", "question": "who did ben stiller date", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["people.person.parents", "people.person.children"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["people.marriage.spouse", "people.marriage.spouse"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["base.popstra.friendship.participant", "base.popstra.celebrity.friendship"], ["people.person.profession", "people.person.profession"], ["people.person.ethnicity", "people.person.ethnicity"], ["people.person.languages", "people.person.languages"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["people.person.parents", "people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did ben stiller date [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.03589898720383644, -0.10468897968530655], "norm_scores": [0.51719069480896, 0.48280927538871765]}}
{"id": "WebQTest-1683", "question": "what is the most common language in norway", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the most common language in norway [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.07375919818878174, -0.08254353702068329], "norm_scores": [0.5021960735321045, 0.4978039562702179]}}
{"id": "WebQTest-1684", "question": "what influenced william shakespeare to start writing", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what influenced william shakespeare to start writing [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.03355236351490021, -0.05041815713047981], "norm_scores": [0.5042163729667664, 0.49578362703323364]}}
{"id": "WebQTest-1685", "question": "where did benjamin franklin went to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did benjamin franklin went to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.041493404656648636, -0.07930111140012741], "norm_scores": [0.5094507932662964, 0.4905492067337036]}}
{"id": "WebQTest-1686", "question": "who is the governor of arizona 2009", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "ground_paths": [["location.country.first_level_divisions", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the governor of arizona 2009 [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.055085521191358566, -0.05605122074484825], "norm_scores": [0.5002413988113403, 0.4997585713863373]}}
{"id": "WebQTest-1687", "question": "what movies has spike lee made", "prediction": [["film.film.directed_by"], ["film.producer.film"]], "ground_paths": [["film.film.directed_by"], ["film.film.produced_by"], ["film.director.film"], ["film.film.executive_produced_by"], ["film.writer.film"], ["film.producer.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies has spike lee made [/INST]", "raw_output": {"paths": ["<PATH> film.film.directed_by </PATH>", "<PATH> film.producer.film </PATH>"], "scores": [-0.13430356979370117, -0.21971900761127472], "norm_scores": [0.5213408470153809, 0.47865912318229675]}}
{"id": "WebQTest-1688", "question": "what did bernie mac died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did bernie mac died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.005123632028698921, -0.17738981544971466], "norm_scores": [0.5429603457450867, 0.45703962445259094]}}
{"id": "WebQTest-1690", "question": "where is electrolux based", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.founders", "people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is electrolux based [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>"], "scores": [-0.013568972237408161, -0.0908132866024971], "norm_scores": [0.5193014740943909, 0.48069849610328674]}}
{"id": "WebQTest-1691", "question": "what is houston timezone", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is houston timezone [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-4.4016229367116466e-05, -0.9038082361221313], "norm_scores": [0.7117224335670471, 0.2882775366306305]}}
{"id": "WebQTest-1694", "question": "what was shakespeare 's wife called", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.parents"]], "ground_paths": [["people.place_of_interment.interred_here", "people.deceased_person.place_of_burial"], ["people.person.parents", "people.person.children"], ["freebase.valuenotation.has_value", "freebase.valuenotation.has_value"], ["people.person.place_of_birth", "people.deceased_person.place_of_death"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"], ["book.book_subject.works", "book.book_subject.works"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was shakespeare 's wife called [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>"], "scores": [-0.06337085366249084, -0.14225579798221588], "norm_scores": [0.5197110176086426, 0.4802889823913574]}}
{"id": "WebQTest-1695", "question": "when did truman become president", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did truman become president [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>"], "scores": [-0.025469373911619186, -0.08957996964454651], "norm_scores": [0.5160221457481384, 0.4839778244495392]}}
{"id": "WebQTest-1696", "question": "what did ferdinand magellan do", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did ferdinand magellan do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.00936838611960411, -0.3165638744831085], "norm_scores": [0.5762006044387817, 0.42379945516586304]}}
{"id": "WebQTest-1698", "question": "where do burmese cats originate from", "prediction": [["biology.animal_breed.place_of_origin"], ["biology.breed_origin.breeds_originating_here"]], "ground_paths": [["biology.breed_origin.breeds_originating_here"], ["biology.animal_breed.place_of_origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do burmese cats originate from [/INST]", "raw_output": {"paths": ["<PATH> biology.animal_breed.place_of_origin </PATH>", "<PATH> biology.breed_origin.breeds_originating_here </PATH>"], "scores": [-0.014172620140016079, -0.09251110255718231], "norm_scores": [0.5195746421813965, 0.4804254174232483]}}
{"id": "WebQTest-1699", "question": "who played kirk", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.quotations", "media_common.quotation.author"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played kirk [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.06377220898866653, -0.08409129828214645], "norm_scores": [0.5050796270370483, 0.49492040276527405]}}
{"id": "WebQTest-1700", "question": "where is mt st helens located", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mt st helens located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.042377013713121414, -0.13694950938224792], "norm_scores": [0.5236254930496216, 0.47637447714805603]}}
{"id": "WebQTest-1701", "question": "what county is st paul minnesota in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"]], "ground_paths": [["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st paul minnesota in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0578840933740139, -0.09882796555757523], "norm_scores": [0.510234534740448, 0.4897654950618744]}}
{"id": "WebQTest-1702", "question": "who was king henry viii son", "prediction": [["people.person.children"], ["people.person.parents"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was king henry viii son [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.08562520146369934, -0.09000863879919052], "norm_scores": [0.5010958313941956, 0.49890416860580444]}}
{"id": "WebQTest-1705", "question": "where was franz ferdinand from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was franz ferdinand from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.003993447870016098, -0.4037773013114929], "norm_scores": [0.598635733127594, 0.401364266872406]}}
{"id": "WebQTest-1706", "question": "what was benedict arnold most famous for", "prediction": [["base.honouriam.dishonoured_person.dishonor_bestowed"], ["base.honouriam.dishonoured_person.dishonor_bestowed", "base.honouri.dishonor.dishonor_bestowed"]], "ground_paths": [["base.honouriam.dishonoured_person.dishonor_bestowed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was benedict arnold most famous for [/INST]", "raw_output": {"paths": ["<PATH> base.honouriam.dishonoured_person.dishonor_bestowed </PATH>", "<PATH> base.honouriam.dishonoured_person.dishonor_bestowed <SEP> base.honouri.dishonor.dishonor_bestowed </PATH>"], "scores": [-0.06323884427547455, -0.12586279213428497], "norm_scores": [0.5156508684158325, 0.4843491315841675]}}
{"id": "WebQTest-1707", "question": "who plays lex luthor on smallville", "prediction": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"]], "ground_paths": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["tv.tv_program.languages", "people.person.languages"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays lex luthor on smallville [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.046343039721250534, -0.05097381770610809], "norm_scores": [0.5011577010154724, 0.4988422989845276]}}
{"id": "WebQTest-1708", "question": "what are dallas cowboys colors", "prediction": [["sports.sports_team.colors"], ["sports.sports_team.color"]], "ground_paths": [["sports.sports_team.colors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are dallas cowboys colors [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.colors </PATH>", "<PATH> sports.sports_team.color </PATH>"], "scores": [-0.00037992539000697434, -0.5490763187408447], "norm_scores": [0.633833110332489, 0.366166889667511]}}
{"id": "WebQTest-1710", "question": "what is the currency of the czech republic", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency of the czech republic [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.012059870176017284, -0.18313144147396088], "norm_scores": [0.5426638722419739, 0.45733609795570374]}}
{"id": "WebQTest-1711", "question": "when was the last time the la kings won a playoff series", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the la kings won a playoff series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.029960423707962036, -0.06657667458057404], "norm_scores": [0.5091530680656433, 0.4908469617366791]}}
{"id": "WebQTest-1712", "question": "where did spencer pratt go to school", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did spencer pratt go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.05453748255968094, -0.07844328880310059], "norm_scores": [0.505976140499115, 0.4940238296985626]}}
{"id": "WebQTest-1713", "question": "what disease did abraham lincoln had", "prediction": [["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.notable_people_with_this_condition"]], "ground_paths": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what disease did abraham lincoln had [/INST]", "raw_output": {"paths": ["<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>"], "scores": [-0.009464861825108528, -0.10950834304094315], "norm_scores": [0.5249900221824646, 0.4750099778175354]}}
{"id": "WebQTest-1714", "question": "who is vancouver canucks coach", "prediction": [["sports.sports_team.coaches", "sports.sports_team_coach_tenure.coach"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"]], "ground_paths": [["ice_hockey.hockey_coach.current_team"], ["sports.sports_team.coaches", "sports.sports_team_coach.teams_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is vancouver canucks coach [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>"], "scores": [-0.029515469446778297, -0.04723769798874855], "norm_scores": [0.5044304132461548, 0.4955695569515228]}}
{"id": "WebQTest-1715", "question": "where did cs lewis wrote", "prediction": [["book.written_work.author"], ["book.author.works_written"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did cs lewis wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.10819866508245468, -0.14468927681446075], "norm_scores": [0.5091216564178467, 0.4908783733844757]}}
{"id": "WebQTest-1716", "question": "what is carlos pena jr nationality", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is carlos pena jr nationality [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.006083539687097073, -0.2930295169353485], "norm_scores": [0.5712483525276184, 0.42875170707702637]}}
{"id": "WebQTest-1719", "question": "who was married to jane wyman", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was married to jane wyman [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.0487162210047245, -0.09644991159439087], "norm_scores": [0.5119311213493347, 0.4880688488483429]}}
{"id": "WebQTest-1721", "question": "what language does peru speak", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does peru speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.04625638946890831, -0.13853174448013306], "norm_scores": [0.5230525135993958, 0.476947546005249]}}
{"id": "WebQTest-1723", "question": "where is english spoken in the world map", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is english spoken in the world map [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.047506220638751984, -0.10981576889753342], "norm_scores": [0.5155723690986633, 0.48442766070365906]}}
{"id": "WebQTest-1724", "question": "when did arsenal won the league", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "ground_paths": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did arsenal won the league [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>"], "scores": [-0.03797683119773865, -0.05565434694290161], "norm_scores": [0.5044192671775818, 0.4955807328224182]}}
{"id": "WebQTest-1725", "question": "what timezone is phoenix az in right now", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is phoenix az in right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-4.661620914703235e-05, -0.8867654204368591], "norm_scores": [0.7082125544548035, 0.29178741574287415]}}
{"id": "WebQTest-1726", "question": "where is south portland", "prediction": [["location.location.containedby"], ["location.us_county.hud_county_place"]], "ground_paths": [["location.hud_county_place.county"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is south portland [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.hud_county_place </PATH>"], "scores": [-0.03664866089820862, -0.13153113424777985], "norm_scores": [0.5237028002738953, 0.47629716992378235]}}
{"id": "WebQTest-1727", "question": "what countries speak french as a first language", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["base.rosetta.local_name.locale"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries speak french as a first language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.0576544888317585, -0.11144181340932846], "norm_scores": [0.5134435296058655, 0.48655641078948975]}}
{"id": "WebQTest-1728", "question": "who was ptolemy and what did he do", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was ptolemy and what did he do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.010545449331402779, -0.26037585735321045], "norm_scores": [0.5621348023414612, 0.4378652572631836]}}
{"id": "WebQTest-1729", "question": "what was the name of henry viii first wife", "prediction": [["people.person.children", "people.person.children"], ["people.person.parents", "people.person.parents"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.religion", "people.person.religion"], ["book.written_work.subjects", "theater.play.characters"], ["people.family_member.family", "people.family.members"], ["people.person.parents", "people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the name of henry viii first wife [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>"], "scores": [-0.14419031143188477, -0.14740732312202454], "norm_scores": [0.5008042454719543, 0.49919575452804565]}}
{"id": "WebQTest-1730", "question": "where does the parana river flow", "prediction": [["geography.river.basin_countries"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the parana river flow [/INST]", "raw_output": {"paths": ["<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.09533102810382843, -0.10020743310451508], "norm_scores": [0.5012190937995911, 0.49878090620040894]}}
{"id": "WebQTest-1731", "question": "what is president nixon 's first name", "prediction": [["people.person.nationality", "people.person.nationality"], ["common.topic.notable_types", "common.topic.notable_types"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is president nixon 's first name [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>"], "scores": [-0.07933327555656433, -0.11064739525318146], "norm_scores": [0.507827877998352, 0.49217212200164795]}}
{"id": "WebQTest-1732", "question": "where did theodor schwann get his education", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.deceased_person.place_of_death", "location.location.contains"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did theodor schwann get his education [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.047086622565984726, -0.06894037127494812], "norm_scores": [0.5054632425308228, 0.49453678727149963]}}
{"id": "WebQTest-1733", "question": "who or what influenced frank lloyd wright", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who or what influenced frank lloyd wright [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.034258946776390076, -0.05067828670144081], "norm_scores": [0.504104733467102, 0.49589526653289795]}}
{"id": "WebQTest-1734", "question": "what year did houston rockets win their first championship", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did houston rockets win their first championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.029932290315628052, -0.06337691098451614], "norm_scores": [0.5083603858947754, 0.4916395843029022]}}
{"id": "WebQTest-1735", "question": "where do most of the people live in russia", "prediction": [["location.administrative_division.country"], ["location.location.containedby"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do most of the people live in russia [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.country </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.1160852462053299, -0.11811840534210205], "norm_scores": [0.5005083084106445, 0.49949172139167786]}}
{"id": "WebQTest-1736", "question": "where is holy roman empire located", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is holy roman empire located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.10608381032943726, -0.11229444295167923], "norm_scores": [0.5015527009963989, 0.49844735860824585]}}
{"id": "WebQTest-1737", "question": "what movies has liam hemsworth played in", "prediction": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["film.actor.film", "film.film.starring"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["people.person.place_of_birth", "film.film.featured_film_locations"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_winner.awards_won", "award.award_honor.honored_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies has liam hemsworth played in [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.05885452777147293, -0.09487006813287735], "norm_scores": [0.5090029239654541, 0.4909970760345459]}}
{"id": "WebQTest-1738", "question": "who plays nicholas newman on the young and the restless", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays nicholas newman on the young and the restless [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.06800517439842224, -0.07850208133459091], "norm_scores": [0.5026242136955261, 0.4973757863044739]}}
{"id": "WebQTest-1739", "question": "who is the voice of eric cartman on south park", "prediction": [["film.performance.special_performance_type", "film.performance.actor"], ["film.performance.special_performance_type", "film.actor.film"]], "ground_paths": [["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the voice of eric cartman on south park [/INST]", "raw_output": {"paths": ["<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.actor.film </PATH>"], "scores": [-0.057266972959041595, -0.092181496322155], "norm_scores": [0.5087277293205261, 0.4912722706794739]}}
{"id": "WebQTest-1740", "question": "who is angelina jolies brother", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.nationality", "people.person.place_of_birth"]], "ground_paths": [["film.producer.films_executive_produced", "film.producer.films_executive_produced"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["people.person.sibling_s", "people.sibling_relationship.sibling"], ["people.person.parents", "people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is angelina jolies brother [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.nationality <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.05312426760792732, -0.10290789604187012], "norm_scores": [0.5124433040618896, 0.4875566363334656]}}
{"id": "WebQTest-1741", "question": "where is the headquarters of bank of america", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"]], "ground_paths": [["common.image.appears_in_topic_gallery", "location.location.containedby"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the headquarters of bank of america [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>"], "scores": [-0.015595396049320698, -0.09874504059553146], "norm_scores": [0.5207754373550415, 0.4792245626449585]}}
{"id": "WebQTest-1742", "question": "what type of government japan have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government japan have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>"], "scores": [-0.04129275679588318, -0.04870915785431862], "norm_scores": [0.5018540620803833, 0.4981459081172943]}}
{"id": "WebQTest-1744", "question": "what team does josh hamilton play for", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does josh hamilton play for [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>"], "scores": [-0.03702542930841446, -0.04680084064602852], "norm_scores": [0.502443790435791, 0.4975561499595642]}}
{"id": "WebQTest-1746", "question": "where is burundi country", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is burundi country [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.016223888844251633, -0.3113098740577698], "norm_scores": [0.5732408165931702, 0.4267592132091522]}}
{"id": "WebQTest-1748", "question": "through which countries of the sahel does the niger river flow", "prediction": [["location.location.partially_containedby"], ["geography.river.basin_countries"]], "ground_paths": [["geography.river.basin_countries"], ["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: through which countries of the sahel does the niger river flow [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>"], "scores": [-0.07896817475557327, -0.08506699651479721], "norm_scores": [0.5015246868133545, 0.4984752833843231]}}
{"id": "WebQTest-1750", "question": "what city does paul ryan live in", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city does paul ryan live in [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.06791043281555176, -0.08776749670505524], "norm_scores": [0.5049641132354736, 0.49503591656684875]}}
{"id": "WebQTest-1751", "question": "what city is acadia university in", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city is acadia university in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.03560411185026169, -0.27383506298065186], "norm_scores": [0.5592776536941528, 0.44072237610816956]}}
{"id": "WebQTest-1752", "question": "what did f. scott fitzgerald do", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did f. scott fitzgerald do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.003543433966115117, -0.40440142154693604], "norm_scores": [0.5988937616348267, 0.40110620856285095]}}
{"id": "WebQTest-1753", "question": "what countries does tomtom western europe include", "prediction": [["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"], ["common.topic.notable_types"]], "ground_paths": [["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"], ["organization.organization.phone_number", "common.phone_number.service_location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does tomtom western europe include [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.organization_extra.phone_number <SEP> base.schemastaging.phone_sandbox.service_location </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.020357493311166763, -0.10474810749292374], "norm_scores": [0.5210851430892944, 0.47891485691070557]}}
{"id": "WebQTest-1754", "question": "what is the national language of iran", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the national language of iran [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.06396158784627914, -0.10565509647130966], "norm_scores": [0.510421872138977, 0.48957812786102295]}}
{"id": "WebQTest-1755", "question": "where did the second battle of ypres occur", "prediction": [["time.event.locations"], ["location.location.events"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the second battle of ypres occur [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.054588790982961655, -0.12773366272449493], "norm_scores": [0.5182780623435974, 0.4817219078540802]}}
{"id": "WebQTest-1756", "question": "what languages do people speak in spain", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what languages do people speak in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.06294208019971848, -0.0863124430179596], "norm_scores": [0.5058423280715942, 0.49415770173072815]}}
{"id": "WebQTest-1757", "question": "what did michael crabtree do", "prediction": [["american_football.football_player.games", "american_football.football_player_game_statistics.position_s"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.position"]], "ground_paths": [["common.topic.notable_types"], ["people.person.profession"], ["american_football.football_player.position_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did michael crabtree do [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.games <SEP> american_football.football_player_game_statistics.position_s </PATH>", "<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.position </PATH>"], "scores": [-0.06875614821910858, -0.0741623193025589], "norm_scores": [0.501351535320282, 0.498648464679718]}}
{"id": "WebQTest-1758", "question": "where do they speak afrikaans", "prediction": [["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do they speak afrikaans [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.020486807450652122, -0.2005280703306198], "norm_scores": [0.5448890924453735, 0.4551108777523041]}}
{"id": "WebQTest-1759", "question": "what 3 states border florida", "prediction": [["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.tropical_cyclone.affected_areas"]], "ground_paths": [["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.contains", "location.location.contains"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.containedby", "location.location.contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_contains", "location.location.partially_contains"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.tropical_cyclone.affected_areas"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.events", "time.event.locations"], ["location.administrative_division.first_level_division_of", "location.administrative_division.first_level_division_of"], ["location.location.time_zones", "location.location.time_zones"], ["location.administrative_division.first_level_division_of", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_contains", "location.location.partially_containedby"], ["location.location.partially_containedby", "location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what 3 states border florida [/INST]", "raw_output": {"paths": ["<PATH> meteorology.tropical_cyclone.affected_areas <SEP> meteorology.cyclone_affected_area.cyclones </PATH>", "<PATH> meteorology.tropical_cyclone.affected_areas <SEP> meteorology.tropical_cyclone.affected_areas </PATH>"], "scores": [-0.058034081012010574, -0.09244050085544586], "norm_scores": [0.5086007714271545, 0.49139925837516785]}}
{"id": "WebQTest-1760", "question": "who plays ziva david on ncis", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.tv_guest_role.character", "tv.tv_guest_role.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_guest_role.character", "tv.tv_actor.guest_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays ziva david on ncis [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.06286560744047165, -0.08344712108373642], "norm_scores": [0.5051451921463013, 0.4948548376560211]}}
{"id": "WebQTest-1761", "question": "who ruled after king henry viii died", "prediction": [["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who ruled after king henry viii died [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.12048336863517761, -0.23878774046897888], "norm_scores": [0.5295416712760925, 0.47045835852622986]}}
{"id": "WebQTest-1763", "question": "who are the st louis cardinals coaches", "prediction": [["baseball.baseball_team.current_coaches"], ["baseball.baseball_team.current_coaches", "baseball.current_coaching_tenure.baseball_coach"]], "ground_paths": [["baseball.baseball_team.current_coaches", "baseball.current_coaching_tenure.baseball_coach"], ["baseball.current_coaching_tenure.baseball_team", "baseball.baseball_coach.current_team_coaching"], ["baseball.baseball_team.current_coaches", "baseball.baseball_coach.current_team_coaching"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the st louis cardinals coaches [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_team.current_coaches </PATH>", "<PATH> baseball.baseball_team.current_coaches <SEP> baseball.current_coaching_tenure.baseball_coach </PATH>"], "scores": [-0.064643993973732, -0.09801112115383148], "norm_scores": [0.5083410143852234, 0.491659015417099]}}
{"id": "WebQTest-1764", "question": "where is daud ibrahim", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is daud ibrahim [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.06662464886903763, -0.11383144557476044], "norm_scores": [0.5117995142936707, 0.48820051550865173]}}
{"id": "WebQTest-1765", "question": "what region is turkey considered", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["base.locations.countries.continent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what region is turkey considered [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.06269914656877518, -0.16654333472251892], "norm_scores": [0.5259377360343933, 0.4740622341632843]}}
{"id": "WebQTest-1766", "question": "what language do russians speak", "prediction": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["people.ethnicity.languages_spoken"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["people.ethnicity.geographic_distribution", "location.country.languages_spoken"], ["people.ethnicity.geographic_distribution", "language.human_language.countries_spoken_in"], ["people.ethnicity.geographic_distribution", "location.country.official_language"], ["language.human_language.main_country"], ["people.ethnicity.included_in_group", "people.ethnicity.languages_spoken"], ["people.ethnicity.geographic_distribution", "language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do russians speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.052591972053050995, -0.0883505567908287], "norm_scores": [0.5089386701583862, 0.4910613000392914]}}
{"id": "WebQTest-1767", "question": "where was martin luther king jr buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"]], "ground_paths": [["people.deceased_person.place_of_burial"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was martin luther king jr buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>"], "scores": [-0.01152449008077383, -0.11860113590955734], "norm_scores": [0.5267435908317566, 0.47325634956359863]}}
{"id": "WebQTest-1768", "question": "what are the official languages of argentina", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the official languages of argentina [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.03677899017930031, -0.09134764224290848], "norm_scores": [0.5136387944221497, 0.48636123538017273]}}
{"id": "WebQTest-1769", "question": "where is the city wales", "prediction": [["base.biblioness.bibs_location.country"], ["location.location.containedby"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_children"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the city wales [/INST]", "raw_output": {"paths": ["<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.07394496351480484, -0.09199244529008865], "norm_scores": [0.5045117735862732, 0.4954882562160492]}}
{"id": "WebQTest-1770", "question": "what country does rafael nadal play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["people.person.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country does rafael nadal play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.055041663348674774, -0.09438996016979218], "norm_scores": [0.5098358392715454, 0.49016422033309937]}}
{"id": "WebQTest-1771", "question": "what is the islamic belief", "prediction": [["religion.belief.belief_of"], ["religion.religion.beliefs"]], "ground_paths": [["religion.religion.beliefs"], ["religion.belief.belief_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the islamic belief [/INST]", "raw_output": {"paths": ["<PATH> religion.belief.belief_of </PATH>", "<PATH> religion.religion.beliefs </PATH>"], "scores": [-0.032687362283468246, -0.0740031749010086], "norm_scores": [0.5103274583816528, 0.4896724820137024]}}
{"id": "WebQTest-1772", "question": "who is the prime minister of spain 2011", "prediction": [["base.onephylogeny.type_of_thing.things_of_this_type", "people.person.profession"], ["people.person.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the prime minister of spain 2011 [/INST]", "raw_output": {"paths": ["<PATH> base.onephylogeny.type_of_thing.things_of_this_type <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.048244740813970566, -0.050988949835300446], "norm_scores": [0.5006860494613647, 0.49931392073631287]}}
{"id": "WebQTest-1773", "question": "who was the son of king leonidas", "prediction": [["people.person.gender"], ["people.person.children"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the son of king leonidas [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.05776086077094078, -0.19232092797756195], "norm_scores": [0.5335893034934998, 0.46641066670417786]}}
{"id": "WebQTest-1774", "question": "who was judi dench married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["people.person.parents", "people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.profession", "people.person.profession"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was judi dench married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.05062653869390488, -0.06652183830738068], "norm_scores": [0.5039737224578857, 0.49602627754211426]}}
{"id": "WebQTest-1775", "question": "how many languages do they speak in spain", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how many languages do they speak in spain [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.0647478774189949, -0.07591339945793152], "norm_scores": [0.5027913451194763, 0.4972086250782013]}}
{"id": "WebQTest-1776", "question": "where are the sahara desert located", "prediction": [["location.location.containedby"], ["location.location.partially_containedby"]], "ground_paths": [["book.written_work.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the sahara desert located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.012473154813051224, -0.22236357629299164], "norm_scores": [0.552280843257904, 0.44771918654441833]}}
{"id": "WebQTest-1777", "question": "what is rihanna mum called", "prediction": [["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is rihanna mum called [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.07084223628044128, -0.09533083438873291], "norm_scores": [0.506121814250946, 0.49387815594673157]}}
{"id": "WebQTest-1779", "question": "when did president theodore roosevelt take office", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.office_holder"], ["government.government_position_held.office_holder", "government.government_position_held.basic_title"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did president theodore roosevelt take office [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.basic_title </PATH>"], "scores": [-0.06276854872703552, -0.10315529257059097], "norm_scores": [0.5100952982902527, 0.4899046719074249]}}
{"id": "WebQTest-1780", "question": "who does michael oher play for in 2010", "prediction": [["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"]], "ground_paths": [["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does michael oher play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>"], "scores": [-0.04468808323144913, -0.05130516737699509], "norm_scores": [0.501654326915741, 0.4983457326889038]}}
{"id": "WebQTest-1781", "question": "who or what influenced albert einstein", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "ground_paths": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who or what influenced albert einstein [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.04108818620443344, -0.04134787619113922], "norm_scores": [0.5000649094581604, 0.4999350905418396]}}
{"id": "WebQTest-1782", "question": "where does egypt export to", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"]], "ground_paths": [["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.location.partially_contains", "location.location.containedby"], ["location.location.events", "location.location.events"], ["location.location.partially_contains", "location.location.partially_containedby"], ["language.human_language.countries_spoken_in", "location.country.official_language"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["location.location.events", "time.event.locations"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.contains_major_portion_of", "base.locations.countries.continent"], ["base.schemastaging.disputed_location.claimed_by", "base.schemastaging.disputed_location_claimant.disputed_territories"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.contains", "location.location.contains"], ["location.statistical_region.places_exported_to", "location.imports_and_exports.exported_to"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does egypt export to [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>"], "scores": [-0.056853167712688446, -0.06038792058825493], "norm_scores": [0.5008836984634399, 0.49911633133888245]}}
{"id": "WebQTest-1784", "question": "where is the seat of the african union", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization_founder.organizations_founded", "location.location.containedby"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the seat of the african union [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.location.containedby </PATH>"], "scores": [-0.030383238568902016, -0.10747385025024414], "norm_scores": [0.5192631483078003, 0.4807368814945221]}}
{"id": "WebQTest-1785", "question": "what was kim richards in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["film.actor.film", "film.film.starring"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was kim richards in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.06285525858402252, -0.13399337232112885], "norm_scores": [0.5177770256996155, 0.48222294449806213]}}
{"id": "WebQTest-1788", "question": "what artistic style did henri rousseau work in", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what artistic style did henri rousseau work in [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.006319417152553797, -0.13557887077331543], "norm_scores": [0.5322698950767517, 0.4677300453186035]}}
{"id": "WebQTest-1789", "question": "what is john edwards indicted for", "prediction": [["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"]], "ground_paths": [["base.crime.acquitted_person.acquittal", "base.crime.acquittal.crime_type"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is john edwards indicted for [/INST]", "raw_output": {"paths": ["<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>"], "scores": [-0.05342578887939453, -0.11013629287481308], "norm_scores": [0.5141738057136536, 0.48582619428634644]}}
{"id": "WebQTest-1790", "question": "where was huey newton killed", "prediction": [["people.deceased_person.place_of_death"], ["people.person.place_of_birth"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was huey newton killed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.0012290820013731718, -0.32502371072769165], "norm_scores": [0.5802487730979919, 0.41975125670433044]}}
{"id": "WebQTest-1791", "question": "when was the printing press invented by gutenberg", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the printing press invented by gutenberg [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>"], "scores": [-0.07025015354156494, -0.13622035086154938], "norm_scores": [0.5164865851402283, 0.4835134446620941]}}
{"id": "WebQTest-1792", "question": "where is midlothian scotland", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.administrative_division.second_level_division_of"], ["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is midlothian scotland [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.01452172826975584, -0.30917561054229736], "norm_scores": [0.5731350779533386, 0.4268649220466614]}}
{"id": "WebQTest-1793", "question": "where does marta play soccer", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does marta play soccer [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.08584041148424149, -0.10471823066473007], "norm_scores": [0.5047193169593811, 0.4952806830406189]}}
{"id": "WebQTest-1794", "question": "where is mitt romney 's family from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mitt romney 's family from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.05770615488290787, -0.12504646182060242], "norm_scores": [0.516828715801239, 0.483171284198761]}}
{"id": "WebQTest-1795", "question": "what are john terry 's children called", "prediction": [["people.person.children"], ["people.person.parents"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are john terry 's children called [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.05984705314040184, -0.10880178213119507], "norm_scores": [0.5122362375259399, 0.48776376247406006]}}
{"id": "WebQTest-1796", "question": "what city was michael jackson born in", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city was michael jackson born in [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.053535494953393936, -0.08702471852302551], "norm_scores": [0.5083715319633484, 0.4916284680366516]}}
{"id": "WebQTest-1797", "question": "who fought in the battle of vicksburg", "prediction": [["time.event.locations"], ["military.military_conflict.military_personnel_involved"]], "ground_paths": [["base.culturalevent.event.entity_involved"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who fought in the battle of vicksburg [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>"], "scores": [-0.09588966518640518, -0.12583483755588531], "norm_scores": [0.5074857473373413, 0.4925142526626587]}}
{"id": "WebQTest-1799", "question": "what kind of writing is nikolai gogol famous for", "prediction": [["book.written_work.author"], ["book.author.works_written"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of writing is nikolai gogol famous for [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.09065133333206177, -0.13952597975730896], "norm_scores": [0.5122162103652954, 0.4877837598323822]}}
{"id": "WebQTest-1800", "question": "what year was first world series", "prediction": [["time.event.instance_of_recurring_event"], ["sports.sports_championship_event.champion"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was first world series [/INST]", "raw_output": {"paths": ["<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.04188808053731918, -0.0602412223815918], "norm_scores": [0.5045881867408752, 0.49541184306144714]}}
{"id": "WebQTest-1801", "question": "who plays simon birch", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["award.award_honor.honored_for", "award.award_winner.awards_won"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays simon birch [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.05878477543592453, -0.07922890037298203], "norm_scores": [0.5051108598709106, 0.49488914012908936]}}
{"id": "WebQTest-1802", "question": "where are the timezones in the usa", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the timezones in the usa [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>"], "scores": [-0.00012513344699982554, -0.40388303995132446], "norm_scores": [0.5995901823043823, 0.4004098176956177]}}
{"id": "WebQTest-1803", "question": "what does gm make", "prediction": [["automotive.company.make_s"], ["automotive.make.parent_company"]], "ground_paths": [["business.business_operation.industry"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does gm make [/INST]", "raw_output": {"paths": ["<PATH> automotive.company.make_s </PATH>", "<PATH> automotive.make.parent_company </PATH>"], "scores": [-0.05250218138098717, -0.07710971683263779], "norm_scores": [0.506151556968689, 0.49384841322898865]}}
{"id": "WebQTest-1804", "question": "what do people speak in uk", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people speak in uk [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.056899379938840866, -0.0795525386929512], "norm_scores": [0.5056630373001099, 0.49433696269989014]}}
{"id": "WebQTest-1805", "question": "what do people in jamaica speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people in jamaica speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.050177205353975296, -0.07011745125055313], "norm_scores": [0.5049849152565002, 0.49501511454582214]}}
{"id": "WebQTest-1806", "question": "what position did lebron james play", "prediction": [["basketball.basketball_player.position_s"], ["sports.sports_position.sport", "sports.sports_team.sport"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.position"], ["sports.sports_team_roster.player", "sports.sports_team_roster.position"], ["basketball.basketball_player.position_s"], ["basketball.basketball_position.players"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what position did lebron james play [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.position_s </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>"], "scores": [-0.03353558108210564, -0.12941493093967438], "norm_scores": [0.5239514708518982, 0.4760484993457794]}}
{"id": "WebQTest-1807", "question": "what was wayne gretzky 's first team", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was wayne gretzky 's first team [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.022413400933146477, -0.058457065373659134], "norm_scores": [0.5090099573135376, 0.4909900724887848]}}
{"id": "WebQTest-1808", "question": "who is the president of peru now", "prediction": [["people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the president of peru now [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.020763512700796127, -0.07415485382080078], "norm_scores": [0.5133446455001831, 0.4866553246974945]}}
{"id": "WebQTest-1809", "question": "what kind of guitar did jerry garcia play", "prediction": [["music.group_member.instruments_played"], ["music.guitarist.guitars_played"]], "ground_paths": [["music.guitarist.guitars_played"], ["music.guitar.guitarists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of guitar did jerry garcia play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.guitarist.guitars_played </PATH>"], "scores": [-0.05171533301472664, -0.10542646050453186], "norm_scores": [0.5134245753288269, 0.4865754544734955]}}
{"id": "WebQTest-1810", "question": "what type of art leonardo da vinci do", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art leonardo da vinci do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.0008920522523112595, -0.300142377614975], "norm_scores": [0.5742592215538025, 0.4257407486438751]}}
{"id": "WebQTest-1811", "question": "who are the senators of new jersey now", "prediction": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["location.administrative_division.first_level_division_of", "people.person.nationality"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["location.location.containedby", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the senators of new jersey now [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> location.administrative_division.first_level_division_of <SEP> people.person.nationality </PATH>"], "scores": [-0.03892609849572182, -0.046675894409418106], "norm_scores": [0.5019374489784241, 0.4980625510215759]}}
{"id": "WebQTest-1812", "question": "where was rihanna raised", "prediction": [["location.location.people_born_here"], ["music.artist.origin"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was rihanna raised [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.08492036908864975, -0.11459095031023026], "norm_scores": [0.5074170827865601, 0.49258288741111755]}}
{"id": "WebQTest-1813", "question": "what team does messi play for 2011", "prediction": [["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"]], "ground_paths": [["sports.sports_award.award_winner", "sports.sports_award.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does messi play for 2011 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>"], "scores": [-0.06812167912721634, -0.11606332659721375], "norm_scores": [0.5119830965995789, 0.48801687359809875]}}
{"id": "WebQTest-1815", "question": "where does lil wayne go to school", "prediction": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does lil wayne go to school [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.04723271727561951, -0.04930322989821434], "norm_scores": [0.5005176067352295, 0.4994823634624481]}}
{"id": "WebQTest-1817", "question": "where did jrr tolkien go to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["people.person.employment_history", "business.employment_tenure.company"], ["education.education.student", "education.education.institution"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did jrr tolkien go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.03661157935857773, -0.06791022419929504], "norm_scores": [0.5078240036964417, 0.49217596650123596]}}
{"id": "WebQTest-1818", "question": "what state is saint louis university in", "prediction": [["location.location.containedby"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state is saint louis university in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>"], "scores": [-0.032012395560741425, -0.07564947009086609], "norm_scores": [0.5109075307846069, 0.4890924394130707]}}
{"id": "WebQTest-1819", "question": "what religion does canada follow", "prediction": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"]], "ground_paths": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["book.newspaper.circulation_areas", "book.periodical_subject.periodicals"], ["royalty.monarch.kingdom", "people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religion does canada follow [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>"], "scores": [-0.03191036358475685, -0.0700908675789833], "norm_scores": [0.5095439553260803, 0.49045607447624207]}}
{"id": "WebQTest-1821", "question": "which countries have spanish as their main language", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries have spanish as their main language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.04311342537403107, -0.12675800919532776], "norm_scores": [0.5208989381790161, 0.4791010320186615]}}
{"id": "WebQTest-1822", "question": "who is in paul ryan 's family", "prediction": [["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.parents", "people.person.children"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.parents"], ["people.person.parents"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is in paul ryan 's family [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.11903098225593567, -0.16530334949493408], "norm_scores": [0.5115660429000854, 0.48843398690223694]}}
{"id": "WebQTest-1823", "question": "what government does iraq use", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what government does iraq use [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.020322071388363838, -0.08830229192972183], "norm_scores": [0.5169885158538818, 0.4830114543437958]}}
{"id": "WebQTest-1824", "question": "what type of currency do they use in spain", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of currency do they use in spain [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.01641104556620121, -0.1728234589099884], "norm_scores": [0.5390235781669617, 0.46097642183303833]}}
{"id": "WebQTest-1825", "question": "what did lincoln steffens wrote", "prediction": [["book.author.works_written"], ["book.written_work.author"]], "ground_paths": [["book.written_work.author"], ["book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did lincoln steffens wrote [/INST]", "raw_output": {"paths": ["<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.06612718105316162, -0.0833585187792778], "norm_scores": [0.5043076872825623, 0.49569225311279297]}}
{"id": "WebQTest-1826", "question": "what schools did anders celsius attend", "prediction": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.place_of_birth", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what schools did anders celsius attend [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.055249664932489395, -0.056593719869852066], "norm_scores": [0.5003359913825989, 0.49966397881507874]}}
{"id": "WebQTest-1827", "question": "what can do in new york", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "ground_paths": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.neighborhood.neighborhood_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what can do in new york [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>"], "scores": [-0.04359618201851845, -0.05469844490289688], "norm_scores": [0.5027755498886108, 0.49722447991371155]}}
{"id": "WebQTest-1828", "question": "what currency does france accept", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does france accept [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.036284539848566055, -0.10729774087667465], "norm_scores": [0.517745852470398, 0.48225417733192444]}}
{"id": "WebQTest-1829", "question": "what type of religion did massachusetts have", "prediction": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"]], "ground_paths": [["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of religion did massachusetts have [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>"], "scores": [-0.008795990608632565, -0.12840980291366577], "norm_scores": [0.5298678278923035, 0.47013214230537415]}}
{"id": "WebQTest-1830", "question": "what language do they speak in colombia south america", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they speak in colombia south america [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.039539825171232224, -0.07276158034801483], "norm_scores": [0.5083046555519104, 0.4916953146457672]}}
{"id": "WebQTest-1833", "question": "what kind of government does brazil has", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does brazil has [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.012936737388372421, -0.11616071313619614], "norm_scores": [0.525783121585846, 0.47421690821647644]}}
{"id": "WebQTest-1835", "question": "what country did gregor mendel live in", "prediction": [["people.deceased_person.place_of_death"], ["people.person.nationality"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did gregor mendel live in [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.05697178840637207, -0.08806859701871872], "norm_scores": [0.5077735781669617, 0.49222642183303833]}}
{"id": "WebQTest-1836", "question": "where laura ingalls wilder lived", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_death"]], "ground_paths": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.administrative_division.first_level_division_of"], ["influence.influence_node.influenced", "location.location.people_born_here"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.place_of_birth"], ["people.deceased_person.place_of_death"], ["people.person.nationality", "location.location.containedby"], ["book.book_edition.author_editor", "book.written_work.subjects"], ["people.person.parents", "people.deceased_person.place_of_death"], ["book.author.works_written", "fictional_universe.work_of_fiction.setting"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where laura ingalls wilder lived [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>"], "scores": [-0.08378711342811584, -0.1039087176322937], "norm_scores": [0.5050302743911743, 0.49496978521347046]}}
{"id": "WebQTest-1838", "question": "what sports are popular in germany", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"]], "ground_paths": [["cricket.cricket_team.country"], ["sports.sports_team.location"], ["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sports are popular in germany [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>"], "scores": [-0.04605609178543091, -0.05767054483294487], "norm_scores": [0.5029035806655884, 0.4970964193344116]}}
{"id": "WebQTest-1839", "question": "what 's my timezone in oklahoma", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what 's my timezone in oklahoma [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-4.5392720494419336e-05, -0.9109117984771729], "norm_scores": [0.7131773829460144, 0.2868225872516632]}}
{"id": "WebQTest-1840", "question": "what do they speak in norway", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do they speak in norway [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.06663837283849716, -0.07584027945995331], "norm_scores": [0.502300500869751, 0.4976995289325714]}}
{"id": "WebQTest-1842", "question": "what currency does singapore use", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does singapore use [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.020519722253084183, -0.14078502357006073], "norm_scores": [0.5300301313400269, 0.46996989846229553]}}
{"id": "WebQTest-1843", "question": "what did the blackfoot indians speak", "prediction": [["people.ethnicity.languages_spoken"], ["people.ethnicity.languages_spoken", "language.human_language.language_family", "language.human_language.language_family"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did the blackfoot indians speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> language.human_language.language_family <SEP> language.human_language.language_family </PATH>"], "scores": [-0.004647064488381147, -0.1696983426809311], "norm_scores": [0.5411693453788757, 0.4588305950164795]}}
{"id": "WebQTest-1844", "question": "who plays robin hood in prince of thieves", "prediction": [["film.film.country", "people.person.nationality"], ["film.film.starring", "film.actor.film"]], "ground_paths": [["film.producer.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays robin hood in prince of thieves [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film.starring <SEP> film.actor.film </PATH>"], "scores": [-0.04719368368387222, -0.14783130586147308], "norm_scores": [0.5251381993293762, 0.47486183047294617]}}
{"id": "WebQTest-1845", "question": "what standard time is texas", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what standard time is texas [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>"], "scores": [-7.852736598579213e-05, -0.4246246814727783], "norm_scores": [0.6045705676078796, 0.395429402589798]}}
{"id": "WebQTest-1847", "question": "where does kurdish people live", "prediction": [["people.ethnicity.geographic_distribution"], ["location.location.containedby"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does kurdish people live [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.004947460722178221, -0.3114418387413025], "norm_scores": [0.576029360294342, 0.42397063970565796]}}
{"id": "WebQTest-1848", "question": "what instrument does louis armstrong", "prediction": [["music.group_member.instruments_played"], ["music.artist.track_contributions", "music.track_contribution.role"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what instrument does louis armstrong [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>"], "scores": [-0.0009762406698428094, -0.23200751841068268], "norm_scores": [0.557502269744873, 0.44249773025512695]}}
{"id": "WebQTest-1849", "question": "who does alyson stoner play in camp rock", "prediction": [["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.film_character.portrayed_in_films"]], "ground_paths": [["film.actor.film", "film.performance.character"], ["film.film.starring", "film.performance.character"], ["film.performance.actor", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does alyson stoner play in camp rock [/INST]", "raw_output": {"paths": ["<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.07630738615989685, -0.07690481841564178], "norm_scores": [0.5001493692398071, 0.49985069036483765]}}
{"id": "WebQTest-1850", "question": "what religion does madonna practice", "prediction": [["people.person.religion"], ["religion.religion.notable_figures"]], "ground_paths": [["people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religion does madonna practice [/INST]", "raw_output": {"paths": ["<PATH> people.person.religion </PATH>", "<PATH> religion.religion.notable_figures </PATH>"], "scores": [-0.013429944403469563, -0.14062485098838806], "norm_scores": [0.5317559242248535, 0.4682440459728241]}}
{"id": "WebQTest-1851", "question": "what countries are located near egypt", "prediction": [["location.location.partially_contains", "location.location.partially_containedby"], ["location.location.partially_contains", "location.location.partially_contains"]], "ground_paths": [["time.event.locations", "time.event.locations"], ["location.location.partially_contains", "location.location.containedby"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["location.location.contains", "location.location.containedby"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.partially_containedby", "location.location.partially_contains"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.events", "time.event.locations"], ["location.location.contains_major_portion_of", "location.location.containedby"], ["location.location.contains_major_portion_of", "base.locations.countries.continent"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["government.form_of_government.countries", "location.country.form_of_government"], ["location.location.contains", "location.location.contains"], ["location.statistical_region.places_exported_to", "location.imports_and_exports.exported_to"], ["location.location.events", "location.location.events"], ["language.human_language.countries_spoken_in", "location.country.official_language"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["film.film.featured_film_locations", "film.film.featured_film_locations"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["base.schemastaging.disputed_location.claimed_by", "base.schemastaging.disputed_location_claimant.disputed_territories"], ["location.location.partially_containedby", "location.location.contains"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["location.location.partially_contains", "location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are located near egypt [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_contains <SEP> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>"], "scores": [-0.15102075040340424, -0.15674790740013123], "norm_scores": [0.5014318227767944, 0.49856823682785034]}}
{"id": "WebQTest-1853", "question": "what made richard hammond famous", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what made richard hammond famous [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.009949860163033009, -0.23334293067455292], "norm_scores": [0.5556171536445618, 0.44438281655311584]}}
{"id": "WebQTest-1854", "question": "who originally voiced lois griffin", "prediction": [["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["people.person.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who originally voiced lois griffin [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.0671430304646492, -0.1063506081700325], "norm_scores": [0.5098006129264832, 0.49019935727119446]}}
{"id": "WebQTest-1855", "question": "what airport do you fly into in paris", "prediction": [["aviation.airport.serves"], ["location.location.nearby_airports"]], "ground_paths": [["aviation.airport.serves"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what airport do you fly into in paris [/INST]", "raw_output": {"paths": ["<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.nearby_airports </PATH>"], "scores": [-0.034068234264850616, -0.11635885387659073], "norm_scores": [0.5205610394477844, 0.4794389307498932]}}
{"id": "WebQTest-1857", "question": "what rihanna new album called 2012", "prediction": [["music.album.artist"], ["music.artist.album"]], "ground_paths": [["music.artist.album"], ["music.album.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what rihanna new album called 2012 [/INST]", "raw_output": {"paths": ["<PATH> music.album.artist </PATH>", "<PATH> music.artist.album </PATH>"], "scores": [-0.05962903052568436, -0.13310010731220245], "norm_scores": [0.5183595418930054, 0.4816405177116394]}}
{"id": "WebQTest-1858", "question": "where does dave ramsey live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does dave ramsey live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.07776735723018646, -0.08697689324617386], "norm_scores": [0.502302348613739, 0.4976976215839386]}}
{"id": "WebQTest-1860", "question": "what discovery did galileo make", "prediction": [["astronomy.astronomer.astronomical_objects_discovered"], ["law.invention.inventor"]], "ground_paths": [["astronomy.astronomer.astronomical_objects_discovered"], ["astronomy.astronomical_discovery.discoverer"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what discovery did galileo make [/INST]", "raw_output": {"paths": ["<PATH> astronomy.astronomer.astronomical_objects_discovered </PATH>", "<PATH> law.invention.inventor </PATH>"], "scores": [-0.03961464390158653, -0.16910602152347565], "norm_scores": [0.5323276519775391, 0.46767231822013855]}}
{"id": "WebQTest-1861", "question": "what is the government system of saudi arabia", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the government system of saudi arabia [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>"], "scores": [-0.04509395360946655, -0.04522828757762909], "norm_scores": [0.5000335574150085, 0.49996641278266907]}}
{"id": "WebQTest-1864", "question": "who plays the role of tony dinozzo on ncis", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays the role of tony dinozzo on ncis [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.05741496384143829, -0.08948831260204315], "norm_scores": [0.5080176591873169, 0.4919823706150055]}}
{"id": "WebQTest-1865", "question": "what did jesus do for living", "prediction": [["people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did jesus do for living [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.005765457171946764, -0.3667377233505249], "norm_scores": [0.5892757773399353, 0.4107241928577423]}}
{"id": "WebQTest-1866", "question": "what is the state name of new york city", "prediction": [["location.location.containedby"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["base.biblioness.bibs_location.state"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the state name of new york city [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.06079281494021416, -0.17482565343379974], "norm_scores": [0.5284773707389832, 0.47152265906333923]}}
{"id": "WebQTest-1868", "question": "who plays knox overstreet", "prediction": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays knox overstreet [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.08818721026182175, -0.09175952523946762], "norm_scores": [0.5008930563926697, 0.4991069436073303]}}
{"id": "WebQTest-1870", "question": "what movies did david carradine play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"], ["people.person.languages", "film.film.language"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.personal_film_appearance.person", "film.personal_film_appearance.film"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.film.edited_by"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["film.producer.films_executive_produced"], ["film.director.film"], ["people.person.languages", "tv.tv_program.languages"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["award.award_winner.awards_won", "award.award_honor.honored_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did david carradine play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.061769429594278336, -0.13862116634845734], "norm_scores": [0.5192034840583801, 0.4807965159416199]}}
{"id": "WebQTest-1871", "question": "what is the stanley cup named after", "prediction": [["sports.sports_championship_event.championship"], ["sports.sports_championship_event.champion"]], "ground_paths": [["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the stanley cup named after [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.04937548190355301, -0.059199754148721695], "norm_scores": [0.5024560689926147, 0.49754393100738525]}}
{"id": "WebQTest-1873", "question": "where is traditional chinese used", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is traditional chinese used [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.07894884794950485, -0.09850641340017319], "norm_scores": [0.5048892498016357, 0.49511075019836426]}}
{"id": "WebQTest-1874", "question": "who does portugal trade with", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"]], "ground_paths": [["sports.sport_country.multi_event_tournaments_participated_in", "sports.sport_country.multi_event_tournaments_participated_in"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["location.country.form_of_government", "location.country.form_of_government"], ["location.location.time_zones", "location.location.time_zones"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"], ["common.topic.notable_types", "common.topic.notable_types"], ["military.military_combatant.belongs_to_combatant_group", "military.military_combatant.includes_allies"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["government.form_of_government.countries", "location.country.form_of_government"], ["location.statistical_region.places_exported_to", "location.imports_and_exports.exported_to"], ["people.person.nationality", "location.location.people_born_here"], ["language.human_language.countries_spoken_in", "location.country.official_language"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["book.written_work.subjects", "book.written_work.subjects"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"], ["location.country.form_of_government", "government.form_of_government.countries"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["organization.organization.founders", "organization.organization.founders"], ["location.imports_and_exports.imported_from", "location.imports_and_exports.exported_to"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does portugal trade with [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.imports_and_exports.imported_from <SEP> location.statistical_region.places_imported_from </PATH>"], "scores": [-0.05700792744755745, -0.08534131199121475], "norm_scores": [0.5070828795433044, 0.49291709065437317]}}
{"id": "WebQTest-1875", "question": "what type of political system is brazil", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of political system is brazil [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.016363197937607765, -0.10134325176477432], "norm_scores": [0.5212321877479553, 0.4787677526473999]}}
{"id": "WebQTest-1876", "question": "what language did the ancient babylonians speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language did the ancient babylonians speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.029557662084698677, -0.15323124825954437], "norm_scores": [0.5308790802955627, 0.46912097930908203]}}
{"id": "WebQTest-1877", "question": "what kind of painter was wassily kandinsky", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of painter was wassily kandinsky [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.013496247120201588, -0.12913227081298828], "norm_scores": [0.5288768410682678, 0.4711231589317322]}}
{"id": "WebQTest-1878", "question": "what form of government does the united states use", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does the united states use [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.02532977983355522, -0.07540007680654526], "norm_scores": [0.5125149488449097, 0.48748505115509033]}}
{"id": "WebQTest-1879", "question": "what money does cuba use", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["finance.currency.countries_used"], ["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money does cuba use [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.06056743860244751, -0.06607107073068619], "norm_scores": [0.5013759136199951, 0.4986240863800049]}}
{"id": "WebQTest-1880", "question": "who is my representative in illinois house", "prediction": [["government.government_position_held.governmental_body", "government.politician.government_positions_held"], ["government.government_position_held.governmental_body", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.governmental_body", "government.politician.government_positions_held"], ["government.government_position_held.governmental_body", "government.government_position_held.office_holder"], ["government.governmental_body.members", "government.government_position_held.office_holder"], ["government.governmental_body.members", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is my representative in illinois house [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.governmental_body <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.04688926786184311, -0.06291545927524567], "norm_scores": [0.5040064454078674, 0.49599355459213257]}}
{"id": "WebQTest-1881", "question": "what timezone is sweden gmt", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is sweden gmt [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>"], "scores": [-0.014119705185294151, -0.10277780890464783], "norm_scores": [0.5221500396728516, 0.4778499901294708]}}
{"id": "WebQTest-1882", "question": "what was franklin d roosevelt 's job before president", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_position_held.basic_title", "government.government_position_held.jurisdiction_of_office", "government.governmental_jurisdiction.government_positions"]], "ground_paths": [["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_position_held.appointed_by", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was franklin d roosevelt 's job before president [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.jurisdiction_of_office <SEP> government.governmental_jurisdiction.government_positions </PATH>"], "scores": [-0.04221086949110031, -0.049384333193302155], "norm_scores": [0.501793384552002, 0.49820664525032043]}}
{"id": "WebQTest-1883", "question": "where did robert downey jr go to jail", "prediction": [["people.person.nationality", "location.location.containedby"], ["base.prison.imprisonment.prisoner", "base.prison.prison.prisoners"]], "ground_paths": [["base.prison.imprisonment.prisoner", "base.prison.imprisonment.prison"], ["celebrities.rehab.celebrity", "celebrities.rehab.rehab_facility"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did robert downey jr go to jail [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> base.prison.imprisonment.prisoner <SEP> base.prison.prison.prisoners </PATH>"], "scores": [-0.024695688858628273, -0.0720704197883606], "norm_scores": [0.5118414759635925, 0.48815855383872986]}}
{"id": "WebQTest-1884", "question": "what degrees did barack obama get", "prediction": [["education.education.student", "education.education.degree"], ["people.person.education", "education.education.degree"]], "ground_paths": [["people.person.education", "education.education.degree"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what degrees did barack obama get [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> people.person.education <SEP> education.education.degree </PATH>"], "scores": [-0.03137349709868431, -0.058903153985738754], "norm_scores": [0.5068820118904114, 0.493118017911911]}}
{"id": "WebQTest-1885", "question": "what does god shiva represent", "prediction": [["religion.religion.deities"], ["religion.deity.deity_of"]], "ground_paths": [["religion.deity.deity_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does god shiva represent [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.deities </PATH>", "<PATH> religion.deity.deity_of </PATH>"], "scores": [-0.04440993443131447, -0.0980953723192215], "norm_scores": [0.5134181380271912, 0.48658186197280884]}}
{"id": "WebQTest-1886", "question": "what inventions were made by ben franklin", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"]], "ground_paths": [["law.inventor.inventions"], ["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inventions were made by ben franklin [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>"], "scores": [-0.03586019203066826, -0.10482577979564667], "norm_scores": [0.5172345638275146, 0.48276543617248535]}}
{"id": "WebQTest-1887", "question": "what is the name of the currency used in switzerland", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the currency used in switzerland [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.02638443373143673, -0.12223348021507263], "norm_scores": [0.5239439606666565, 0.4760560691356659]}}
{"id": "WebQTest-1888", "question": "what year did the milwaukee brewers go to the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the milwaukee brewers go to the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>"], "scores": [-0.021095987409353256, -0.09843789041042328], "norm_scores": [0.519325852394104, 0.480674147605896]}}
{"id": "WebQTest-1889", "question": "who was vice president with bill clinton", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"]], "ground_paths": [["government.us_vice_president.to_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vice president with bill clinton [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>"], "scores": [-0.061126530170440674, -0.07949156314134598], "norm_scores": [0.504591166973114, 0.4954088628292084]}}
{"id": "WebQTest-1890", "question": "what songs did franz liszt compose", "prediction": [["music.composition.composer"], ["music.composition.lyricist"]], "ground_paths": [["music.composition.composer"], ["book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what songs did franz liszt compose [/INST]", "raw_output": {"paths": ["<PATH> music.composition.composer </PATH>", "<PATH> music.composition.lyricist </PATH>"], "scores": [-0.01604481413960457, -0.29734209179878235], "norm_scores": [0.5698642730712891, 0.4301357865333557]}}
{"id": "WebQTest-1891", "question": "where did the russian japanese war happen", "prediction": [["time.event.locations"], ["location.location.events"]], "ground_paths": [["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the russian japanese war happen [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.06680066883563995, -0.11979839205741882], "norm_scores": [0.5132463574409485, 0.4867537021636963]}}
{"id": "WebQTest-1892", "question": "what year did the la kings win the cup", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the la kings win the cup [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.03357663378119469, -0.05814644321799278], "norm_scores": [0.5061421394348145, 0.49385786056518555]}}
{"id": "WebQTest-1893", "question": "what kind of money does egypt have", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money does egypt have [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.011784136295318604, -0.21595704555511475], "norm_scores": [0.5508666038513184, 0.44913333654403687]}}
{"id": "WebQTest-1894", "question": "what baseball team was jackie robinson on", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what baseball team was jackie robinson on [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>"], "scores": [-0.03949831426143646, -0.041534703224897385], "norm_scores": [0.5005090832710266, 0.4994909167289734]}}
{"id": "WebQTest-1895", "question": "what body of water does st lawrence river flow into", "prediction": [["geography.river.mouth"], ["geography.body_of_water.mouth"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what body of water does st lawrence river flow into [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> geography.body_of_water.mouth </PATH>"], "scores": [-0.01667388528585434, -0.22907216846942902], "norm_scores": [0.5529008507728577, 0.4470991790294647]}}
{"id": "WebQTest-1896", "question": "what form of government does the united states have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does the united states have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.025977302342653275, -0.07395632565021515], "norm_scores": [0.5119924545288086, 0.4880075454711914]}}
{"id": "WebQTest-1897", "question": "what are the names of ariel 's six sisters", "prediction": [["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.children"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.parents"]], "ground_paths": [["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"], ["fictional_universe.sibling_relationship_of_fictional_characters.siblings", "fictional_universe.fictional_character.siblings"], ["fictional_universe.fictional_character.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.parents"], ["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character.siblings", "fictional_universe.fictional_character.siblings"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.children"], ["fictional_universe.sibling_relationship_of_fictional_characters.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the names of ariel 's six sisters [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.parents <SEP> fictional_universe.fictional_character.children </PATH>", "<PATH> fictional_universe.fictional_character.parents <SEP> fictional_universe.fictional_character.parents </PATH>"], "scores": [-0.03970620408654213, -0.05048085004091263], "norm_scores": [0.5026936531066895, 0.49730637669563293]}}
{"id": "WebQTest-1898", "question": "what voices does seth macfarlane play on family guy", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_universe.characters"]], "ground_paths": [["tv.tv_program.regular_cast", "tv.regular_tv_appearance.character"], ["tv.tv_program_creator.programs_created", "fictional_universe.fictional_character.character_created_by"], ["tv.regular_tv_appearance.series", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["tv.tv_program.regular_cast", "tv.tv_character.appeared_in_tv_program"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_universe.characters"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what voices does seth macfarlane play on family guy [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes <SEP> fictional_universe.fictional_character.appears_in_these_fictional_universes </PATH>", "<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes <SEP> fictional_universe.fictional_universe.characters </PATH>"], "scores": [-0.03443369269371033, -0.04314783215522766], "norm_scores": [0.5021785497665405, 0.4978214502334595]}}
{"id": "WebQTest-1899", "question": "who was william mckinley married to", "prediction": [["base.famouspets.pet_ownership.owner", "base.famouspets.pet_owner.pets_owned"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_ownership.owner"]], "ground_paths": [["people.person.spouse_s", "people.marriage.spouse"], ["people.person.children", "people.person.parents"], ["people.person.profession", "common.topic.notable_types"], ["people.person.children", "people.person.children"], ["people.person.nationality", "people.person.nationality"], ["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_owner.pets_owned"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_owner.pets_owned"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was william mckinley married to [/INST]", "raw_output": {"paths": ["<PATH> base.famouspets.pet_ownership.owner <SEP> base.famouspets.pet_owner.pets_owned </PATH>", "<PATH> base.famouspets.pet_ownership.owner <SEP> base.famouspets.pet_ownership.owner </PATH>"], "scores": [-0.08622738718986511, -0.09161341935396194], "norm_scores": [0.5013465285301208, 0.49865350127220154]}}
{"id": "WebQTest-1900", "question": "who played darth vader in the first movie", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played darth vader in the first movie [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.05668921023607254, -0.07433798164129257], "norm_scores": [0.504412055015564, 0.49558791518211365]}}
{"id": "WebQTest-1901", "question": "where did kevin james go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kevin james go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.04458765685558319, -0.05072708800435066], "norm_scores": [0.5015348792076111, 0.4984651505947113]}}
{"id": "WebQTest-1902", "question": "what year was hitler was born", "prediction": [["people.person.place_of_birth"], ["base.activism.activist.area_of_activism", "time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was hitler was born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> time.event.locations </PATH>"], "scores": [-0.047543082386255264, -0.11874162405729294], "norm_scores": [0.5177921056747437, 0.48220789432525635]}}
{"id": "WebQTest-1904", "question": "what 's there to see in atlanta", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "ground_paths": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what 's there to see in atlanta [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>"], "scores": [-0.05259183049201965, -0.05985109135508537], "norm_scores": [0.5018147826194763, 0.4981851875782013]}}
{"id": "WebQTest-1905", "question": "where was auschwitz concentration camp located", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was auschwitz concentration camp located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.010415568947792053, -0.40555453300476074], "norm_scores": [0.5975192189216614, 0.4024808406829834]}}
{"id": "WebQTest-1907", "question": "what kind of currency does dominican republic use", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of currency does dominican republic use [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.0004319546278566122, -0.48337146639823914], "norm_scores": [0.6184417605400085, 0.38155823945999146]}}
{"id": "WebQTest-1909", "question": "what is spoken in czech republic", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is spoken in czech republic [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.043780867010354996, -0.08087419718503952], "norm_scores": [0.5092722177505493, 0.4907277226448059]}}
{"id": "WebQTest-1910", "question": "where was shakespeare born at", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was shakespeare born at [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.05171206220984459, -0.06322912126779556], "norm_scores": [0.50287926197052, 0.49712076783180237]}}
{"id": "WebQTest-1911", "question": "where did scott fitzgerald go to college", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did scott fitzgerald go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.047366246581077576, -0.04811720550060272], "norm_scores": [0.5001877546310425, 0.4998122751712799]}}
{"id": "WebQTest-1912", "question": "what did joan crawford died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did joan crawford died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.008547727949917316, -0.14159217476844788], "norm_scores": [0.5332121253013611, 0.4667878746986389]}}
{"id": "WebQTest-1913", "question": "where did rob kardashian go to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did rob kardashian go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.043389420956373215, -0.06362778693437576], "norm_scores": [0.5050594210624695, 0.4949405789375305]}}
{"id": "WebQTest-1914", "question": "what year was allen iverson mvp", "prediction": [["sports.sports_award_winner.awards", "sports.sports_award.season"], ["award.award_winner.awards", "award.award_honor.season"]], "ground_paths": [["sports.sports_award_winner.awards", "sports.sports_league_season.awards"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["sports.sports_award.award_winner", "sports.sports_award.season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was allen iverson mvp [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> award.award_winner.awards <SEP> award.award_honor.season </PATH>"], "scores": [-0.07651251554489136, -0.1154409870505333], "norm_scores": [0.5097309350967407, 0.49026912450790405]}}
{"id": "WebQTest-1915", "question": "where did harry s truman go to school", "prediction": [["book.written_work.subjects", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["book.written_work.subjects", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did harry s truman go to school [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.03612608462572098, -0.06850522011518478], "norm_scores": [0.508094072341919, 0.49190592765808105]}}
{"id": "WebQTest-1916", "question": "what county is utica ny in", "prediction": [["location.us_county.hud_county_place"], ["location.location.containedby"]], "ground_paths": [["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is utica ny in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.08140189945697784, -0.11330008506774902], "norm_scores": [0.507973849773407, 0.49202612042427063]}}
{"id": "WebQTest-1917", "question": "what did charles babbage discover", "prediction": [["law.invention.inventor"], ["symbols.namesake.named_after"]], "ground_paths": [["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did charles babbage discover [/INST]", "raw_output": {"paths": ["<PATH> law.invention.inventor </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.014692476019263268, -0.24086695909500122], "norm_scores": [0.5563037991523743, 0.44369620084762573]}}
{"id": "WebQTest-1919", "question": "who plays king julian madagascar", "prediction": [["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.performance.film", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays king julian madagascar [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.11194153130054474, -0.126447394490242], "norm_scores": [0.5036264061927795, 0.49637362360954285]}}
{"id": "WebQTest-1920", "question": "who is rick scott", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_position_held.office_holder"]], "ground_paths": [["common.topic.notable_types"], ["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is rick scott [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.009681778028607368, -0.12136353552341461], "norm_scores": [0.5278914570808411, 0.47210854291915894]}}
{"id": "WebQTest-1921", "question": "where did the crimean war take place", "prediction": [["time.event.locations"], ["location.location.events"]], "ground_paths": [["time.event.locations"], ["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the crimean war take place [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.05372375249862671, -0.12489565461874008], "norm_scores": [0.5177854299545288, 0.4822145104408264]}}
{"id": "WebQTest-1922", "question": "what offices did theodore roosevelt hold", "prediction": [["government.politician.government_positions_held", "government.government_position_held.basic_title"], ["business.employment_tenure.person", "business.employment_tenure.title"]], "ground_paths": [["government.politician.government_positions_held", "government.government_position_held.basic_title"], ["government.government_position_held.office_holder", "government.government_position_held.basic_title"], ["government.government_position_held.office_holder", "government.government_office_category.officeholders"], ["common.topic.notable_types", "type.type.expected_by"], ["business.employment_tenure.person", "business.employment_tenure.title"], ["government.government_position_held.appointed_by", "government.government_position_held.basic_title"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what offices did theodore roosevelt hold [/INST]", "raw_output": {"paths": ["<PATH> government.politician.government_positions_held <SEP> government.government_position_held.basic_title </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.title </PATH>"], "scores": [-0.040211908519268036, -0.04633818939328194], "norm_scores": [0.5015315413475037, 0.49846842885017395]}}
{"id": "WebQTest-1923", "question": "what movies has miley cyrus been in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"]], "ground_paths": [["people.person.languages", "film.film.language"], ["music.artist.genre", "music.album.genre"], ["music.artist.concert_tours", "music.album.supporting_tours"], ["film.actor.film", "film.film.starring"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["common.topic.notable_types", "common.topic.notable_types"], ["music.concert_tour.artist", "music.album.supporting_tours"], ["film.film.music"], ["film.film_featured_song.performed_by", "film.film.featured_song"], ["people.person.children", "film.film.produced_by"], ["people.person.parents", "film.film.produced_by"], ["people.person.children", "film.film.executive_produced_by"], ["people.person.children", "film.producer.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["music.artist.origin", "film.film.featured_film_locations"], ["film.performance.actor", "film.film.starring"], ["music.artist.genre", "media_common.netflix_title.netflix_genres"], ["film.actor.film", "film.performance.film"], ["music.artist.album", "media_common.adaptation.adapted_from"], ["music.lyricist.lyrics_written", "film.film.featured_song"], ["film.person_or_entity_appearing_in_film.films", "film.film.personal_appearances"], ["music.recording_contribution.contributor", "music.recording_contribution.album"], ["people.person.languages", "tv.tv_program.languages"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies has miley cyrus been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>"], "scores": [-0.10368910431861877, -0.11439035087823868], "norm_scores": [0.5026752948760986, 0.49732470512390137]}}
{"id": "WebQTest-1925", "question": "who did james franco play in milk", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did james franco play in milk [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.054829150438308716, -0.1219814121723175], "norm_scores": [0.516781747341156, 0.4832182228565216]}}
{"id": "WebQTest-1926", "question": "when did the libyan conflict start", "prediction": [["time.event.locations"], ["time.event.includes_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the libyan conflict start [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> time.event.includes_event </PATH>"], "scores": [-0.060221076011657715, -0.13513991236686707], "norm_scores": [0.5187209844589233, 0.48127907514572144]}}
{"id": "WebQTest-1928", "question": "what wnba team is diana taurasi on", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what wnba team is diana taurasi on [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.05082717910408974, -0.07036174833774567], "norm_scores": [0.5048834681510925, 0.4951165020465851]}}
{"id": "WebQTest-1929", "question": "what do see in paris", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["location.location.containedby"], ["base.arthist.helynevek.intezmeny"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do see in paris [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.04067477583885193, -0.05169328674674034], "norm_scores": [0.5027546286582947, 0.4972454011440277]}}
{"id": "WebQTest-1932", "question": "who made the laws in canada", "prediction": [["government.governmental_jurisdiction.government_bodies"], ["government.governmental_jurisdiction.government"]], "ground_paths": [["government.governmental_body.jurisdiction"], ["government.governmental_jurisdiction.government_bodies"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who made the laws in canada [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.government_bodies </PATH>", "<PATH> government.governmental_jurisdiction.government </PATH>"], "scores": [-0.04175931215286255, -0.09365334361791611], "norm_scores": [0.5129705667495728, 0.48702940344810486]}}
{"id": "WebQTest-1933", "question": "what year did adam morrison get drafted", "prediction": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did adam morrison get drafted [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>"], "scores": [-0.04478990286588669, -0.05987974628806114], "norm_scores": [0.5037723779678345, 0.49622759222984314]}}
{"id": "WebQTest-1934", "question": "where michael jackson from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where michael jackson from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.024163587018847466, -0.15258385241031647], "norm_scores": [0.5320610404014587, 0.46793898940086365]}}
{"id": "WebQTest-1935", "question": "what college did maya moore go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did maya moore go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.04380710795521736, -0.06025296822190285], "norm_scores": [0.5041114091873169, 0.4958886504173279]}}
{"id": "WebQTest-1936", "question": "when was the last time the dodgers won a championship", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the dodgers won a championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.04111973196268082, -0.04963582009077072], "norm_scores": [0.5021290183067322, 0.4978709816932678]}}
{"id": "WebQTest-1937", "question": "who is captain james cook facts", "prediction": [["people.person.profession"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is captain james cook facts [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.009626664221286774, -0.28520381450653076], "norm_scores": [0.5684615969657898, 0.4315384328365326]}}
{"id": "WebQTest-1938", "question": "where did robin williams get married", "prediction": [["people.person.children", "people.person.place_of_birth"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.spouse_s", "people.marriage.location_of_ceremony"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did robin williams get married [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.07531500607728958, -0.09442948549985886], "norm_scores": [0.5047785043716431, 0.4952215254306793]}}
{"id": "WebQTest-1939", "question": "what form of government does canada", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "ground_paths": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does canada [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>"], "scores": [-0.03273482248187065, -0.0591244213283062], "norm_scores": [0.5065970420837402, 0.49340295791625977]}}
{"id": "WebQTest-1940", "question": "who do islamic people follow", "prediction": [["religion.religion.deities"], ["religion.deity.deity_of"]], "ground_paths": [["religion.deity.deity_of"], ["religion.religion.deities"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who do islamic people follow [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.deities </PATH>", "<PATH> religion.deity.deity_of </PATH>"], "scores": [-0.023456981405615807, -0.1239919438958168], "norm_scores": [0.5251125693321228, 0.4748874008655548]}}
{"id": "WebQTest-1941", "question": "what kind of government does peru", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does peru [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.03321544826030731, -0.06042531132698059], "norm_scores": [0.5068020820617676, 0.4931979477405548]}}
{"id": "WebQTest-1942", "question": "who was in the israeli palestinian conflict", "prediction": [["time.event.people_involved"], ["people.person.nationality"]], "ground_paths": [["time.event.people_involved"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was in the israeli palestinian conflict [/INST]", "raw_output": {"paths": ["<PATH> time.event.people_involved </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0027600640896707773, -0.406578004360199], "norm_scores": [0.5996046662330627, 0.40039539337158203]}}
{"id": "WebQTest-1943", "question": "who founded the afl", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who founded the afl [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.006491934880614281, -0.17109666764736176], "norm_scores": [0.5410585403442383, 0.4589414894580841]}}
{"id": "WebQTest-1944", "question": "where did diego velazquez die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people_deceased_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did diego velazquez die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>"], "scores": [-0.000844797003082931, -0.36154019832611084], "norm_scores": [0.589208722114563, 0.4107912480831146]}}
{"id": "WebQTest-1946", "question": "what is the country code for mexico", "prediction": [["location.country.internet_tld"], ["location.location.containedby"]], "ground_paths": [["location.country.internet_tld"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the country code for mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.internet_tld </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.07542083412408829, -0.1561652272939682], "norm_scores": [0.5201751589775085, 0.47982487082481384]}}
{"id": "WebQTest-1947", "question": "what currency do they use in switzerland", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency do they use in switzerland [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.023300373926758766, -0.13138389587402344], "norm_scores": [0.5269945859909058, 0.47300538420677185]}}
{"id": "WebQTest-1950", "question": "where is president kennedy buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"]], "ground_paths": [["people.deceased_person.place_of_burial"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is president kennedy buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>"], "scores": [-0.0168217271566391, -0.10292347520589828], "norm_scores": [0.5215121507644653, 0.47848784923553467]}}
{"id": "WebQTest-1952", "question": "where was the prophet mohammed from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the prophet mohammed from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.019031507894396782, -0.13221703469753265], "norm_scores": [0.528266191482544, 0.47173380851745605]}}
{"id": "WebQTest-1954", "question": "what language do ukrainian people speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do ukrainian people speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.03262050449848175, -0.08182191103696823], "norm_scores": [0.5122978687286377, 0.4877021014690399]}}
{"id": "WebQTest-1955", "question": "who are the judges appointed in the supreme court", "prediction": [["law.court.jurisdiction", "people.person.nationality"], ["book.written_work.subjects", "book.author.works_written"]], "ground_paths": [["law.court.jurisdiction", "people.person.nationality"], ["book.written_work.subjects", "book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the judges appointed in the supreme court [/INST]", "raw_output": {"paths": ["<PATH> law.court.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> book.written_work.subjects <SEP> book.author.works_written </PATH>"], "scores": [-0.045084089040756226, -0.07368607074022293], "norm_scores": [0.5071499943733215, 0.49285000562667847]}}
{"id": "WebQTest-1956", "question": "which country in europe has the largest land area", "prediction": [["location.location.containedby"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country in europe has the largest land area [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.07435103505849838, -0.12426858395338058], "norm_scores": [0.5124768018722534, 0.4875231981277466]}}
{"id": "WebQTest-1957", "question": "when did kaley cuoco join charmed", "prediction": [["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.seasons"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did kaley cuoco join charmed [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.regular_tv_appearance.seasons </PATH>"], "scores": [-0.06495285779237747, -0.07755672931671143], "norm_scores": [0.5031509399414062, 0.49684908986091614]}}
{"id": "WebQTest-1958", "question": "where is chris rock from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is chris rock from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.03345734253525734, -0.09367880970239639], "norm_scores": [0.5150508284568787, 0.48494917154312134]}}
{"id": "WebQTest-1960", "question": "when was the last time the toronto maple leafs win the stanley cup", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the toronto maple leafs win the stanley cup [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.04320358484983444, -0.04798425734043121], "norm_scores": [0.5011951923370361, 0.49880483746528625]}}
{"id": "WebQTest-1961", "question": "what kind of language do egyptian speak", "prediction": [["people.ethnicity.people", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.ethnicity.people", "people.person.nationality", "language.human_language.main_country"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language do egyptian speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> language.human_language.main_country </PATH>"], "scores": [-0.08066606521606445, -0.09274779260158539], "norm_scores": [0.5030204057693481, 0.49697959423065186]}}
{"id": "WebQTest-1962", "question": "what language do brazilians use", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do brazilians use [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.006309064570814371, -0.22274509072303772], "norm_scores": [0.5538987517356873, 0.44610121846199036]}}
{"id": "WebQTest-1963", "question": "what position did vince lombardi play", "prediction": [["american_football.football_player.position_s"], ["sports.sports_team_coach.sports_coached", "sports.sports_team_coach.sports_coached"]], "ground_paths": [["american_football.football_player.position_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what position did vince lombardi play [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.position_s </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> sports.sports_team_coach.sports_coached </PATH>"], "scores": [-0.03916309401392937, -0.06449619680643082], "norm_scores": [0.5063329339027405, 0.49366703629493713]}}
{"id": "WebQTest-1964", "question": "who played stanley kowalski in a streetcar named desire movie", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["film.film.starring", "film.actor.film"], ["theater.play.country_of_origin", "people.person.nationality"], ["film.performance.character", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["common.image.appears_in_topic_gallery", "common.topic.image"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played stanley kowalski in a streetcar named desire movie [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.11377353966236115, -0.1365332454442978], "norm_scores": [0.5056896805763245, 0.49431031942367554]}}
{"id": "WebQTest-1965", "question": "who did the voice for lola bunny", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did the voice for lola bunny [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.04510454460978508, -0.10878755897283554], "norm_scores": [0.5159153938293457, 0.4840846359729767]}}
{"id": "WebQTest-1966", "question": "what book did charles darwin wrote in 1859", "prediction": [["book.written_work.author"], ["book.written_work.subjects"]], "ground_paths": [["exhibitions.exhibition_subject.exhibitions_created_about_this_subject"], [], ["book.written_work.author"], ["base.argumentmaps.innovator.original_ideas"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what book did charles darwin wrote in 1859 [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.008847526274621487, -0.35185036063194275], "norm_scores": [0.5849197506904602, 0.4150802493095398]}}
{"id": "WebQTest-1967", "question": "what language do serbs speak", "prediction": [["people.ethnicity.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["people.ethnicity.languages_spoken"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do serbs speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.03295021131634712, -0.1087646558880806], "norm_scores": [0.5189445614814758, 0.48105546832084656]}}
{"id": "WebQTest-1968", "question": "what was the cause of death for laci peterson", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the cause of death for laci peterson [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.008649181574583054, -0.14415757358074188], "norm_scores": [0.5338253378868103, 0.4661746323108673]}}
{"id": "WebQTest-1969", "question": "what team is kris humphries play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team.roster"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is kris humphries play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.053609635680913925, -0.05592981353402138], "norm_scores": [0.5005800127983093, 0.4994199275970459]}}
{"id": "WebQTest-1971", "question": "what movies gerard butler has been in", "prediction": [["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["music.recording.artist"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.film.produced_by"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.film.produced_by", "film.film.sequel"], ["film.producer.film"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["film.producer.films_executive_produced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies gerard butler has been in [/INST]", "raw_output": {"paths": ["<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.1116923838853836, -0.12757056951522827], "norm_scores": [0.5039694905281067, 0.4960305392742157]}}
{"id": "WebQTest-1972", "question": "where did john franklin live", "prediction": [["people.deceased_person.place_of_death"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did john franklin live [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.08190424740314484, -0.12474209815263748], "norm_scores": [0.5107078552246094, 0.489292174577713]}}
{"id": "WebQTest-1975", "question": "what was the last movie sean connery was in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["film.film.produced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the last movie sean connery was in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.06025025248527527, -0.12519118189811707], "norm_scores": [0.516229510307312, 0.4837704598903656]}}
{"id": "WebQTest-1976", "question": "what team did drogba play for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["people.person.place_of_birth", "sports.sports_team_location.teams"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did drogba play for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.06880936026573181, -0.10449307411909103], "norm_scores": [0.5089200139045715, 0.49108001589775085]}}
{"id": "WebQTest-1977", "question": "where did crabtree go to college", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did crabtree go to college [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.048030611127614975, -0.058349523693323135], "norm_scores": [0.5025796890258789, 0.4974202811717987]}}
{"id": "WebQTest-1978", "question": "where does freddie ljungberg come from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does freddie ljungberg come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.021190954372286797, -0.2135382443666458], "norm_scores": [0.547939121723175, 0.45206090807914734]}}
{"id": "WebQTest-1979", "question": "who is number 22 on miami dolphins", "prediction": [["american_football.football_player.position_s", "people.person.nationality", "people.person.nationality"], ["sports.sports_team_roster.team", "sports.sports_team_roster.player"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is number 22 on miami dolphins [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.position_s <SEP> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> sports.sports_team_roster.team <SEP> sports.sports_team_roster.player </PATH>"], "scores": [-0.0510040782392025, -0.06876340508460999], "norm_scores": [0.5044397115707397, 0.49556028842926025]}}
{"id": "WebQTest-1981", "question": "what state did john adams live in", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["government.government_position_held.office_holder", "government.government_position_held.district_represented"]], "ground_paths": [["symbols.name_source.namesakes", "location.location.containedby"], ["symbols.name_source.namesakes", "base.usnationalparks.us_national_park.state"], ["government.politician.government_positions_held", "government.government_position_held.district_represented"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["tv.tv_program.country_of_origin", "base.locations.countries.states_provinces_within"], ["people.deceased_person.place_of_burial", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state did john adams live in [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.district_represented </PATH>"], "scores": [-0.051074523478746414, -0.08895287662744522], "norm_scores": [0.5094684362411499, 0.4905315339565277]}}
{"id": "WebQTest-1982", "question": "what type of music did vivaldi compose", "prediction": [["music.artist.genre"], ["music.genre.artists"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did vivaldi compose [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>"], "scores": [-0.014330108650028706, -0.21154701709747314], "norm_scores": [0.5491450428962708, 0.45085498690605164]}}
{"id": "WebQTest-1985", "question": "what team does mike fisher play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does mike fisher play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.03751097992062569, -0.04537958651781082], "norm_scores": [0.5019671320915222, 0.4980328381061554]}}
{"id": "WebQTest-1986", "question": "what films has daniel craig been in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["people.person.languages", "film.film.language"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.film.executive_produced_by"], ["people.person.languages", "tv.tv_program.languages"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.film.starring"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what films has daniel craig been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.07580576837062836, -0.12654639780521393], "norm_scores": [0.5126824378967285, 0.4873175621032715]}}
{"id": "WebQTest-1987", "question": "what materials did eduardo paolozzi use in his work", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_artist.art_forms", "visual_art.visual_artist.art_forms"]], "ground_paths": [["visual_art.visual_artist.artworks", "visual_art.artwork.media"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what materials did eduardo paolozzi use in his work [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_artist.art_forms <SEP> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.010391575284302235, -0.09228171408176422], "norm_scores": [0.5204611420631409, 0.4795389175415039]}}
{"id": "WebQTest-1988", "question": "what did joe biden study in college", "prediction": [["education.education.student", "education.education.major_field_of_study"], ["people.person.education", "education.education.major_field_of_study"]], "ground_paths": [["education.education.student", "education.education.major_field_of_study"], ["common.topic.subjects", "book.written_work.school_or_movement"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did joe biden study in college [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>"], "scores": [-0.03854886069893837, -0.043531034141778946], "norm_scores": [0.5012455582618713, 0.49875444173812866]}}
{"id": "WebQTest-1990", "question": "who plays timon 's voice in the lion king", "prediction": [["film.film.starring", "film.actor.film"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["film.film.starring", "film.actor.film"], ["music.recording.releases", "music.artist.track"], ["film.film.country", "people.person.nationality"], ["music.recording.releases", "music.featured_artist.recordings"], ["film.performance.film", "film.actor.film"], ["film.film.language", "people.person.languages"], ["film.film.featured_song", "music.recording.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays timon 's voice in the lion king [/INST]", "raw_output": {"paths": ["<PATH> film.film.starring <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.1248956024646759, -0.14877571165561676], "norm_scores": [0.505969762802124, 0.49403026700019836]}}
{"id": "WebQTest-1991", "question": "what part of the country is ohio in", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part of the country is ohio in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.06382275372743607, -0.11702555418014526], "norm_scores": [0.5132975578308105, 0.48670244216918945]}}
{"id": "WebQTest-1992", "question": "which island is kailua on", "prediction": [["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which island is kailua on [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0450800359249115, -0.19874058663845062], "norm_scores": [0.5383397340774536, 0.4616602957248688]}}
{"id": "WebQTest-1993", "question": "what language do speak in argentina", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do speak in argentina [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.03169665113091469, -0.10024356096982956], "norm_scores": [0.5171300172805786, 0.482869952917099]}}
{"id": "WebQTest-1994", "question": "what did whoopi goldberg won a grammy for", "prediction": [["award.award_honor.award_winner", "award.award_honor.award"], ["award.award_winner.awards_won", "award.award_honor.award"]], "ground_paths": [["award.award_honor.award_winner", "award.award_category.winners"], ["award.award_nomination.award_nominee", "award.award_category.nominees"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did whoopi goldberg won a grammy for [/INST]", "raw_output": {"paths": ["<PATH> award.award_honor.award_winner <SEP> award.award_honor.award </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.award </PATH>"], "scores": [-0.0430058017373085, -0.05745815113186836], "norm_scores": [0.5036130547523499, 0.49638697504997253]}}
{"id": "WebQTest-1996", "question": "who played maggie in himym", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"]], "ground_paths": [["fictional_universe.fictional_character.place_of_birth", "people.person.nationality"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played maggie in himym [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.050235141068696976, -0.09847835451364517], "norm_scores": [0.512058436870575, 0.48794153332710266]}}
{"id": "WebQTest-1997", "question": "who does owen schmitt play for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does owen schmitt play for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.051327988505363464, -0.054280836135149], "norm_scores": [0.5007382035255432, 0.4992617964744568]}}
{"id": "WebQTest-1998", "question": "what party was thomas jefferson affiliated with", "prediction": [["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [["organization.organization_founder.organizations_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what party was thomas jefferson affiliated with [/INST]", "raw_output": {"paths": ["<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.06497475504875183, -0.06922126561403275], "norm_scores": [0.5010616183280945, 0.4989383816719055]}}
{"id": "WebQTest-1999", "question": "what religions are popular in france", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "ground_paths": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["periodicals.newspaper_circulation_area.newspapers", "book.periodical.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religions are popular in france [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>"], "scores": [-0.03194145858287811, -0.033270761370658875], "norm_scores": [0.5003323554992676, 0.4996677041053772]}}
{"id": "WebQTest-2000", "question": "who played berger in sex and the city", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["film.film.country", "people.person.nationality"]], "ground_paths": [["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played berger in sex and the city [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>"], "scores": [-0.06860006600618362, -0.08680195361375809], "norm_scores": [0.5045503973960876, 0.49544966220855713]}}
{"id": "WebQTest-2001", "question": "who won the 2000 fa cup final", "prediction": [["sports.sports_award_type.winners", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"]], "ground_paths": [["time.event.instance_of_recurring_event", "sports.sports_team.championships"], ["sports.sports_award.award", "sports.sports_award.award_winner"], ["time.event.instance_of_recurring_event", "sports.sports_championship_event.champion"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.champion"], ["sports.sports_award_type.winners", "sports.sports_award.award_winner"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won the 2000 fa cup final [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_type.winners <SEP> sports.sports_award.award_winner </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.0490315705537796, -0.05644693970680237], "norm_scores": [0.5018538236618042, 0.4981461465358734]}}
{"id": "WebQTest-2002", "question": "what are the names of michael jackson 's kids", "prediction": [["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the names of michael jackson 's kids [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.0506080724298954, -0.13310961425304413], "norm_scores": [0.5206137299537659, 0.4793862998485565]}}
{"id": "WebQTest-2003", "question": "what movies have robert pattinson in them", "prediction": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "ground_paths": [["music.recording.artist", "music.recording.releases"], ["film.actor.film", "film.film.starring"], ["film.producer.films_executive_produced"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["music.artist.track", "music.recording.releases"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies have robert pattinson in them [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.0644163265824318, -0.06699005514383316], "norm_scores": [0.5006434321403503, 0.49935656785964966]}}
{"id": "WebQTest-2004", "question": "what the time zone in japan", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the time zone in japan [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>"], "scores": [-0.002186407567933202, -0.20194458961486816], "norm_scores": [0.549774169921875, 0.4502258896827698]}}
{"id": "WebQTest-2005", "question": "who plays bob kelso in scrubs", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays bob kelso in scrubs [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.04715174064040184, -0.07818829268217087], "norm_scores": [0.5077584981918335, 0.4922414720058441]}}
{"id": "WebQTest-2006", "question": "what race is vanessa carlton", "prediction": [["people.person.ethnicity"], ["people.ethnicity.people"]], "ground_paths": [["people.person.ethnicity"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what race is vanessa carlton [/INST]", "raw_output": {"paths": ["<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>"], "scores": [-0.033939726650714874, -0.21746620535850525], "norm_scores": [0.5457533001899719, 0.45424672961235046]}}
{"id": "WebQTest-2007", "question": "what do people in czech republic speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people in czech republic speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.04593736678361893, -0.06353678554296494], "norm_scores": [0.5043997168540955, 0.49560025334358215]}}
{"id": "WebQTest-2008", "question": "which countries speak arabic language", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries speak arabic language [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.06059468165040016, -0.07815291732549667], "norm_scores": [0.5043894648551941, 0.4956105351448059]}}
{"id": "WebQTest-2009", "question": "where does tim cook work", "prediction": [["people.person.employment_history", "business.employer.employees"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["organization.leadership.person", "organization.organization.leadership"], ["people.person.employment_history", "business.employment_tenure.company"], ["business.employment_tenure.person", "business.employment_tenure.company"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does tim cook work [/INST]", "raw_output": {"paths": ["<PATH> people.person.employment_history <SEP> business.employer.employees </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.054489340633153915, -0.11047781258821487], "norm_scores": [0.513993501663208, 0.48600655794143677]}}
{"id": "WebQTest-2010", "question": "what degrees does romney have", "prediction": [["people.person.education", "education.education.degree"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.degree"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what degrees does romney have [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.05334718897938728, -0.059819869697093964], "norm_scores": [0.5016182065010071, 0.4983818531036377]}}
{"id": "WebQTest-2011", "question": "where is bob marley buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"]], "ground_paths": [["people.deceased_person.place_of_burial"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is bob marley buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>"], "scores": [-0.012421579100191593, -0.1137390211224556], "norm_scores": [0.5253077149391174, 0.47469228506088257]}}
{"id": "WebQTest-2012", "question": "what is london uk time zone", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is london uk time zone [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>"], "scores": [-0.00020816377946175635, -0.332397997379303], "norm_scores": [0.5822920799255371, 0.4177078902721405]}}
{"id": "WebQTest-2013", "question": "where to stay in chicago tourist", "prediction": [["travel.travel_destination.accommodation"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where to stay in chicago tourist [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.accommodation </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0850399062037468, -0.10872795432806015], "norm_scores": [0.5059217214584351, 0.49407824873924255]}}
{"id": "WebQTest-2014", "question": "what type of breast cancer did sheryl crow have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"]], "ground_paths": [["medicine.notable_person_with_medical_condition.condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of breast cancer did sheryl crow have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>"], "scores": [-0.008640117943286896, -0.11153669655323029], "norm_scores": [0.5257014632225037, 0.47429850697517395]}}
{"id": "WebQTest-2015", "question": "what type of government does argentina have today", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does argentina have today [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.031985338777303696, -0.06228208169341087], "norm_scores": [0.5075736045837402, 0.4924263656139374]}}
{"id": "WebQTest-2016", "question": "where does fabio capello come from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does fabio capello come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.02395855449140072, -0.17642050981521606], "norm_scores": [0.5380418300628662, 0.4619581699371338]}}
{"id": "WebQTest-2017", "question": "what party was abe lincoln part of", "prediction": [["organization.organization_founder.organizations_founded"], ["people.person.nationality", "organization.organization.geographic_scope"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["influence.influence_node.influenced_by", "organization.organization.founders"], ["government.political_party_tenure.politician", "government.political_party.politicians_in_this_party"], ["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what party was abe lincoln part of [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>"], "scores": [-0.048792459070682526, -0.06205535680055618], "norm_scores": [0.5033156871795654, 0.49668431282043457]}}
{"id": "WebQTest-2018", "question": "what did jack london do for a living", "prediction": [["people.person.profession"], ["fictional_universe.fictional_character.occupation"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did jack london do for a living [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>"], "scores": [-0.0074711088091135025, -0.2004697620868683], "norm_scores": [0.5481004118919373, 0.45189952850341797]}}
{"id": "WebQTest-2019", "question": "who did morgan freeman marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "ground_paths": [["people.person.parents", "people.person.children"], ["people.marriage.spouse", "people.marriage.spouse"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did morgan freeman marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>"], "scores": [-0.04549137130379677, -0.08552809059619904], "norm_scores": [0.5100078582763672, 0.4899921417236328]}}
{"id": "WebQTest-2020", "question": "what are the mountains in northern italy called", "prediction": [["location.location.containedby"], ["location.location.partially_containedby"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the mountains in northern italy called [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.06895451992750168, -0.12367203831672668], "norm_scores": [0.5136759877204895, 0.4863240420818329]}}
{"id": "WebQTest-2021", "question": "what influenced john steinbeck to start writing", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "ground_paths": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what influenced john steinbeck to start writing [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.036247409880161285, -0.04612687975168228], "norm_scores": [0.5024698376655579, 0.49753016233444214]}}
{"id": "WebQTest-2022", "question": "where is dwayne johnson ethnicity", "prediction": [["people.ethnicity.people"], ["people.person.ethnicity"]], "ground_paths": [["people.ethnicity.people"], ["people.person.ethnicity"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is dwayne johnson ethnicity [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.people </PATH>", "<PATH> people.person.ethnicity </PATH>"], "scores": [-0.0895567387342453, -0.08956217765808105], "norm_scores": [0.5000013709068298, 0.49999865889549255]}}
{"id": "WebQTest-2023", "question": "where are the nato headquarters located", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"]], "ground_paths": [["organization.organization.founders", "base.biblioness.bibs_location.country"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the nato headquarters located [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>"], "scores": [-0.021732961758971214, -0.12516561150550842], "norm_scores": [0.5258351564407349, 0.4741649031639099]}}
{"id": "WebQTest-2024", "question": "in which state was the battle of antietam fought", "prediction": [["location.location.events"], ["time.event.locations"]], "ground_paths": [["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: in which state was the battle of antietam fought [/INST]", "raw_output": {"paths": ["<PATH> location.location.events </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.008702751249074936, -0.3065022826194763], "norm_scores": [0.5739044547080994, 0.42609548568725586]}}
{"id": "WebQTest-2025", "question": "which ocean does the zambezi river flow into", "prediction": [["geography.river.mouth"], ["location.location.containedby"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which ocean does the zambezi river flow into [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01419893093407154, -0.2747440040111542], "norm_scores": [0.5647703409194946, 0.43522971868515015]}}
{"id": "WebQTest-2027", "question": "what team did david beckham play for before la galaxy", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["soccer.football_player.loans", "soccer.football_player_loan.lending_team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did david beckham play for before la galaxy [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.0694478303194046, -0.09606906771659851], "norm_scores": [0.5066549181938171, 0.49334508180618286]}}
{"id": "WebQTest-2028", "question": "who is the current leader of france 2010", "prediction": [["people.person.nationality"], ["base.onephylogeny.type_of_thing.things_of_this_type", "people.person.profession"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the current leader of france 2010 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type <SEP> people.person.profession </PATH>"], "scores": [-0.0025548667181283236, -0.18176034092903137], "norm_scores": [0.5446818470954895, 0.4553181529045105]}}
{"id": "WebQTest-2029", "question": "where was the palace of knossos located", "prediction": [["location.location.containedby"], ["architecture.building.building_complex"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the palace of knossos located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> architecture.building.building_complex </PATH>"], "scores": [-0.004953619092702866, -0.40114378929138184], "norm_scores": [0.597771942615509, 0.40222805738449097]}}
{"id": "WebQTest-2030", "question": "where is roswell area 51", "prediction": [["aviation.airport.serves"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is roswell area 51 [/INST]", "raw_output": {"paths": ["<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.02181130461394787, -0.16651441156864166], "norm_scores": [0.5361127853393555, 0.46388718485832214]}}
{"id": "WebQTest-2031", "question": "when was father chris riley born", "prediction": [["people.person.place_of_birth"], ["people.person.place_of_birth", "location.location.people_born_here"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was father chris riley born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.people_born_here </PATH>"], "scores": [-0.006061112508177757, -0.25940945744514465], "norm_scores": [0.563000500202179, 0.4369995594024658]}}
