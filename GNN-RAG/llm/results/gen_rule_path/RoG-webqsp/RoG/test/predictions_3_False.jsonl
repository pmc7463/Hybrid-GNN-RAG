{"id": "WebQTest-0", "question": "what does jamaican people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does jamaican people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.008169983513653278, -0.023380741477012634, -0.05129494145512581], "norm_scores": [0.3398244082927704, 0.3346945643424988, 0.32548102736473083]}}
{"id": "WebQTest-1", "question": "what did james k polk do before he was president", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["people.person.profession", "base.onephylogeny.type_of_thing.things_of_this_type"], ["people.person.nationality", "government.government_office_or_title.jurisdiction"]], "ground_paths": [["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["people.person.profession", "base.onephylogeny.type_of_thing.things_of_this_type"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["people.person.nationality", "government.government_office_or_title.jurisdiction"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did james k polk do before he was president [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> people.person.profession <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> people.person.nationality <SEP> government.government_office_or_title.jurisdiction </PATH>"], "scores": [-0.01982685923576355, -0.022368159145116806, -0.028023742139339447], "norm_scores": [0.3345266580581665, 0.33367761969566345, 0.3317957818508148]}}
{"id": "WebQTest-3", "question": "who plays ken barlow in coronation street", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays ken barlow in coronation street [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.017513839527964592, -0.02660307288169861, -0.031673166900873184], "norm_scores": [0.3359207808971405, 0.3328813314437866, 0.3311978876590729]}}
{"id": "WebQTest-6", "question": "where is jamarcus russell from", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is jamarcus russell from [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.00789277721196413, -0.020250800997018814, -0.09155786037445068], "norm_scores": [0.34394314885139465, 0.3397188186645508, 0.3163380026817322]}}
{"id": "WebQTest-7", "question": "where was george washington carver from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was george washington carver from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.005064880941063166, -0.03688019514083862, -0.04015674069523811], "norm_scores": [0.3408081531524658, 0.33013588190078735, 0.3290559649467468]}}
{"id": "WebQTest-8", "question": "what else did ben franklin invent", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["law.inventor.us_patents"]], "ground_paths": [["law.invention.inventor"], ["law.inventor.inventions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what else did ben franklin invent [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.us_patents </PATH>"], "scores": [-0.006870923098176718, -0.025583401322364807, -0.06981831789016724], "norm_scores": [0.34241244196891785, 0.3360646665096283, 0.32152286171913147]}}
{"id": "WebQTest-9", "question": "who was richard nixon married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.parents"]], "ground_paths": [["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_owner.pets_owned"], ["people.person.children", "people.person.parents"], ["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_ownership.owner"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_ownership.owner"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was richard nixon married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>"], "scores": [-0.022450082004070282, -0.03371842950582504, -0.04671051353216171], "norm_scores": [0.3372878432273865, 0.33350852131843567, 0.3292035758495331]}}
{"id": "WebQTest-12", "question": "who is governor of ohio 2011", "prediction": [["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["location.location.containedby", "people.person.nationality"]], "ground_paths": [["government.political_district.representatives", "government.politician.government_positions_held"], ["base.locations.states_and_provences.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is governor of ohio 2011 [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>"], "scores": [-0.016713447868824005, -0.021680165082216263, -0.032739486545324326], "norm_scores": [0.3356665074825287, 0.3340035080909729, 0.330329954624176]}}
{"id": "WebQTest-13", "question": "who was vice president after kennedy died", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vice president after kennedy died [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.012039124965667725, -0.013505764305591583, -0.06658179312944412], "norm_scores": [0.3395064175128937, 0.3390088379383087, 0.3214847445487976]}}
{"id": "WebQTest-14", "question": "where is the fukushima daiichi nuclear plant located", "prediction": [["location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["location.location.street_address", "location.mailing_address.citytown"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the fukushima daiichi nuclear plant located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.street_address <SEP> location.mailing_address.citytown </PATH>"], "scores": [-0.009403283707797527, -0.02633478119969368, -0.03948435187339783], "norm_scores": [0.3385724127292633, 0.33288809657096863, 0.3285394608974457]}}
{"id": "WebQTest-16", "question": "what countries are part of the uk", "prediction": [["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["base.biblioness.bibs_location.country"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are part of the uk [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.02225334197282791, -0.024695932865142822, -0.031171897426247597], "norm_scores": [0.3345957398414612, 0.3337794542312622, 0.3316248953342438]}}
{"id": "WebQTest-19", "question": "what is my timezone in louisiana", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is my timezone in louisiana [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-1.2822571989090648e-05, -0.15877465903759003, -0.18302786350250244], "norm_scores": [0.3723070025444031, 0.31765216588974, 0.31004077196121216]}}
{"id": "WebQTest-20", "question": "what did st augustine do", "prediction": [["people.person.profession"], ["people.profession.people_with_this_profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did st augustine do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0019072163850069046, -0.05442636087536812, -0.06710191071033478], "norm_scores": [0.3465338349342346, 0.32880380749702454, 0.32466232776641846]}}
{"id": "WebQTest-21", "question": "who is keyshia cole dad", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.gender"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is keyshia cole dad [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.006339091341942549, -0.02584240399301052, -0.16772469878196716], "norm_scores": [0.3531510531902313, 0.3463301658630371, 0.30051878094673157]}}
{"id": "WebQTest-22", "question": "what kind government does egypt have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind government does egypt have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.005506351590156555, -0.02370516024529934, -0.13942156732082367], "norm_scores": [0.3500628173351288, 0.3437497019767761, 0.3061874508857727]}}
{"id": "WebQTest-23", "question": "what town was martin luther king assassinated in", "prediction": [["people.deceased_person.place_of_death"], ["base.biblioness.bibs_location.country"], ["location.location.events"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what town was martin luther king assassinated in [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.0016441080952063203, -0.06921697407960892, -0.07614307105541229], "norm_scores": [0.3493000864982605, 0.32647669315338135, 0.32422327995300293]}}
{"id": "WebQTest-24", "question": "where did edgar allan poe died", "prediction": [["people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["deceased_person.place_of_death"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did edgar allan poe died [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> deceased_person.place_of_death </PATH>"], "scores": [-0.00021406511950772256, -0.09098096936941147, -0.11157339066267014], "norm_scores": [0.35614463686943054, 0.32524216175079346, 0.3186131417751312]}}
{"id": "WebQTest-26", "question": "what to do today in atlanta with kids", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do today in atlanta with kids [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01532835140824318, -0.01630954071879387, -0.024478087201714516], "norm_scores": [0.3344581127166748, 0.33413007855415344, 0.33141186833381653]}}
{"id": "WebQTest-28", "question": "what electorate does anna bligh represent", "prediction": [["people.person.nationality", "base.biblioness.bibs_location.country"], ["government.politician.government_positions_held", "government.government_position_held.district_represented"], ["government.government_position_held.office_holder", "government.government_position_held.district_represented"]], "ground_paths": [["government.politician.government_positions_held", "government.political_district.representatives"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what electorate does anna bligh represent [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.district_represented </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.district_represented </PATH>"], "scores": [-0.014789795503020287, -0.01976146548986435, -0.025715461000800133], "norm_scores": [0.33510103821754456, 0.3334392011165619, 0.33145976066589355]}}
{"id": "WebQTest-31", "question": "where did andy murray started playing tennis", "prediction": [["people.person.nationality", "location.location.containedby"], ["sports.sports_facility.teams"], ["sports.sports_facility.athlete", "sports.sports_facility.location"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did andy murray started playing tennis [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_facility.athlete <SEP> sports.sports_facility.location </PATH>"], "scores": [-0.022947950288653374, -0.033995382487773895, -0.05852976441383362], "norm_scores": [0.3385174870491028, 0.33479830622673035, 0.3266841769218445]}}
{"id": "WebQTest-32", "question": "what is the australian dollar called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_used", "common.topic.notable_types", "common.topic.notable_types"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the australian dollar called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>"], "scores": [-0.007572351023554802, -0.029215095564723015, -0.0604548305273056], "norm_scores": [0.3416372537612915, 0.3343227207660675, 0.3240399956703186]}}
{"id": "WebQTest-33", "question": "what timezone is sweden", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is sweden [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.0005854463670402765, -0.061673350632190704, -0.15373800694942474], "norm_scores": [0.3573037087917328, 0.3361301124095917, 0.30656614899635315]}}
{"id": "WebQTest-34", "question": "who did cam newton sign with", "prediction": [["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did cam newton sign with [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.football_player.rushing <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.team </PATH>"], "scores": [-0.023365091532468796, -0.025090055540204048, -0.02513986825942993], "norm_scores": [0.3337222933769226, 0.3331471383571625, 0.3331305682659149]}}
{"id": "WebQTest-35", "question": "what county is frederick md in", "prediction": [["location.us_county.hud_county_place"], ["location.us_county.county_seat"], ["location.location.containedby"]], "ground_paths": [["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is frederick md in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.019464612007141113, -0.021238235756754875, -0.029885122552514076], "norm_scores": [0.3346875309944153, 0.33409443497657776, 0.33121803402900696]}}
{"id": "WebQTest-36", "question": "what highschool did harper lee go to", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what highschool did harper lee go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.012927158735692501, -0.019604869186878204, -0.026715097948908806], "norm_scores": [0.3356097638607025, 0.33337610960006714, 0.33101412653923035]}}
{"id": "WebQTest-37", "question": "what timezone is utah in", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is utah in [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-1.3335949006432202e-05, -0.14416958391666412, -0.19609177112579346], "norm_scores": [0.37206530570983887, 0.3221164643764496, 0.30581825971603394]}}
{"id": "WebQTest-38", "question": "who did george w. bush run against for the second term", "prediction": [["time.event.locations", "people.person.nationality"], ["common.topic.image", "common.image.appears_in_topic_gallery"], ["government.election.campaigns", "government.election_campaign.candidate"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did george w. bush run against for the second term [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations <SEP> people.person.nationality </PATH>", "<PATH> common.topic.image <SEP> common.image.appears_in_topic_gallery </PATH>", "<PATH> government.election.campaigns <SEP> government.election_campaign.candidate </PATH>"], "scores": [-0.012857574969530106, -0.022804725915193558, -0.023235347121953964], "norm_scores": [0.3355954587459564, 0.33227378129959106, 0.3321307599544525]}}
{"id": "WebQTest-39", "question": "who is niall ferguson 's wife", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["freebase.valuenotation.has_value", "freebase.valuenotation.is_reviewed"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.profession", "people.person.profession"], ["people.person.profession", "common.topic.notable_types"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is niall ferguson 's wife [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.027360938489437103, -0.029134191572666168, -0.03174254670739174], "norm_scores": [0.3340173661708832, 0.3334255814552307, 0.3325570225715637]}}
{"id": "WebQTest-41", "question": "where is the galapagos islands located on a world map", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.island.island_group"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the galapagos islands located on a world map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.island.island_group </PATH>"], "scores": [-0.00022960957721807063, -0.0874568447470665, -0.1066354364156723], "norm_scores": [0.3551730811595917, 0.3255050480365753, 0.31932181119918823]}}
{"id": "WebQTest-42", "question": "what are the songs that justin bieber wrote", "prediction": [["music.composer.compositions"], ["music.recording.artist"], ["music.composition.composer"]], "ground_paths": [["music.featured_artist.recordings"], ["music.composer.compositions"], ["music.lyricist.lyrics_written"], ["music.featured_artist.albums"], ["music.recording.featured_artists"], ["music.album.artist"], ["music.artist.album"], ["music.recording.artist"], ["music.composition.composer"], ["music.composition.lyricist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the songs that justin bieber wrote [/INST]", "raw_output": {"paths": ["<PATH> music.composer.compositions </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.composition.composer </PATH>"], "scores": [-0.01841508224606514, -0.021433662623167038, -0.03313110023736954], "norm_scores": [0.3353029191493988, 0.33429232239723206, 0.33040475845336914]}}
{"id": "WebQTest-43", "question": "what did george clemenceau do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did george clemenceau do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.003533089766278863, -0.04775182530283928, -0.05440439656376839], "norm_scores": [0.34398001432418823, 0.3291010558605194, 0.32691892981529236]}}
{"id": "WebQTest-44", "question": "what is the state flower of arizona", "prediction": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"]], "ground_paths": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the state flower of arizona [/INST]", "raw_output": {"paths": ["<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.012920164503157139, -0.01645599864423275, -0.024660101160407066], "norm_scores": [0.3350309133529663, 0.33384838700294495, 0.33112069964408875]}}
{"id": "WebQTest-45", "question": "where was rihanna born and raised", "prediction": [["location.location.people_born_here"], ["music.artist.origin"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was rihanna born and raised [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.007295835763216019, -0.02665909379720688, -0.05517655611038208], "norm_scores": [0.3408234417438507, 0.33428746461868286, 0.32488909363746643]}}
{"id": "WebQTest-46", "question": "what year did president william henry harrison take office", "prediction": [["government.election_campaign.candidate", "government.election_campaign.election"], ["government.election_campaign.candidate", "time.event.instance_of_recurring_event"], ["people.person.nationality", "location.location.events"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did president william henry harrison take office [/INST]", "raw_output": {"paths": ["<PATH> government.election_campaign.candidate <SEP> government.election_campaign.election </PATH>", "<PATH> government.election_campaign.candidate <SEP> time.event.instance_of_recurring_event </PATH>", "<PATH> people.person.nationality <SEP> location.location.events </PATH>"], "scores": [-0.019344216212630272, -0.03594460338354111, -0.03722300007939339], "norm_scores": [0.33717527985572815, 0.33162423968315125, 0.33120056986808777]}}
{"id": "WebQTest-47", "question": "where george lopez was born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where george lopez was born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.008121891878545284, -0.02050759270787239, -0.0933556780219078], "norm_scores": [0.34411701560020447, 0.33988115191459656, 0.316001832485199]}}
{"id": "WebQTest-48", "question": "where are samsung based", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.place_founded"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are samsung based [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.place_founded </PATH>"], "scores": [-0.008791266940534115, -0.032415442168712616, -0.03926679864525795], "norm_scores": [0.3393699526786804, 0.3314465582370758, 0.3291834592819214]}}
{"id": "WebQTest-49", "question": "what did the islamic people believe in", "prediction": [["religion.belief.belief_of"], ["religion.religion.beliefs"], ["religion.deity.deity_of"]], "ground_paths": [["religion.religion.beliefs"], ["religion.belief.belief_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did the islamic people believe in [/INST]", "raw_output": {"paths": ["<PATH> religion.belief.belief_of </PATH>", "<PATH> religion.religion.beliefs </PATH>", "<PATH> religion.deity.deity_of </PATH>"], "scores": [-0.008485506288707256, -0.0173153355717659, -0.10753760486841202], "norm_scores": [0.34519606828689575, 0.3421614468097687, 0.31264251470565796]}}
{"id": "WebQTest-51", "question": "who will play mr gray in the film", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who will play mr gray in the film [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>"], "scores": [-0.02211715467274189, -0.02225426770746708, -0.02942507527768612], "norm_scores": [0.33415964245796204, 0.33411383628845215, 0.3317265212535858]}}
{"id": "WebQTest-52", "question": "what did george orwell died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did george orwell died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>"], "scores": [-0.0016105264658108354, -0.043308649212121964, -0.12572380900382996], "norm_scores": [0.3518105149269104, 0.3374423384666443, 0.3107471168041229]}}
{"id": "WebQTest-54", "question": "what country did adolf hitler control", "prediction": [["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["base.culturalevent.event.entity_involved", "location.location.events"]], "ground_paths": [["government.government_position_held.office_holder", "government.governmental_jurisdiction.governing_officials"], ["base.crime.crime.victim_s", "time.event.locations"], ["government.political_appointer.appointees", "government.governmental_jurisdiction.governing_officials"], ["people.deceased_person.place_of_death", "location.country.capital"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["military.military_command.military_commander", "military.military_combatant.military_commanders"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["organization.organization.founders", "organization.organization.place_founded"], ["military.military_commander.military_commands", "military.military_command.military_combatant"], ["organization.organization_founder.organizations_founded", "organization.organization.place_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did adolf hitler control [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>"], "scores": [-0.026680730283260345, -0.028017990291118622, -0.030145036056637764], "norm_scores": [0.3338669240474701, 0.3334207534790039, 0.3327122926712036]}}
{"id": "WebQTest-55", "question": "who is michael buble", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is michael buble [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.00031427317298948765, -0.08193898946046829, -0.13626185059547424], "norm_scores": [0.3578449785709381, 0.32979628443717957, 0.3123587667942047]}}
{"id": "WebQTest-56", "question": "what county is kansas city kansas", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"]], "ground_paths": [["location.location.containedby"], ["location.hud_county_place.county"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is kansas city kansas [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.017883505672216415, -0.022478250786662102, -0.03785140812397003], "norm_scores": [0.3360615074634552, 0.3345209062099457, 0.3294175863265991]}}
{"id": "WebQTest-58", "question": "who did jackie robinson first play for", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did jackie robinson first play for [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.014011714607477188, -0.016965853050351143, -0.03325724974274635], "norm_scores": [0.33579710125923157, 0.3348065912723541, 0.3293962776660919]}}
{"id": "WebQTest-59", "question": "when did annie open", "prediction": [["theater.theater_character.plays_appears_in"], ["theater.play.characters"], ["film.film.directed_by"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did annie open [/INST]", "raw_output": {"paths": ["<PATH> theater.theater_character.plays_appears_in </PATH>", "<PATH> theater.play.characters </PATH>", "<PATH> film.film.directed_by </PATH>"], "scores": [-0.031006189063191414, -0.04268433526158333, -0.04369080439209938], "norm_scores": [0.3360457420349121, 0.332144170999527, 0.3318100869655609]}}
{"id": "WebQTest-60", "question": "where did eleanor roosevelt die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here"], ["people.person.place_of_birth"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did eleanor roosevelt die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.0015002250438556075, -0.0673714354634285, -0.07969331741333008], "norm_scores": [0.3495235741138458, 0.3272419571876526, 0.3232344686985016]}}
{"id": "WebQTest-61", "question": "what are the religions practiced in indonesia", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["royalty.kingdom.rulers", "people.person.religion"]], "ground_paths": [["book.book_subject.works", "book.written_work.subjects"], ["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the religions practiced in indonesia [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> royalty.kingdom.rulers <SEP> people.person.religion </PATH>"], "scores": [-0.006135435774922371, -0.017191385850310326, -0.07292161881923676], "norm_scores": [0.3419504761695862, 0.3381907045841217, 0.3198588192462921]}}
{"id": "WebQTest-62", "question": "what was jesse james killed with", "prediction": [["people.deceased_person.cause_of_death"], ["people.deceased_person.place_of_death"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was jesse james killed with [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.0014828850980848074, -0.06021560728549957, -0.07107868045568466], "norm_scores": [0.3477378487586975, 0.3279024660587311, 0.3243596851825714]}}
{"id": "WebQTest-63", "question": "who was vp for lincoln", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["government.political_party_tenure.politician", "government.political_party_tenure.politician"]], "ground_paths": [["government.us_vice_president.to_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vp for lincoln [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.politician </PATH>"], "scores": [-0.007773614954203367, -0.01862938515841961, -0.0976775735616684], "norm_scores": [0.3444449007511139, 0.34072592854499817, 0.3148292005062103]}}
{"id": "WebQTest-64", "question": "what other books did charles dickens write", "prediction": [["book.written_work.author"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["fictional_universe.fictional_universe_creator.fictional_universes_created"]], "ground_paths": [["fictional_universe.fictional_character_creator.fictional_characters_created"], ["tv.tv_program_creator.programs_created"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other books did charles dickens write [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> fictional_universe.fictional_universe_creator.fictional_universes_created </PATH>"], "scores": [-0.005767088383436203, -0.02915782667696476, -0.047976087778806686], "norm_scores": [0.34065163135528564, 0.3327760100364685, 0.3265722990036011]}}
{"id": "WebQTest-65", "question": "who was the first dictator of the soviet union", "prediction": [["people.person.nationality"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the first dictator of the soviet union [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.015904245898127556, -0.019478516653180122, -0.032802533358335495], "norm_scores": [0.33560699224472046, 0.33440959453582764, 0.3299834430217743]}}
{"id": "WebQTest-66", "question": "what is the currency of puerto rico called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency of puerto rico called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.0005753846489824355, -0.061945509165525436, -0.13355198502540588], "norm_scores": [0.3551185727119446, 0.33398017287254333, 0.3109011948108673]}}
{"id": "WebQTest-67", "question": "what kind of cancer did carl wilson have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of cancer did carl wilson have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.012507960200309753, -0.015852298587560654, -0.03528222441673279], "norm_scores": [0.33623117208480835, 0.33510857820510864, 0.3286602795124054]}}
{"id": "WebQTest-68", "question": "who plays the voice of kitt in knight rider", "prediction": [["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays the voice of kitt in knight rider [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.016011018306016922, -0.021054312586784363, -0.021515192463994026], "norm_scores": [0.3345063030719757, 0.3328235447406769, 0.3326701819896698]}}
{"id": "WebQTest-69", "question": "what county is brentwood tennessee in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"]], "ground_paths": [["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is brentwood tennessee in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.014681712724268436, -0.02310974895954132, -0.03549085557460785], "norm_scores": [0.33658549189567566, 0.33376064896583557, 0.3296538293361664]}}
{"id": "WebQTest-71", "question": "who did michael j fox marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.person.spouse_s"]], "ground_paths": [["award.award_winner.awards_won", "award.award_honor.award_winner"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["people.person.spouse_s", "people.marriage.spouse"], ["common.image.appears_in_topic_gallery", "common.topic.image"], ["people.person.profession", "people.person.profession"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did michael j fox marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>"], "scores": [-0.02555854432284832, -0.027512194588780403, -0.040594592690467834], "norm_scores": [0.33521896600723267, 0.3345647156238556, 0.33021628856658936]}}
{"id": "WebQTest-72", "question": "what battles did stonewall jackson fight in", "prediction": [["base.culturalevent.event.entity_involved"], ["military.military_person.participated_in_conflicts"], ["military.military_conflict.military_personnel_involved"]], "ground_paths": [["base.culturalevent.event.entity_involved"], ["military.military_conflict.military_personnel_involved"], ["common.topic.image"], ["military.military_commander.military_commands", "military.military_conflict.commanders"], ["people.person.nationality", "book.written_work.subjects"], ["military.military_person.participated_in_conflicts"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what battles did stonewall jackson fight in [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>"], "scores": [-0.011233357712626457, -0.029489479959011078, -0.030163224786520004], "norm_scores": [0.33747783303260803, 0.3313726782798767, 0.33114951848983765]}}
{"id": "WebQTest-73", "question": "what language do the maasai tribe speak", "prediction": [["people.ethnicity.languages_spoken"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do the maasai tribe speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.0006501621101051569, -0.08077727258205414, -0.09000984579324722], "norm_scores": [0.35242095589637756, 0.3252841830253601, 0.32229480147361755]}}
{"id": "WebQTest-74", "question": "who did benjamin franklin get married to", "prediction": [["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.spouse_s", "people.person.spouse_s"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.place_of_interment.interred_here", "people.place_of_interment.interred_here"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did benjamin franklin get married to [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>"], "scores": [-0.0283953920006752, -0.03077104315161705, -0.036855410784482956], "norm_scores": [0.3345373272895813, 0.33374351263046265, 0.3317190706729889]}}
{"id": "WebQTest-75", "question": "what disease did patrick swayze died from", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"], ["medicine.notable_person_with_medical_condition.condition"]], "ground_paths": [["medicine.disease.notable_people_with_this_condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what disease did patrick swayze died from [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>"], "scores": [-0.005183560308068991, -0.027568744495511055, -0.03870582580566406], "norm_scores": [0.33957046270370483, 0.3320535719394684, 0.3283759355545044]}}
{"id": "WebQTest-76", "question": "what works of art did leonardo da vinci produce", "prediction": [["visual_art.visual_artist.art_forms"], ["film.film.subjects"], ["visual_art.art_form.artists"]], "ground_paths": [["visual_art.visual_artist.artworks"], ["visual_art.artwork.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what works of art did leonardo da vinci produce [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> film.film.subjects </PATH>", "<PATH> visual_art.art_form.artists </PATH>"], "scores": [-0.0008100288105197251, -0.07548801600933075, -0.08457090705633163], "norm_scores": [0.351161390542984, 0.3258926272392273, 0.3229459822177887]}}
{"id": "WebQTest-77", "question": "what capital of austria", "prediction": [["location.country.capital"], ["location.location.containedby"], ["location.administrative_division.first_level_division_of"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what capital of austria [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.administrative_division.first_level_division_of </PATH>"], "scores": [-0.006871411576867104, -0.038881585001945496, -0.06845074892044067], "norm_scores": [0.3437873125076294, 0.3329569101333618, 0.3232557773590088]}}
{"id": "WebQTest-78", "question": "what type of cancer did eva peron have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of cancer did eva peron have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.007935131900012493, -0.027487508952617645, -0.03853307664394379], "norm_scores": [0.3389252722263336, 0.3323628604412079, 0.32871192693710327]}}
{"id": "WebQTest-79", "question": "what country did buddha come from", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did buddha come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.006731961388140917, -0.03340638428926468, -0.048447635024785995], "norm_scores": [0.34096869826316833, 0.33199378848075867, 0.3270375430583954]}}
{"id": "WebQTest-80", "question": "what county is greeley colorado in", "prediction": [["location.us_county.county_seat"], ["location.us_county.hud_county_place"], ["location.location.containedby"]], "ground_paths": [["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is greeley colorado in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.017905035987496376, -0.024600379168987274, -0.026586662977933884], "norm_scores": [0.3350439667701721, 0.3328082263469696, 0.33214783668518066]}}
{"id": "WebQTest-82", "question": "what did peter tchaikovsky do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"], ["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did peter tchaikovsky do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0008780090138316154, -0.06982765346765518, -0.07292553037405014], "norm_scores": [0.3491790294647217, 0.32591453194618225, 0.32490643858909607]}}
{"id": "WebQTest-83", "question": "which countries speak german officially", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries speak german officially [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.018098443746566772, -0.020812595263123512, -0.024059589952230453], "norm_scores": [0.33429765701293945, 0.33339157700538635, 0.3323107957839966]}}
{"id": "WebQTest-84", "question": "what type of music did john lennon sing", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did john lennon sing [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.001199332415126264, -0.05505063012242317, -0.11371911317110062], "norm_scores": [0.35196980834007263, 0.33351707458496094, 0.31451308727264404]}}
{"id": "WebQTest-85", "question": "who are the colorado representatives", "prediction": [["book.written_work.subjects", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"]], "ground_paths": [["government.election.district", "government.election.winner"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the colorado representatives [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.02510662190616131, -0.027034681290388107, -0.028277773410081863], "norm_scores": [0.3339000940322876, 0.33325693011283875, 0.33284294605255127]}}
{"id": "WebQTest-86", "question": "which country does greenland belong to", "prediction": [["location.location.containedby"], ["location.administrative_division.country"], ["location.country.administrative_divisions"]], "ground_paths": [["location.country.administrative_divisions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country does greenland belong to [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.country.administrative_divisions </PATH>"], "scores": [-0.017357613891363144, -0.03387196362018585, -0.038570303469896317], "norm_scores": [0.3375377058982849, 0.33200928568840027, 0.3304530382156372]}}
{"id": "WebQTest-87", "question": "what the zip code for seattle washington", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"], ["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the zip code for seattle washington [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00016005853831302375, -0.09007110446691513, -0.15452821552753448], "norm_scores": [0.3608846068382263, 0.32985302805900574, 0.30926239490509033]}}
{"id": "WebQTest-88", "question": "who is willow smith mom name", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.parent"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is willow smith mom name [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.007840246893465519, -0.022686287760734558, -0.12793219089508057], "norm_scores": [0.3481769859790802, 0.3430461287498474, 0.3087769150733948]}}
{"id": "WebQTest-89", "question": "what do you call the chinese writing system", "prediction": [["language.human_language.writing_system"], ["language.language_writing_system.writing_systems"], ["language.language_writing_system.writing_system"]], "ground_paths": [["language.language_writing_system.languages"], ["language.human_language.writing_system"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do you call the chinese writing system [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.writing_system </PATH>", "<PATH> language.language_writing_system.writing_systems </PATH>", "<PATH> language.language_writing_system.writing_system </PATH>"], "scores": [-0.008031235076487064, -0.04452376440167427, -0.053100135177373886], "norm_scores": [0.3424544036388397, 0.33018264174461365, 0.327362984418869]}}
{"id": "WebQTest-90", "question": "who played on the jeffersons", "prediction": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "ground_paths": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["tv.tv_program.regular_cast", "tv.tv_actor.starring_roles"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["award.award_winning_work.awards_won", "award.award_honor.award_winner"], ["award.award_honor.honored_for", "award.award_winner.awards_won"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_honor.honored_for", "award.award_honor.award_winner"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played on the jeffersons [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.023896291851997375, -0.024990055710077286, -0.028630660846829414], "norm_scores": [0.33398085832595825, 0.333615779876709, 0.33240342140197754]}}
{"id": "WebQTest-91", "question": "what is the name of the san francisco newspaper", "prediction": [["book.newspaper.circulation_areas"], ["periodicals.newspaper_circulation_area.newspapers"], ["location.location.containedby"]], "ground_paths": [["periodicals.newspaper_circulation_area.newspapers"], ["book.newspaper.circulation_areas"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the san francisco newspaper [/INST]", "raw_output": {"paths": ["<PATH> book.newspaper.circulation_areas </PATH>", "<PATH> periodicals.newspaper_circulation_area.newspapers </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0031227802392095327, -0.02998422645032406, -0.09296268969774246], "norm_scores": [0.3463115096092224, 0.33713290095329285, 0.31655555963516235]}}
{"id": "WebQTest-93", "question": "what continent does armenia belong to", "prediction": [["base.locations.countries.continent"], ["location.location.containedby"], ["base.locations.continents.countries_within"]], "ground_paths": [["base.locations.countries.continent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what continent does armenia belong to [/INST]", "raw_output": {"paths": ["<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.015073065645992756, -0.0244035292416811, -0.0286151934415102], "norm_scores": [0.33587905764579773, 0.3327597379684448, 0.33136120438575745]}}
{"id": "WebQTest-94", "question": "what did randy savage died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did randy savage died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0008968505426310003, -0.0542665533721447, -0.13143661618232727], "norm_scores": [0.3539007306098938, 0.3355083167552948, 0.310590922832489]}}
{"id": "WebQTest-95", "question": "what was the title of the book charles darwin wrote", "prediction": [["book.written_work.author"], ["common.topic.notable_types"], ["book.book_edition.author_editor"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], [], ["exhibitions.exhibition_subject.exhibitions_created_about_this_subject"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the title of the book charles darwin wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.001450918847694993, -0.07530374825000763, -0.07550591975450516], "norm_scores": [0.3499649465084076, 0.32505038380622864, 0.32498466968536377]}}
{"id": "WebQTest-96", "question": "where did richard nixon die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["people.place_of_death.people"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did richard nixon die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people </PATH>"], "scores": [-0.0005508854519575834, -0.0778302326798439, -0.09878334403038025], "norm_scores": [0.35309869050979614, 0.3268391788005829, 0.32006213068962097]}}
{"id": "WebQTest-97", "question": "what songs does smokey robinson sing", "prediction": [["music.recording.artist"], ["music.artist.track"], ["music.composition.composer"]], "ground_paths": [["music.featured_artist.recordings"], ["music.lyricist.lyrics_written"], ["music.recording.featured_artists"], ["music.recording.producer"], ["music.album.artist"], ["music.producer.releases_produced"], ["music.artist.album"], ["music.recording.artist"], ["music.composition.lyricist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what songs does smokey robinson sing [/INST]", "raw_output": {"paths": ["<PATH> music.recording.artist </PATH>", "<PATH> music.artist.track </PATH>", "<PATH> music.composition.composer </PATH>"], "scores": [-0.014359443448483944, -0.037200454622507095, -0.04739663004875183], "norm_scores": [0.33956775069236755, 0.3318995535373688, 0.3285326659679413]}}
{"id": "WebQTest-98", "question": "who was vice president when jfk was president", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vice president when jfk was president [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.010317256674170494, -0.014564190991222858, -0.08027155697345734], "norm_scores": [0.3415068984031677, 0.3400596082210541, 0.3184334933757782]}}
{"id": "WebQTest-99", "question": "what is serbian language called", "prediction": [["language.human_language.countries_spoken_in"], ["language.human_language.language_family"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is serbian language called [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.017688481137156487, -0.025959786027669907, -0.035875916481018066], "norm_scores": [0.3362768888473511, 0.33350691199302673, 0.3302161693572998]}}
{"id": "WebQTest-100", "question": "what language is spoken in haiti today", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is spoken in haiti today [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.011672488413751125, -0.016996538266539574, -0.05151591822504997], "norm_scores": [0.33833733201026917, 0.3365408182144165, 0.32512184977531433]}}
{"id": "WebQTest-102", "question": "who played barbara gordon batgirl", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played barbara gordon batgirl [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.01147484965622425, -0.03228210657835007, -0.03931250795722008], "norm_scores": [0.3387586772441864, 0.33178287744522095, 0.32945847511291504]}}
{"id": "WebQTest-103", "question": "what basketball teams has shaq played for", "prediction": [["sports.sports_award_winner.awards", "sports.sports_award.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"]], "ground_paths": [["sports.sports_award.award_winner", "sports.sports_award.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what basketball teams has shaq played for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.0213689673691988, -0.021570084616541862, -0.02599388360977173], "norm_scores": [0.33386921882629395, 0.33380210399627686, 0.3323286771774292]}}
{"id": "WebQTest-104", "question": "where is jay leno from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is jay leno from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.004545723553746939, -0.034207768738269806, -0.05989309027791023], "norm_scores": [0.34282615780830383, 0.3328065872192383, 0.3243671953678131]}}
{"id": "WebQTest-105", "question": "what language do people from thailand speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people from thailand speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.01563320867717266, -0.019285472109913826, -0.02923227660357952], "norm_scores": [0.3352501392364502, 0.3340279459953308, 0.330721914768219]}}
{"id": "WebQTest-106", "question": "who did armie hammer play in the social network", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.performance.character"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did armie hammer play in the social network [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>"], "scores": [-0.01896890439093113, -0.02693158946931362, -0.027963735163211823], "norm_scores": [0.3352201282978058, 0.3325614631175995, 0.33221837878227234]}}
{"id": "WebQTest-107", "question": "what was thomas jefferson role in the declaration of independence", "prediction": [["book.written_work.subjects", "people.person.nationality"], ["book.author.book_editions_published"], ["book.written_work.subjects"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was thomas jefferson role in the declaration of independence [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> people.person.nationality </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.029804430902004242, -0.03278932720422745, -0.03361533582210541], "norm_scores": [0.33408883213996887, 0.3330931067466736, 0.33281809091567993]}}
{"id": "WebQTest-108", "question": "what was the book written by charles darwin", "prediction": [["book.written_work.subjects"], ["book.written_work.author"], ["book.book_subject.works"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], [], ["exhibitions.exhibition_subject.exhibitions_created_about_this_subject"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the book written by charles darwin [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.book_subject.works </PATH>"], "scores": [-0.013645513914525509, -0.01765812747180462, -0.056614212691783905], "norm_scores": [0.3385312855243683, 0.3371756076812744, 0.3242931365966797]}}
{"id": "WebQTest-109", "question": "who did tim tebow play college football for", "prediction": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did tim tebow play college football for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.01602143980562687, -0.022482411935925484, -0.028061609715223312], "norm_scores": [0.3353913426399231, 0.3332313597202301, 0.3313773572444916]}}
{"id": "WebQTest-110", "question": "who does peyton manning play football for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does peyton manning play football for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.009651829488575459, -0.021166859194636345, -0.03111758828163147], "norm_scores": [0.337005078792572, 0.33314675092697144, 0.32984814047813416]}}
{"id": "WebQTest-111", "question": "where is the carpathian mountain range located", "prediction": [["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.partially_contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the carpathian mountain range located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.016155382618308067, -0.01681891456246376, -0.037696488201618195], "norm_scores": [0.3357929289340973, 0.33557018637657166, 0.32863691449165344]}}
{"id": "WebQTest-112", "question": "what was robert burns", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["influence.influence_node.influenced_by", "people.person.profession"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was robert burns [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>"], "scores": [-0.0037678987719118595, -0.04387333244085312, -0.06071978434920311], "norm_scores": [0.34419527649879456, 0.33066433668136597, 0.32514044642448425]}}
{"id": "WebQTest-114", "question": "who did the voice of darth vader in episode 3", "prediction": [["film.performance.character", "film.performance.actor"], ["film.performance.special_performance_type", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did the voice of darth vader in episode 3 [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.027253376320004463, -0.03162955120205879, -0.03361906856298447], "norm_scores": [0.334527850151062, 0.3330670893192291, 0.33240512013435364]}}
{"id": "WebQTest-115", "question": "what super bowl did peyton manning win", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship.events"]], "ground_paths": [["tv.tv_actor.guest_roles", "tv.tv_guest_role.episodes_appeared_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what super bowl did peyton manning win [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.005588465835899115, -0.03774517774581909, -0.05495491996407509], "norm_scores": [0.3424437940120697, 0.3316071331501007, 0.3259490430355072]}}
{"id": "WebQTest-116", "question": "which country was justin bieber born in", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country was justin bieber born in [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0072301472537219524, -0.02659263089299202, -0.05423547700047493], "norm_scores": [0.34072640538215637, 0.33419257402420044, 0.3250810503959656]}}
{"id": "WebQTest-118", "question": "who fought in the gulf war 1991", "prediction": [["military.military_combatant_group.conflict", "military.military_combatant.military_conflicts"], ["military.military_combatant_group.conflict", "military.military_combatant_group.combatants"], ["time.event.includes_event", "base.culturalevent.event.entity_involved"]], "ground_paths": [["military.military_conflict.combatants", "military.military_combatant_group.combatants"], ["military.military_person.participated_in_conflicts", "people.person.nationality"], ["base.culturalevent.event.entity_involved", "people.person.nationality"], ["book.book_subject.works", "media_common.netflix_title.netflix_genres"], ["film.film.subjects", "film.film.country"], ["film.film_subject.films", "tv.tv_program.country_of_origin"], ["film.film_genre.films_in_this_genre", "film.film.country"], ["time.event.locations"], ["film.film_subject.films", "film.film.country"], ["film.film.genre", "film.film.country"], ["military.casualties.military_conflict", "military.military_combatant.casualties"], ["base.culturalevent.event.entity_involved", "people.person.place_of_birth"], ["military.military_conflict.casualties", "military.military_combatant.casualties"], ["time.event.included_in_event", "base.culturalevent.event.entity_involved"], ["time.event.includes_event", "base.culturalevent.event.entity_involved"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who fought in the gulf war 1991 [/INST]", "raw_output": {"paths": ["<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant.military_conflicts </PATH>", "<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant_group.combatants </PATH>", "<PATH> time.event.includes_event <SEP> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.0202191025018692, -0.02916017733514309, -0.032641202211380005], "norm_scores": [0.33571088314056396, 0.33272263407707214, 0.3315664529800415]}}
{"id": "WebQTest-119", "question": "who plays london tipton in suite life on deck", "prediction": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "ground_paths": [["tv.tv_program.genre", "music.artist.genre"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays london tipton in suite life on deck [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>"], "scores": [-0.015279893763363361, -0.029832592234015465, -0.03263681381940842], "norm_scores": [0.33688798546791077, 0.3320208489894867, 0.33109110593795776]}}
{"id": "WebQTest-121", "question": "what team did david beckham play for in 2011", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"]], "ground_paths": [["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player_loan.player", "soccer.football_player_loan.lending_team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["book.written_work.subjects", "book.written_work.subjects"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did david beckham play for in 2011 [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>"], "scores": [-0.02218308299779892, -0.024897688999772072, -0.028851447626948357], "norm_scores": [0.3343762457370758, 0.33346977829933167, 0.33215394616127014]}}
{"id": "WebQTest-122", "question": "where did francisco coronado come from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did francisco coronado come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0075583248399198055, -0.02244967594742775, -0.06633603572845459], "norm_scores": [0.3415142595767975, 0.33646634221076965, 0.32201939821243286]}}
{"id": "WebQTest-123", "question": "what all does google now do", "prediction": [["organization.organization.sectors"], ["organization.organization.founders"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.sectors"]], "ground_paths": [["internet.website.owner"], ["computer.computer_manufacturer_brand.computer_models"], ["computer.software.developer"], ["business.consumer_company.products"], ["business.company_product_relationship.company", "business.consumer_product.company"], ["computer.software_developer.software"], ["projects.project_participation.participant", "projects.project_participation.project"], ["business.consumer_company.products", "business.consumer_product.company"], ["business.company_product_relationship.company", "business.company_product_relationship.consumer_product"], ["business.consumer_company.products", "business.company_product_relationship.consumer_product"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what all does google now do [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.sectors </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization.sectors </PATH>"], "scores": [-0.04268370941281319, -0.04597026854753494, -0.05339599400758743], "norm_scores": [0.3348890244960785, 0.33379021286964417, 0.33132079243659973]}}
{"id": "WebQTest-124", "question": "what language does egyptian people speak", "prediction": [["location.country.languages_spoken"], ["people.ethnicity.languages_spoken"], ["people.ethnicity.people", "people.person.nationality", "language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does egyptian people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.03664633259177208, -0.04042031988501549, -0.04048486426472664], "norm_scores": [0.33417969942092896, 0.3329208791255951, 0.33289939165115356]}}
{"id": "WebQTest-125", "question": "what are abraham sons names", "prediction": [["people.person.children"], ["people.person.parents"], ["fictional_universe.fictional_character.parents"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are abraham sons names [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> fictional_universe.fictional_character.parents </PATH>"], "scores": [-0.005858867894858122, -0.03029973804950714, -0.09311235696077347], "norm_scores": [0.34574559330940247, 0.33739766478538513, 0.31685671210289]}}
{"id": "WebQTest-126", "question": "who wrote the jana gana mana", "prediction": [["music.composition.composer"], ["music.composition.lyricist"], ["music.composer.compositions"]], "ground_paths": [["music.composition.composer"], ["music.lyricist.lyrics_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who wrote the jana gana mana [/INST]", "raw_output": {"paths": ["<PATH> music.composition.composer </PATH>", "<PATH> music.composition.lyricist </PATH>", "<PATH> music.composer.compositions </PATH>"], "scores": [-0.00821242667734623, -0.0288979671895504, -0.04320451617240906], "norm_scores": [0.339542418718338, 0.3325909376144409, 0.3278665840625763]}}
{"id": "WebQTest-127", "question": "what type of cancer did gilda radner die of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["medicine.disease.notable_people_with_this_condition"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of cancer did gilda radner die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>"], "scores": [-0.008225975558161736, -0.026470527052879333, -0.02836647629737854], "norm_scores": [0.3376117944717407, 0.331508070230484, 0.33088013529777527]}}
{"id": "WebQTest-128", "question": "who plays juni cortez", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays juni cortez [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.020487003028392792, -0.029388895258307457, -0.03253107890486717], "norm_scores": [0.33566442131996155, 0.33268964290618896, 0.3316459059715271]}}
{"id": "WebQTest-129", "question": "what was lucille ball", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was lucille ball [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0015136293368414044, -0.06352511048316956, -0.07349692285060883], "norm_scores": [0.34838125109672546, 0.32743382453918457, 0.32418492436408997]}}
{"id": "WebQTest-131", "question": "who inspired obama", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["people.person.profession"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who inspired obama [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.01007083710283041, -0.01599705033004284, -0.11707364767789841], "norm_scores": [0.3457080125808716, 0.34366536140441895, 0.31062665581703186]}}
{"id": "WebQTest-132", "question": "who is michael j fox wife", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.parents"]], "ground_paths": [["award.award_winner.awards_won", "award.award_honor.award_winner"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["people.person.spouse_s", "people.marriage.spouse"], ["common.image.appears_in_topic_gallery", "common.topic.image"], ["people.person.profession", "people.person.profession"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is michael j fox wife [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>"], "scores": [-0.022272570058703423, -0.036349277943372726, -0.044770050793886185], "norm_scores": [0.3374074697494507, 0.33269116282463074, 0.32990139722824097]}}
{"id": "WebQTest-133", "question": "where did dolly parton grow up", "prediction": [["music.artist.origin"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did dolly parton grow up [/INST]", "raw_output": {"paths": ["<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.008673655800521374, -0.0315319187939167, -0.032895807176828384], "norm_scores": [0.3385848104953766, 0.3309331238269806, 0.3304820656776428]}}
{"id": "WebQTest-134", "question": "what type of books did agatha christie wrote", "prediction": [["book.book_edition.author_editor"], ["book.written_work.author"], ["book.author.works_written"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of books did agatha christie wrote [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.010759481228888035, -0.021795829758048058, -0.04109941050410271], "norm_scores": [0.33793601393699646, 0.33422693610191345, 0.3278370499610901]}}
{"id": "WebQTest-138", "question": "what are the four main languages spoken in spain", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the four main languages spoken in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.017322516068816185, -0.018489569425582886, -0.024928277358412743], "norm_scores": [0.33430764079093933, 0.3339177370071411, 0.33177462220191956]}}
{"id": "WebQTest-139", "question": "where is the nra headquarters located", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the nra headquarters located [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>"], "scores": [-0.0036431988701224327, -0.03799644485116005, -0.0508316308259964], "norm_scores": [0.3424495756626129, 0.330885112285614, 0.3266652822494507]}}
{"id": "WebQTest-141", "question": "who is gimli 's father in the hobbit", "prediction": [["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.parents"], ["people.person.gender"]], "ground_paths": [["fictional_universe.fictional_character.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is gimli 's father in the hobbit [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> fictional_universe.fictional_character.parents </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.029427003115415573, -0.03026490844786167, -0.03959530591964722], "norm_scores": [0.3345549404621124, 0.3342747390270233, 0.33117035031318665]}}
{"id": "WebQTest-142", "question": "who developed the tcp ip reference model", "prediction": [["law.invention.inventor"], ["base.biblioness.bibs_location.country", "people.person.nationality"], ["internet.website.owner", "organization.organization_founder.organizations_founded"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], ["base.argumentmaps.original_idea.innovator"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who developed the tcp ip reference model [/INST]", "raw_output": {"paths": ["<PATH> law.invention.inventor </PATH>", "<PATH> base.biblioness.bibs_location.country <SEP> people.person.nationality </PATH>", "<PATH> internet.website.owner <SEP> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.01059224084019661, -0.048448242247104645, -0.06777021288871765], "norm_scores": [0.3439643979072571, 0.3311866819858551, 0.3248489201068878]}}
{"id": "WebQTest-145", "question": "what college did albert einstein go to", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["business.employment_tenure.person", "business.employer.employees"], ["people.person.nationality", "location.location.containedby"], ["people.person.employment_history", "business.employer.employees"], ["people.person.nationality", "organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did albert einstein go to [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.020962843671441078, -0.022282816469669342, -0.0232120119035244], "norm_scores": [0.333730012178421, 0.33328977227211, 0.3329802453517914]}}
{"id": "WebQTest-146", "question": "what team does jordan own", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team_owner.teams_owned"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_team.location"], ["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does jordan own [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team_owner.teams_owned </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.003536892356351018, -0.04408399015665054, -0.04918849840760231], "norm_scores": [0.34297803044319153, 0.32934942841529846, 0.32767254114151]}}
{"id": "WebQTest-147", "question": "who is the state governor of tennessee", "prediction": [["location.administrative_division.first_level_division_of", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["base.locations.countries.states_provinces_within", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the state governor of tennessee [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.first_level_division_of <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.02076607011258602, -0.025172440335154533, -0.028031524270772934], "norm_scores": [0.3346312344074249, 0.3331599533557892, 0.3322088122367859]}}
{"id": "WebQTest-149", "question": "what do portuguese people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do portuguese people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01295712124556303, -0.02252287231385708, -0.03343223035335541], "norm_scores": [0.3366762101650238, 0.3334709703922272, 0.329852819442749]}}
{"id": "WebQTest-150", "question": "what language does cuba speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does cuba speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.007869093678891659, -0.021801471710205078, -0.07542558014392853], "norm_scores": [0.34236735105514526, 0.33763042092323303, 0.32000216841697693]}}
{"id": "WebQTest-153", "question": "what are the sights to see in madrid", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["common.topic.image"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["base.arthist.helynevek.intezmeny"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the sights to see in madrid [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01695004664361477, -0.01923939771950245, -0.020215285941958427], "norm_scores": [0.3339507579803467, 0.33318713307380676, 0.33286210894584656]}}
{"id": "WebQTest-154", "question": "what instruments did louis armstrong play", "prediction": [["music.group_member.instruments_played"], ["music.artist.track_contributions", "music.track_contribution.role"], ["music.instrument.family"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what instruments did louis armstrong play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>", "<PATH> music.instrument.family </PATH>"], "scores": [-0.00026525379507802427, -0.08424089848995209, -0.12056151777505875], "norm_scores": [0.35636505484580994, 0.3276611268520355, 0.31597381830215454]}}
{"id": "WebQTest-155", "question": "what year was george w bush elected", "prediction": [["time.recurring_event.instances"], ["time.event.instance_of_recurring_event"], ["time.event.locations", "location.location.events"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was george w bush elected [/INST]", "raw_output": {"paths": ["<PATH> time.recurring_event.instances </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.event.locations <SEP> location.location.events </PATH>"], "scores": [-0.014744540676474571, -0.03396768495440483, -0.04652843996882439], "norm_scores": [0.3390202522277832, 0.33256545662879944, 0.32841432094573975]}}
{"id": "WebQTest-156", "question": "what time zone am i in california", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["base.locations.countries.states_provinces_within"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone am i in california [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> base.locations.countries.states_provinces_within </PATH>"], "scores": [-1.716524457151536e-05, -0.13619700074195862, -0.15835341811180115], "norm_scores": [0.36680442094802856, 0.3201049864292145, 0.31309056282043457]}}
{"id": "WebQTest-157", "question": "where is mission san buenaventura located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.hud_county_place.county"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mission san buenaventura located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.0008679698803462088, -0.06249864771962166, -0.09102008491754532], "norm_scores": [0.35038265585899353, 0.32944032549858093, 0.32017695903778076]}}
{"id": "WebQTest-159", "question": "what time in hilo hawaii", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["time.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time in hilo hawaii [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> time.location.time_zones </PATH>"], "scores": [-7.561137317679822e-05, -0.09454222768545151, -0.15310589969158173], "norm_scores": [0.36127662658691406, 0.32871049642562866, 0.3100128471851349]}}
{"id": "WebQTest-160", "question": "where did rudolf virchow conduct his research", "prediction": [["people.deceased_person.place_of_death", "location.location.containedby"], ["business.employment_tenure.person", "business.employer.employees"], ["people.person.employment_history", "business.employment_tenure.company"]], "ground_paths": [["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.employment_history", "business.employment_tenure.company"], ["business.employment_tenure.person", "business.employer.employees"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did rudolf virchow conduct his research [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employer.employees </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.012311466969549656, -0.024855464696884155, -0.03350462019443512], "norm_scores": [0.33709028363227844, 0.33288824558258057, 0.330021470785141]}}
{"id": "WebQTest-161", "question": "what university did gordon brown attend", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what university did gordon brown attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.014199084602296352, -0.01962883397936821, -0.022771136835217476], "norm_scores": [0.33489060401916504, 0.33307716250419617, 0.332032173871994]}}
{"id": "WebQTest-162", "question": "what countries does greece share borders with", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "location.location.partially_contains"]], "ground_paths": [["media_common.netflix_title.netflix_genres", "film.film.country"], ["location.location.partially_contains", "geography.river.basin_countries"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["organization.organization_founder.organizations_founded", "organization.organization_founder.organizations_founded"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["location.location.events", "time.event.locations"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["media_common.netflix_genre.titles", "film.film.country"], ["location.location.containedby", "location.location.partially_containedby"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.statistical_region.places_imported_from", "location.statistical_region.places_exported_to"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.partially_contains", "location.location.partially_contains"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.country.official_language", "location.country.languages_spoken"], ["organization.organization.founders", "organization.organization.founders"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["location.location.partially_contains", "location.location.partially_containedby"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["location.location.events", "location.location.events"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.containedby", "location.location.containedby"], ["location.location.containedby", "base.locations.continents.countries_within"], ["location.country.form_of_government", "location.country.form_of_government"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["time.event.locations", "time.event.locations"], ["location.location.containedby", "base.locations.countries.continent"], ["military.military_combatant.military_conflicts", "military.military_combatant_group.combatants"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does greece share borders with [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>"], "scores": [-0.028899263590574265, -0.04260498285293579, -0.04763137921690941], "norm_scores": [0.3369465172290802, 0.3323599100112915, 0.3306935429573059]}}
{"id": "WebQTest-163", "question": "what year did the seahawks win the superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the seahawks win the superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.011130406521260738, -0.014441849663853645, -0.04620061442255974], "norm_scores": [0.3375833332538605, 0.33646729588508606, 0.32594940066337585]}}
{"id": "WebQTest-164", "question": "what did stephen hawking become famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["fictional_universe.fictional_character.occupation"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did stephen hawking become famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>"], "scores": [-0.0021294320467859507, -0.050010647624731064, -0.07360389828681946], "norm_scores": [0.3467085361480713, 0.33049890398979187, 0.32279258966445923]}}
{"id": "WebQTest-165", "question": "who plays donna noble", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_guest_role.character", "tv.tv_actor.guest_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["tv.tv_guest_role.character", "tv.tv_guest_role.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.ethnicity", "people.person.ethnicity"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays donna noble [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.022399984300136566, -0.027784591540694237, -0.038956694304943085], "norm_scores": [0.33577221632003784, 0.3339690566062927, 0.33025869727134705]}}
{"id": "WebQTest-166", "question": "what was dr seuss education", "prediction": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was dr seuss education [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.01248006708920002, -0.01353294774889946, -0.04445412755012512], "norm_scores": [0.33698633313179016, 0.336631715297699, 0.32638195157051086]}}
{"id": "WebQTest-167", "question": "where did queensland get its name from", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["symbols.armorial_grant.coat_of_arms", "symbols.armorial_grant.armiger"]], "ground_paths": [["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did queensland get its name from [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.armorial_grant.coat_of_arms <SEP> symbols.armorial_grant.armiger </PATH>"], "scores": [-0.00667104497551918, -0.02714996412396431, -0.07671473920345306], "norm_scores": [0.3433968722820282, 0.336436003446579, 0.3201671242713928]}}
{"id": "WebQTest-168", "question": "what document did james madison write", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"]], "ground_paths": [["book.author.works_written"], ["book.book_subject.works"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what document did james madison write [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.012876247987151146, -0.017042921856045723, -0.05674359202384949], "norm_scores": [0.3386475145816803, 0.33723944425582886, 0.32411307096481323]}}
{"id": "WebQTest-169", "question": "what are the three official languages of belgium", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the three official languages of belgium [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.009550548158586025, -0.02083343267440796, -0.03927071392536163], "norm_scores": [0.3378951847553253, 0.33410418033599854, 0.32800063490867615]}}
{"id": "WebQTest-170", "question": "who played bilbo in lord of the rings", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.actor.film"], ["people.person.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played bilbo in lord of the rings [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.031227100640535355, -0.03435380756855011, -0.03535303846001625], "norm_scores": [0.3341396450996399, 0.33309653401374817, 0.3327638506889343]}}
{"id": "WebQTest-171", "question": "what airport do you fly into to get to destin fl", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["location.location.containedby"]], "ground_paths": [["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what airport do you fly into to get to destin fl [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.010935389436781406, -0.017982926219701767, -0.04677591845393181], "norm_scores": [0.3380923867225647, 0.335718035697937, 0.3261895775794983]}}
{"id": "WebQTest-172", "question": "what do you call members of the senate", "prediction": [["government.government_office_or_title.governmental_body_if_any", "common.topic.notable_types", "common.topic.notable_types"], ["government.governmental_body.offices_positions", "common.topic.notable_types", "common.topic.notable_types"], ["government.government_position_held.governmental_body", "government.government_position_held.office_position_or_title"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do you call members of the senate [/INST]", "raw_output": {"paths": ["<PATH> government.government_office_or_title.governmental_body_if_any <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> government.governmental_body.offices_positions <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.012259041890501976, -0.020268207415938377, -0.02094598114490509], "norm_scores": [0.3351910412311554, 0.332517147064209, 0.3322918713092804]}}
{"id": "WebQTest-173", "question": "what countries are part of the baltic", "prediction": [["location.location.containedby"], ["location.location.contains"], ["common.topic.notable_types"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are part of the baltic [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.011250950396060944, -0.019352145493030548, -0.05202776938676834], "norm_scores": [0.338756263256073, 0.3360230028629303, 0.3252207040786743]}}
{"id": "WebQTest-174", "question": "what team does colin kaepernick play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does colin kaepernick play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.013112903572618961, -0.02542892098426819, -0.025944998487830162], "norm_scores": [0.3361333906650543, 0.33201897144317627, 0.3318476378917694]}}
{"id": "WebQTest-175", "question": "where did joe flacco attend college", "prediction": [["people.person.nationality", "location.location.containedby"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did joe flacco attend college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.016425898298621178, -0.020411944016814232, -0.02240433357656002], "norm_scores": [0.33444130420684814, 0.33311086893081665, 0.3324478268623352]}}
{"id": "WebQTest-176", "question": "who is emma stone father", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.gender"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is emma stone father [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.007873713038861752, -0.024314040318131447, -0.12862853705883026], "norm_scores": [0.34843870997428894, 0.34275707602500916, 0.3088042140007019]}}
{"id": "WebQTest-177", "question": "where was theodore roosevelt buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["people.person.place_of_interment"]], "ground_paths": [["people.place_of_interment.interred_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was theodore roosevelt buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_interment </PATH>"], "scores": [-0.010264309123158455, -0.023693135008215904, -0.061605703085660934], "norm_scores": [0.34052810072898865, 0.3359857499599457, 0.3234861493110657]}}
{"id": "WebQTest-178", "question": "who was the leader of the us during wwii", "prediction": [["event.speech_or_presentation.speech_topic", "event.speech_or_presentation.speaker_s"], ["government.government_office_or_title.office_holders", "government.government_position_held.office_holder"], ["military.military_conflict.commanders", "military.military_command.military_commander"]], "ground_paths": [["event.speech_topic.speeches_or_presentations_on_this_topic", "event.speech_or_presentation.speaker_s"], ["military.military_command.military_conflict", "military.military_commander.military_commands"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the leader of the us during wwii [/INST]", "raw_output": {"paths": ["<PATH> event.speech_or_presentation.speech_topic <SEP> event.speech_or_presentation.speaker_s </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.government_position_held.office_holder </PATH>", "<PATH> military.military_conflict.commanders <SEP> military.military_command.military_commander </PATH>"], "scores": [-0.016043607145547867, -0.0279300045222044, -0.028158072382211685], "norm_scores": [0.33600538969039917, 0.3320351541042328, 0.33195942640304565]}}
{"id": "WebQTest-179", "question": "what artistic movement did henri matisse belong to", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what artistic movement did henri matisse belong to [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.0013154182815924287, -0.04161570221185684, -0.10093887150287628], "norm_scores": [0.3489574193954468, 0.33517393469810486, 0.31586864590644836]}}
{"id": "WebQTest-180", "question": "where the missouri river ends", "prediction": [["geography.river.mouth"], ["location.location.partially_containedby"], ["location.location.containedby"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where the missouri river ends [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.003613384673371911, -0.0572827085852623, -0.05907444655895233], "norm_scores": [0.34556707739830017, 0.32750964164733887, 0.32692334055900574]}}
{"id": "WebQTest-181", "question": "what type of currency do they use in thailand", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["royalty.monarch.kingdom", "location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of currency do they use in thailand [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> royalty.monarch.kingdom <SEP> location.country.currency_used </PATH>"], "scores": [-0.009796722792088985, -0.01625019498169422, -0.10610901564359665], "norm_scores": [0.34461989998817444, 0.3424030840396881, 0.31297701597213745]}}
{"id": "WebQTest-182", "question": "who is khloe kardashian 's husband", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["people.marriage.spouse", "people.marriage.spouse"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is khloe kardashian 's husband [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.01933257095515728, -0.020375775173306465, -0.03568254038691521], "norm_scores": [0.33526214957237244, 0.33491262793540955, 0.32982519268989563]}}
{"id": "WebQTest-184", "question": "what state does romney live in", "prediction": [["people.person.places_lived", "people.place_lived.location"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.administrative_division.country"]], "ground_paths": [["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state does romney live in [/INST]", "raw_output": {"paths": ["<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.country </PATH>"], "scores": [-0.02882934734225273, -0.030460068956017494, -0.03817535191774368], "norm_scores": [0.3345524072647095, 0.33400729298591614, 0.331440269947052]}}
{"id": "WebQTest-185", "question": "what type of government does france use", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does france use [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.010945516638457775, -0.013301383703947067, -0.1351308375597], "norm_scores": [0.34711819887161255, 0.3463013768196106, 0.30658039450645447]}}
{"id": "WebQTest-186", "question": "who rules denmark right now", "prediction": [["royalty.monarch.kingdom"], ["people.person.nationality"], ["royalty.kingdom.rulers"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who rules denmark right now [/INST]", "raw_output": {"paths": ["<PATH> royalty.monarch.kingdom </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> royalty.kingdom.rulers </PATH>"], "scores": [-0.010712062940001488, -0.019433047622442245, -0.0655766949057579], "norm_scores": [0.3403756618499756, 0.33742016553878784, 0.3222041428089142]}}
{"id": "WebQTest-187", "question": "what did stephen hawking study", "prediction": [["people.person.profession"], ["education.education.student", "education.education.major_field_of_study"], ["people.person.education", "education.education.major_field_of_study"]], "ground_paths": [["education.education.student", "education.education.major_field_of_study"], ["book.author.works_written", "book.written_work.subjects"], ["book.written_work.author", "book.written_work.subjects"], ["book.book_edition.author_editor", "book.written_work.subjects"], ["people.person.profession", "people.professional_field.professions_in_this_field"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did stephen hawking study [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>"], "scores": [-0.019197292625904083, -0.020356101915240288, -0.025108013302087784], "norm_scores": [0.33411863446235657, 0.3337317109107971, 0.33214959502220154]}}
{"id": "WebQTest-188", "question": "where did bristol palin go to school", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did bristol palin go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.008781207725405693, -0.02353302761912346, -0.027803128585219383], "norm_scores": [0.3370959460735321, 0.3321596682071686, 0.33074435591697693]}}
{"id": "WebQTest-189", "question": "what region of the world is egypt associated with", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.countries.continent"]], "ground_paths": [["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what region of the world is egypt associated with [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.018473468720912933, -0.018923694267868996, -0.03650211915373802], "norm_scores": [0.33538109064102173, 0.33523011207580566, 0.3293887674808502]}}
{"id": "WebQTest-190", "question": "what did dr. seuss do", "prediction": [["people.person.profession"], ["book.book.interior_illustrations_by"], ["fictional_universe.fictional_character.occupation"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did dr. seuss do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> book.book.interior_illustrations_by </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>"], "scores": [-0.008193399757146835, -0.029976826161146164, -0.057810258120298386], "norm_scores": [0.34129154682159424, 0.3339374363422394, 0.3247709572315216]}}
{"id": "WebQTest-191", "question": "where did drew brees go to college wikianswers", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did drew brees go to college wikianswers [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.017405906692147255, -0.018475016579031944, -0.026977863162755966], "norm_scores": [0.33451470732688904, 0.334157258272171, 0.33132800459861755]}}
{"id": "WebQTest-193", "question": "what school did ben roethlisberger go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did ben roethlisberger go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.01856677606701851, -0.022251183167099953, -0.02268882282078266], "norm_scores": [0.33420127630233765, 0.3329721987247467, 0.33282652497291565]}}
{"id": "WebQTest-194", "question": "where did the iroquois indians come from", "prediction": [["people.ethnicity.geographic_distribution"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["people.ethnicity.geographic_distribution", "base.locations.states_and_provences.country"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the iroquois indians come from [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> base.locations.states_and_provences.country </PATH>"], "scores": [-0.0002026461297646165, -0.10004173219203949, -0.10019171983003616], "norm_scores": [0.3558933734893799, 0.3220774531364441, 0.32202914357185364]}}
{"id": "WebQTest-195", "question": "what kind of guitar did george harrison use", "prediction": [["music.group_member.instruments_played"], ["music.guitarist.guitars_played"], ["music.guitar.guitarists"]], "ground_paths": [["music.guitar.guitarists"], ["music.guitarist.guitars_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of guitar did george harrison use [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.guitar.guitarists </PATH>"], "scores": [-0.009301741607487202, -0.034048788249492645, -0.03652756288647652], "norm_scores": [0.33913272619247437, 0.3308431804180145, 0.3300240933895111]}}
{"id": "WebQTest-196", "question": "what language do navajo people speak", "prediction": [["people.ethnicity.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do navajo people speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.000613494252320379, -0.08153519779443741, -0.08728379756212234], "norm_scores": [0.3522062599658966, 0.32482782006263733, 0.3229658603668213]}}
{"id": "WebQTest-197", "question": "what places make up new england", "prediction": [["location.location.contains"], ["location.location.containedby"], ["book.book_subject.works"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what places make up new england [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> book.book_subject.works </PATH>"], "scores": [-0.009241340681910515, -0.020214395597577095, -0.12508685886859894], "norm_scores": [0.34725841879844666, 0.3434687852859497, 0.309272825717926]}}
{"id": "WebQTest-199", "question": "what type of government does iraq have now", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does iraq have now [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.005662695504724979, -0.021317020058631897, -0.12984582781791687], "norm_scores": [0.3487132787704468, 0.34329691529273987, 0.30798986554145813]}}
{"id": "WebQTest-200", "question": "where was the vietnam war location", "prediction": [["time.event.locations"], ["location.location.events"], ["book.written_work.subjects"]], "ground_paths": [["location.location.events"], ["book.book_subject.works"], ["time.event.locations"], ["book.written_work.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the vietnam war location [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.008324183523654938, -0.030177531763911247, -0.047757960855960846], "norm_scores": [0.3401687741279602, 0.33281558752059937, 0.3270156681537628]}}
{"id": "WebQTest-201", "question": "who did gerald ford select as his vice president when he became president", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did gerald ford select as his vice president when he became president [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.0058859968557953835, -0.019911468029022217, -0.06765274703502655], "norm_scores": [0.3417431116104126, 0.33698347210884094, 0.32127341628074646]}}
{"id": "WebQTest-202", "question": "what is the currency used in italy", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency used in italy [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.005144693423062563, -0.0269591324031353, -0.14313942193984985], "norm_scores": [0.350935697555542, 0.34336310625076294, 0.30570125579833984]}}
{"id": "WebQTest-204", "question": "what is the capital of modern egypt", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of modern egypt [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.01013217680156231, -0.024361232295632362, -0.045690666884183884], "norm_scores": [0.33887529373168945, 0.3340875804424286, 0.32703712582588196]}}
{"id": "WebQTest-205", "question": "what school did sir ernest rutherford go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.contains"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.contains"], ["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did sir ernest rutherford go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.contains </PATH>"], "scores": [-0.013730525970458984, -0.018342815339565277, -0.02922848053276539], "norm_scores": [0.33556824922561646, 0.3340240716934204, 0.33040767908096313]}}
{"id": "WebQTest-206", "question": "what position does brandon roy play", "prediction": [["basketball.basketball_player.position_s"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.position"], ["sports.sports_position.sport", "sports.sports_team.sport"]], "ground_paths": [["basketball.basketball_player.position_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what position does brandon roy play [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.position_s </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.position </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>"], "scores": [-0.009381778538227081, -0.0317121259868145, -0.03942589834332466], "norm_scores": [0.339176207780838, 0.33168622851371765, 0.32913753390312195]}}
{"id": "WebQTest-207", "question": "where is harvard university", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.parent_institution"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is harvard university [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.parent_institution </PATH>"], "scores": [-0.003461720421910286, -0.04817049205303192, -0.07677096128463745], "norm_scores": [0.3465496599674225, 0.33139708638191223, 0.3220532238483429]}}
{"id": "WebQTest-209", "question": "where is laos in world map", "prediction": [["location.location.containedby"], ["base.locations.countries.continent"], ["location.location.contains"]], "ground_paths": [["book.book_subject.works"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is laos in world map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.015239379368722439, -0.027538498863577843, -0.03437476605176926], "norm_scores": [0.3368338942527771, 0.3327164649963379, 0.3304497003555298]}}
{"id": "WebQTest-210", "question": "where is tyrese gibson from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is tyrese gibson from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.006066074129194021, -0.024414299055933952, -0.08061365783214569], "norm_scores": [0.3436446785926819, 0.33739691972732544, 0.3189584016799927]}}
{"id": "WebQTest-211", "question": "what countries are near italy", "prediction": [["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_containedby"]], "ground_paths": [["location.country.official_language", "location.country.official_language"], ["location.country.form_of_government", "government.form_of_government.countries"], ["location.country.currency_used", "finance.currency.countries_used"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.sport_country.multi_event_tournaments_participated_in"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.multi_event_tournament.participating_countries"], ["base.locations.countries.continent", "location.location.contains_major_portion_of"], ["travel.guidebook.travel_destinations_covered", "travel.travel_destination.guidebooks"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["biology.breed_origin.breeds_originating_here", "biology.animal_breed.place_of_origin"], ["geography.lake.basin_countries", "geography.lake.basin_countries"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["organization.organization_founder.organizations_founded", "organization.organization_founder.organizations_founded"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.time_zones", "location.location.time_zones"], ["fictional_universe.fictional_setting.works_set_here", "fictional_universe.work_of_fiction.setting"], ["location.country.official_language", "language.human_language.main_country"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_containedby", "location.location.partially_contains"], ["base.locations.countries.continent", "location.location.containedby"], ["location.location.partially_contains", "geography.river.origin"], ["geography.river.basin_countries", "location.location.partially_contains"], ["organization.organization.founders", "base.nations.supranational_body.member"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["organization.organization.founders", "organization.organization.founders"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["location.location.partially_contains", "location.location.partially_containedby"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["base.locations.countries.continent", "base.locations.countries.continent"], ["common.topic.notable_types", "common.topic.notable_types"], ["military.military_combatant.includes_allies", "military.military_combatant.includes_allies"], ["location.location.containedby", "location.location.containedby"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["location.location.containedby", "geography.river.basin_countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["location.location.containedby", "base.locations.countries.continent"], ["location.country.currency_used", "location.country.currency_used"], ["fictional_universe.fictional_setting.universe", "fictional_universe.fictional_setting.universe"], ["government.form_of_government.countries", "location.country.form_of_government"], ["military.military_combatant.military_conflicts", "military.military_combatant_group.combatants"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are near italy [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>"], "scores": [-0.03573651984333992, -0.03739514201879501, -0.04023491591215134], "norm_scores": [0.3340175747871399, 0.33346402645111084, 0.33251839876174927]}}
{"id": "WebQTest-212", "question": "which states does the colorado river run through", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.containedby"]], "ground_paths": [["location.location.partially_contains"], ["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which states does the colorado river run through [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.012305599637329578, -0.01912873238325119, -0.04035011678934097], "norm_scores": [0.3372061252593994, 0.3349131643772125, 0.32788071036338806]}}
{"id": "WebQTest-213", "question": "who did scarlett johansson date", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["people.person.languages", "people.person.languages"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["base.popstra.celebrity.breakup", "base.popstra.breakup.participant"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.marriage.spouse", "people.person.spouse_s"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["base.popstra.celebrity.breakup", "base.popstra.celebrity.breakup"], ["people.person.spouse_s", "people.marriage.spouse"], ["film.film.executive_produced_by", "film.producer.films_executive_produced"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did scarlett johansson date [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.017034929245710373, -0.021885700523853302, -0.03381817415356636], "norm_scores": [0.3357374668121338, 0.33411282300949097, 0.33014973998069763]}}
{"id": "WebQTest-214", "question": "which continents were part of the roman empire", "prediction": [["fictional_universe.fictional_setting.languages", "language.human_language.region"], ["location.location.partially_contained_by", "location.partial_containment_relationship.partially_contained_by"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["location.location.partially_contained_by", "location.location.partiallycontains"], ["book.book_subject.works", "book.written_work.subjects"], ["fictional_universe.fictional_setting.languages", "language.human_language.region"], ["location.location.partially_contained_by", "location.partial_containment_relationship.partially_contained_by"], ["location.country.official_language", "language.human_language.region"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which continents were part of the roman empire [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_setting.languages <SEP> language.human_language.region </PATH>", "<PATH> location.location.partially_contained_by <SEP> location.partial_containment_relationship.partially_contained_by </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.017736749723553658, -0.03072933666408062, -0.03636734560132027], "norm_scores": [0.33685532212257385, 0.33250704407691956, 0.3306376039981842]}}
{"id": "WebQTest-215", "question": "where was elvis costello born", "prediction": [["location.location.people_born_here"], ["music.artist.origin"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was elvis costello born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.016427140682935715, -0.018102312460541725, -0.033343084156513214], "norm_scores": [0.3353957235813141, 0.33483433723449707, 0.32976990938186646]}}
{"id": "WebQTest-216", "question": "where are the gobi desert located on a map", "prediction": [["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the gobi desert located on a map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0015215344028547406, -0.05902957543730736, -0.07701509445905685], "norm_scores": [0.3482621908187866, 0.328799307346344, 0.3229385316371918]}}
{"id": "WebQTest-217", "question": "what type of government does the us follow", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.planet"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does the us follow [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.planet </PATH>"], "scores": [-0.006595222279429436, -0.020342428237199783, -0.1675471067428589], "norm_scores": [0.3524005711078644, 0.3475891947746277, 0.30001023411750793]}}
{"id": "WebQTest-218", "question": "in which continent is germany", "prediction": [["location.location.containedby"], ["base.locations.countries.continent"], ["location.location.contains"]], "ground_paths": [["base.locations.countries.continent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: in which continent is germany [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00708030816167593, -0.03237525373697281, -0.055671948939561844], "norm_scores": [0.3415776193141937, 0.3330457806587219, 0.32537660002708435]}}
{"id": "WebQTest-220", "question": "what language turkey people speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language turkey people speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.0071943714283406734, -0.030315758660435677, -0.052519164979457855], "norm_scores": [0.3409675061702728, 0.33317431807518005, 0.3258582353591919]}}
{"id": "WebQTest-221", "question": "who does jeremy lin play for", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["people.person.place_of_birth", "sports.sports_team.location"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does jeremy lin play for [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.013952604494988918, -0.025656798854470253, -0.033040259033441544], "norm_scores": [0.33676186203956604, 0.3328433036804199, 0.33039483428001404]}}
{"id": "WebQTest-222", "question": "where is the seychelles on world map", "prediction": [["location.location.containedby"], ["base.locations.continents.countries_within"], ["location.location.contains"]], "ground_paths": [["base.locations.continents.countries_within"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the seychelles on world map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.003894384717568755, -0.047812625765800476, -0.05306823551654816], "norm_scores": [0.3437550961971283, 0.32898467779159546, 0.32726022601127625]}}
{"id": "WebQTest-224", "question": "where did william morris go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did william morris go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.01525117363780737, -0.015610826201736927, -0.026970980688929558], "norm_scores": [0.3346732556819916, 0.33455291390419006, 0.33077386021614075]}}
{"id": "WebQTest-225", "question": "what state is mount st. helens in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.mountain.mountain_type"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state is mount st. helens in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.mountain.mountain_type </PATH>"], "scores": [-0.003993968013674021, -0.03295072540640831, -0.13980445265769958], "norm_scores": [0.35155975818634033, 0.341525673866272, 0.3069145083427429]}}
{"id": "WebQTest-226", "question": "who is jimmy savile", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is jimmy savile [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0007548449211753905, -0.07320794463157654, -0.07771685719490051], "norm_scores": [0.350135862827301, 0.3256646394729614, 0.3241995573043823]}}
{"id": "WebQTest-227", "question": "where were the seneca indians located", "prediction": [["people.ethnicity.geographic_distribution"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where were the seneca indians located [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0007159005617722869, -0.08452567458152771, -0.10041090101003647], "norm_scores": [0.354017436504364, 0.32555660605430603, 0.32042592763900757]}}
{"id": "WebQTest-231", "question": "when did florida marlins join mlb", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.championships"], ["sports.sports_league_participation.league", "sports.sports_league_participation.season"]], "ground_paths": [["baseball.baseball_team.team_stats", "baseball.baseball_team_stats.season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did florida marlins join mlb [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_league_participation.league <SEP> sports.sports_league_participation.season </PATH>"], "scores": [-0.015891967341303825, -0.019320672377943993, -0.038490839302539825], "norm_scores": [0.33622127771377563, 0.3350704610347748, 0.3287082612514496]}}
{"id": "WebQTest-232", "question": "what countries have spanish as the national language", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries have spanish as the national language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.011192568577826023, -0.022920463234186172, -0.030623529106378555], "norm_scores": [0.33680272102355957, 0.3328757882118225, 0.3303214907646179]}}
{"id": "WebQTest-233", "question": "what to see in phoenix arizona", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to see in phoenix arizona [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.013863923959434032, -0.014090292155742645, -0.03588084504008293], "norm_scores": [0.3357961177825928, 0.33572012186050415, 0.3284837305545807]}}
{"id": "WebQTest-234", "question": "what countries does the panama canal go through", "prediction": [["location.location.containedby"], ["geography.body_of_water.bridges", "location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does the panama canal go through [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> geography.body_of_water.bridges <SEP> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0010437385644763708, -0.07125212252140045, -0.09666717052459717], "norm_scores": [0.3519880175590515, 0.3281230926513672, 0.3198888897895813]}}
{"id": "WebQTest-235", "question": "what is charlie chaplin famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["film.film.directed_by"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is charlie chaplin famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> film.film.directed_by </PATH>"], "scores": [-0.0038066410925239325, -0.03405359014868736, -0.13084235787391663], "norm_scores": [0.3507654070854187, 0.340314656496048, 0.30891987681388855]}}
{"id": "WebQTest-237", "question": "what did fred durst do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["music.artist.genre", "music.artist.genre"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did fred durst do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>"], "scores": [-0.003511727089062333, -0.057665251195430756, -0.06868594139814377], "norm_scores": [0.3467176854610443, 0.3284410536289215, 0.3248412609100342]}}
{"id": "WebQTest-239", "question": "where do american bulldogs originate from", "prediction": [["biology.animal_breed.breed_of"], ["biology.breed_origin.breeds_originating_here"], ["biology.animal_breed.place_of_origin"]], "ground_paths": [["biology.animal_breed.place_of_origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do american bulldogs originate from [/INST]", "raw_output": {"paths": ["<PATH> biology.animal_breed.breed_of </PATH>", "<PATH> biology.breed_origin.breeds_originating_here </PATH>", "<PATH> biology.animal_breed.place_of_origin </PATH>"], "scores": [-0.008206921629607677, -0.01646830514073372, -0.08123254776000977], "norm_scores": [0.34230753779411316, 0.3394912779331207, 0.3182012140750885]}}
{"id": "WebQTest-240", "question": "what colleges did albert einstein teach at", "prediction": [["people.person.employment_history", "business.employer.employees"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"], ["business.employment_tenure.person", "business.employer.employees"], ["education.department.academics_old", "education.university.departments"], ["people.person.employment_history", "business.employment_tenure.company"], ["people.person.nationality", "location.location.containedby"], ["people.person.employment_history", "business.employer.employees"], ["people.person.nationality", "organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what colleges did albert einstein teach at [/INST]", "raw_output": {"paths": ["<PATH> people.person.employment_history <SEP> business.employer.employees </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.01739879511296749, -0.02886221557855606, -0.031578872352838516], "norm_scores": [0.3361884653568268, 0.33235660195350647, 0.33145493268966675]}}
{"id": "WebQTest-241", "question": "who plays the voice of brian on family guy", "prediction": [["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "ground_paths": [["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays the voice of brian on family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>"], "scores": [-0.02265845239162445, -0.022794270887970924, -0.030330989509820938], "norm_scores": [0.3341999351978302, 0.33415454626083374, 0.33164554834365845]}}
{"id": "WebQTest-243", "question": "where did george w bush live as a child", "prediction": [["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did george w bush live as a child [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.011046985164284706, -0.02519609034061432, -0.03977886587381363], "norm_scores": [0.3381088674068451, 0.3333585858345032, 0.3285325765609741]}}
{"id": "WebQTest-245", "question": "what year was the first miss america pageant held", "prediction": [["time.event.instance_of_recurring_event"], ["time.recurring_event.instances"], ["time.recurring_event.current_frequency"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was the first miss america pageant held [/INST]", "raw_output": {"paths": ["<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>"], "scores": [-0.007816078141331673, -0.0239857230335474, -0.053097110241651535], "norm_scores": [0.34017202258110046, 0.3347157835960388, 0.3251122236251831]}}
{"id": "WebQTest-246", "question": "what was nikola tesla inventions", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"]], "ground_paths": [["symbols.name_source.namesakes"], ["law.invention.inventor"], ["law.inventor.inventions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was nikola tesla inventions [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>"], "scores": [-0.006034940015524626, -0.03897163271903992, -0.05487566813826561], "norm_scores": [0.3424736559391022, 0.33137744665145874, 0.3261488676071167]}}
{"id": "WebQTest-247", "question": "where did flemish people come from", "prediction": [["people.ethnicity.geographic_distribution"], ["base.popstra.ethnicity.geographic_distribution"], ["people.ethnicity.included_in_group"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did flemish people come from [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> base.popstra.ethnicity.geographic_distribution </PATH>", "<PATH> people.ethnicity.included_in_group </PATH>"], "scores": [-7.816439756425098e-05, -0.1305682361125946, -0.13057635724544525], "norm_scores": [0.36293381452560425, 0.31853434443473816, 0.3185317814350128]}}
{"id": "WebQTest-248", "question": "what state new york city belong to", "prediction": [["location.location.containedby"], ["location.administrative_division.country"], ["base.biblioness.bibs_location.state"]], "ground_paths": [["base.biblioness.bibs_location.state"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state new york city belong to [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>"], "scores": [-0.011549578979611397, -0.04933619126677513, -0.05011313781142235], "norm_scores": [0.34186992049217224, 0.3291928768157959, 0.32893720269203186]}}
{"id": "WebQTest-249", "question": "what was the cause of death for bruce lee", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the cause of death for bruce lee [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0010783919133245945, -0.04777463898062706, -0.1079193502664566], "norm_scores": [0.350502610206604, 0.33451172709465027, 0.3149856925010681]}}
{"id": "WebQTest-250", "question": "what type of government does usa follow", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.planet"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does usa follow [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.planet </PATH>"], "scores": [-0.007877113297581673, -0.01778602972626686, -0.1566714346408844], "norm_scores": [0.35064512491226196, 0.34718775749206543, 0.302167147397995]}}
{"id": "WebQTest-251", "question": "what type of currency do they use in england", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of currency do they use in england [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.002278377069160342, -0.039694517850875854, -0.11630582809448242], "norm_scores": [0.3502002954483032, 0.33733925223350525, 0.31246042251586914]}}
{"id": "WebQTest-252", "question": "where was benjamin franklin educated", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was benjamin franklin educated [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.012743212282657623, -0.025218088179826736, -0.02799920365214348], "norm_scores": [0.336421400308609, 0.3322506546974182, 0.3313279449939728]}}
{"id": "WebQTest-253", "question": "what countries included in the uk", "prediction": [["base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["base.biblioness.bibs_location.country"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries included in the uk [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.022928811609745026, -0.02543044649064541, -0.030497679486870766], "norm_scores": [0.3344525098800659, 0.33361688256263733, 0.33193063735961914]}}
{"id": "WebQTest-254", "question": "what countries in the world speak chinese", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries in the world speak chinese [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.018824579194188118, -0.02134421095252037, -0.03160606324672699], "norm_scores": [0.3350326716899872, 0.33418959379196167, 0.33077773451805115]}}
{"id": "WebQTest-255", "question": "where obama went to school", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where obama went to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.011178839020431042, -0.028691139072179794, -0.029837293550372124], "norm_scores": [0.3373643457889557, 0.33150771260261536, 0.33112800121307373]}}
{"id": "WebQTest-256", "question": "what is the primary language of china", "prediction": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the primary language of china [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.007106844801455736, -0.03606940433382988, -0.03734038770198822], "norm_scores": [0.3399427533149719, 0.33023834228515625, 0.32981887459754944]}}
{"id": "WebQTest-257", "question": "who was the soviet leader during world war ii", "prediction": [["military.military_person.participated_in_conflicts"], ["people.person.nationality"], ["military.military_conflict.military_personnel_involved"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the soviet leader during world war ii [/INST]", "raw_output": {"paths": ["<PATH> military.military_person.participated_in_conflicts </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>"], "scores": [-0.010705561377108097, -0.013399094343185425, -0.0890817791223526], "norm_scores": [0.3422399163246155, 0.34131935238838196, 0.3164407014846802]}}
{"id": "WebQTest-258", "question": "which of the following does australia export the most", "prediction": [["location.statistical_region.major_exports", "location.imports_exports_by_industry.industry"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.statistical_region.major_exports", "location.imports_exports_by_industry.industry"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which of the following does australia export the most [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.major_exports <SEP> location.imports_exports_by_industry.industry </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.008902664296329021, -0.029983896762132645, -0.03336097300052643], "norm_scores": [0.33841201663017273, 0.3313525319099426, 0.33023542165756226]}}
{"id": "WebQTest-260", "question": "where was the city of david", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the city of david [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.005733765196055174, -0.034839581698179245, -0.08133336901664734], "norm_scores": [0.34500589966773987, 0.3351089358329773, 0.31988510489463806]}}
{"id": "WebQTest-261", "question": "what is cindy sherman known for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is cindy sherman known for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.006177190225571394, -0.03249632567167282, -0.051104821264743805], "norm_scores": [0.3412863314151764, 0.3324211537837982, 0.326292484998703]}}
{"id": "WebQTest-262", "question": "what tv shows did shawnee smith play in", "prediction": [["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.nationality", "film.film.country"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"]], "ground_paths": [["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["film.performance.actor", "film.film.starring"], ["people.person.nationality", "tv.tv_program.country_of_origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what tv shows did shawnee smith play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_program.regular_cast </PATH>"], "scores": [-0.022025441750884056, -0.023148182779550552, -0.031924810260534286], "norm_scores": [0.3345569670200348, 0.3341815769672394, 0.33126142621040344]}}
{"id": "WebQTest-264", "question": "what does australia use for currency", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does australia use for currency [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>"], "scores": [-0.01165525708347559, -0.015328995883464813, -0.12134280800819397], "norm_scores": [0.3457280397415161, 0.34446021914482117, 0.3098117709159851]}}
{"id": "WebQTest-265", "question": "which airport to fly into rome", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["location.location.containedby"]], "ground_paths": [["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which airport to fly into rome [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.006068721413612366, -0.030145851895213127, -0.060445502400398254], "norm_scores": [0.34208086133003235, 0.33394286036491394, 0.3239762783050537]}}
{"id": "WebQTest-266", "question": "what is there to do for fun in kansas city", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["location.place_with_neighborhoods.neighborhoods"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is there to do for fun in kansas city [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.011198772117495537, -0.016883421689271927, -0.0329909473657608], "norm_scores": [0.3363860249519348, 0.3344792127609253, 0.3291347324848175]}}
{"id": "WebQTest-267", "question": "where was martin luther king jr raised", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["influence.influence_node.influenced_by"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was martin luther king jr raised [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.0006614222074858844, -0.061483804136514664, -0.11165829002857208], "norm_scores": [0.3526177704334259, 0.331809937953949, 0.3155722916126251]}}
{"id": "WebQTest-268", "question": "what year was lebron james rookie season", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"]], "ground_paths": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was lebron james rookie season [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>"], "scores": [-0.010202493518590927, -0.014782910235226154, -0.03550097346305847], "norm_scores": [0.33664944767951965, 0.33511099219322205, 0.3282395303249359]}}
{"id": "WebQTest-269", "question": "where does delaware river start", "prediction": [["geography.river.origin"], ["location.location.containedby"], ["location.location.partial_containedby"]], "ground_paths": [["geography.river.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does delaware river start [/INST]", "raw_output": {"paths": ["<PATH> geography.river.origin </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partial_containedby </PATH>"], "scores": [-0.0009558099554851651, -0.08047442138195038, -0.10574228316545486], "norm_scores": [0.3540979325771332, 0.3270309567451477, 0.3188711404800415]}}
{"id": "WebQTest-270", "question": "what kind of monarchy does japan have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["royalty.monarch.kingdom"]], "ground_paths": [["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of monarchy does japan have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> royalty.monarch.kingdom </PATH>"], "scores": [-0.007672973908483982, -0.023958774283528328, -0.04580579698085785], "norm_scores": [0.33939361572265625, 0.33391109108924866, 0.3266952633857727]}}
{"id": "WebQTest-271", "question": "where did mitt romney 's parents come from", "prediction": [["people.person.place_of_birth"], ["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did mitt romney 's parents come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.015096011571586132, -0.026830138638615608, -0.02917124517261982], "norm_scores": [0.3362070322036743, 0.33228498697280884, 0.33150795102119446]}}
{"id": "WebQTest-273", "question": "when did michael jordan return to the nba", "prediction": [["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"], ["common.topic.image", "common.image.appears_in_topic_gallery"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.draft"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did michael jordan return to the nba [/INST]", "raw_output": {"paths": ["<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> common.topic.image <SEP> common.image.appears_in_topic_gallery </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.draft </PATH>"], "scores": [-0.011666171252727509, -0.023415155708789825, -0.025961697101593018], "norm_scores": [0.3362332582473755, 0.3323059678077698, 0.3314608037471771]}}
{"id": "WebQTest-276", "question": "where was mission san rafael located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was mission san rafael located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.000635546341072768, -0.07562672346830368, -0.1082223504781723], "norm_scores": [0.35388830304145813, 0.3283204734325409, 0.3177911937236786]}}
{"id": "WebQTest-277", "question": "who was esther 's husband", "prediction": [["people.marriage.spouse", "people.marriage.spouse"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.children"]], "ground_paths": [["visual_art.art_subject.artwork_on_the_subject", "visual_art.artwork.art_subject"], ["people.person.spouse_s", "people.person.spouse_s"], ["book.translated_work.translations", "book.book_subject.works"], ["common.topic.notable_types", "common.topic.notable_types"], ["visual_art.art_subject.artwork_on_the_subject", "visual_art.art_subject.artwork_on_the_subject"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was esther 's husband [/INST]", "raw_output": {"paths": ["<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.028599638491868973, -0.037086885422468185, -0.048310697078704834], "norm_scores": [0.33647027611732483, 0.3336266577243805, 0.3299030661582947]}}
{"id": "WebQTest-278", "question": "who 's dating claire danes", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["base.popstra.dated.participant", "base.popstra.dated.participant"], ["people.person.children", "people.person.parents"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.infidelity.perpetrator", "base.popstra.infidelity.participant"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["people.marriage.spouse", "people.person.spouse_s"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who 's dating claire danes [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.018030622974038124, -0.027329418808221817, -0.03019089624285698], "norm_scores": [0.3357217013835907, 0.33261433243751526, 0.33166393637657166]}}
{"id": "WebQTest-279", "question": "what county is st paul va in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.us_county.hud_county_place"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st paul va in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.hud_county_place </PATH>"], "scores": [-0.018476884812116623, -0.024800913408398628, -0.030041353777050972], "norm_scores": [0.33532315492630005, 0.33320924639701843, 0.3314676582813263]}}
{"id": "WebQTest-281", "question": "what type of sports do japanese play", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["cricket.cricket_team.country"]], "ground_paths": [["sports.sports_team.location"], ["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of sports do japanese play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> cricket.cricket_team.country </PATH>"], "scores": [-0.009796095080673695, -0.015212363563477993, -0.13143840432167053], "norm_scores": [0.34721454977989197, 0.3453390300273895, 0.30744636058807373]}}
{"id": "WebQTest-282", "question": "which countries share a border with russia", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_containedby", "location.location.partially_containedby"]], "ground_paths": [["location.location.partially_containedby", "base.locations.countries.continent"], ["location.location.partially_contains", "geography.river.basin_countries"], ["location.country.languages_spoken", "location.country.official_language"], ["base.locations.continents.countries_within", "location.location.partially_contains"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["time.event.locations", "location.location.events"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["base.rivalries.rival.rivals", "base.rivalries.rival.rivals"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["location.location.events", "time.event.locations"], ["freebase.valuenotation.has_no_value", "freebase.valuenotation.has_no_value"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["language.human_language.main_country", "location.country.languages_spoken"], ["location.location.partially_containedby", "geography.river.origin"], ["location.country.languages_spoken", "language.human_language.main_country"], ["base.locations.continents.countries_within", "base.locations.continents.countries_within"], ["base.rivalries.rival.rivals", "base.rivalries.rivalry.rival"], ["geography.river.basin_countries", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_contains"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_contains", "geography.river.origin"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.partially_contains", "location.location.partially_contains"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["organization.organization.founders", "organization.organization.founders"], ["location.location.partially_containedby", "base.schemastaging.disputed_location.claimed_by"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["sports.multi_event_tournament.participating_countries", "sports.sport_country.multi_event_tournaments_participated_in"], ["location.location.partially_contains", "location.location.partially_containedby"], ["sports.multi_event_tournament.participating_countries", "sports.multi_event_tournament.participating_countries"], ["location.location.partially_contains", "geography.lake.basin_countries"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["location.location.events", "location.location.events"], ["base.locations.continents.countries_within", "base.locations.countries.continent"], ["common.topic.notable_types", "common.topic.notable_types"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.tropical_cyclone.affected_areas"], ["location.location.containedby", "location.location.containedby"], ["language.human_language.main_country", "language.human_language.countries_spoken_in"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["location.location.time_zones", "time.time_zone.locations_in_this_time_zone"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["location.location.partially_containedby", "location.location.containedby"], ["location.location.partially_containedby", "base.locations.continents.countries_within"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["time.event.locations", "time.event.locations"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["periodicals.newspaper_circulation_area.newspapers", "book.newspaper.circulation_areas"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.cyclone_affected_area.cyclones"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["location.location.partially_contains", "location.location.containedby"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["government.form_of_government.countries", "location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries share a border with russia [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>"], "scores": [-0.035071082413196564, -0.03765379264950752, -0.04154002293944359], "norm_scores": [0.3343394100666046, 0.3334770202636719, 0.3321835696697235]}}
{"id": "WebQTest-284", "question": "where did the latin language originate from", "prediction": [["language.human_language.language_family"], ["base.rosetta.languoid.parent"], ["base.rosetta.languoid.child"]], "ground_paths": [["language.human_language.language_family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the latin language originate from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>", "<PATH> base.rosetta.languoid.child </PATH>"], "scores": [-0.004711420275270939, -0.033465493470430374, -0.050588782876729965], "norm_scores": [0.34166839718818665, 0.33198392391204834, 0.326347678899765]}}
{"id": "WebQTest-286", "question": "what school did karl benz go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did karl benz go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.01787855662405491, -0.02094646356999874, -0.02097102254629135], "norm_scores": [0.33401817083358765, 0.3329950273036957, 0.33298683166503906]}}
{"id": "WebQTest-287", "question": "what movies has carmen electra been in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["film.actor.film", "film.film.starring"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.person_or_entity_appearing_in_film.films", "film.personal_film_appearance.film"], ["film.actor.film", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.film_character.portrayed_in_films", "film.film.starring"], ["film.performance.actor", "film.film.starring"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies has carmen electra been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.02124171331524849, -0.02665361389517784, -0.037901923060417175], "norm_scores": [0.33578675985336304, 0.3339744210243225, 0.33023881912231445]}}
{"id": "WebQTest-289", "question": "when do world war ii end", "prediction": [["time.event.included_in_event"], ["time.event.includes_event"], ["time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when do world war ii end [/INST]", "raw_output": {"paths": ["<PATH> time.event.included_in_event </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.009864559397101402, -0.03576810285449028, -0.04331546276807785], "norm_scores": [0.33995896577835083, 0.3312658965587616, 0.3287751376628876]}}
{"id": "WebQTest-291", "question": "where did morgan freeman graduate", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did morgan freeman graduate [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.012757961638271809, -0.012770447880029678, -0.04257722944021225], "norm_scores": [0.3366313576698303, 0.33662718534469604, 0.32674145698547363]}}
{"id": "WebQTest-294", "question": "who was carolus linnaeus", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was carolus linnaeus [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0008858962100930512, -0.07022550702095032, -0.07838001847267151], "norm_scores": [0.3498408794403076, 0.32640498876571655, 0.32375410199165344]}}
{"id": "WebQTest-295", "question": "what type of government does germany have now", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does germany have now [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.009665423072874546, -0.014389180578291416, -0.14035220444202423], "norm_scores": [0.3480948805809021, 0.3464544415473938, 0.3054506778717041]}}
{"id": "WebQTest-296", "question": "what kind of government does libya have today", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does libya have today [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.005729139782488346, -0.021115466952323914, -0.1490781158208847], "norm_scores": [0.35073161125183105, 0.34537646174430847, 0.30389198660850525]}}
{"id": "WebQTest-297", "question": "what time zone is anaheim california", "prediction": [["location.location.time_zones"], ["location.location.containedby"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is anaheim california [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-1.0337850653741043e-05, -0.17139874398708344, -0.18289385735988617], "norm_scores": [0.3737815320491791, 0.31490883231163025, 0.3113096058368683]}}
{"id": "WebQTest-298", "question": "what year did the orioles go to the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the orioles go to the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.006865606177598238, -0.017933795228600502, -0.07608039677143097], "norm_scores": [0.3422173857688904, 0.33845055103302, 0.3193320333957672]}}
{"id": "WebQTest-300", "question": "who is sir francis bacon", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["common.topic.image"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is sir francis bacon [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> common.topic.image </PATH>"], "scores": [-0.00010871818085433915, -0.11799364537000656, -0.1475454419851303], "norm_scores": [0.3634096682071686, 0.32299789786338806, 0.31359240412712097]}}
{"id": "WebQTest-301", "question": "what were amelia earhart 's achievements", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["base.activism.activist.area_of_activism"]], "ground_paths": [["people.person.profession"], ["fictional_universe.fictional_character.occupation"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what were amelia earhart 's achievements [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>"], "scores": [-0.006260501686483622, -0.033979251980781555, -0.05157579481601715], "norm_scores": [0.3414883017539978, 0.33215266466140747, 0.3263590633869171]}}
{"id": "WebQTest-302", "question": "what are the official colors of the dallas cowboys", "prediction": [["sports.sports_team.colors"], ["sports.sports_team.colors", "sports.sports_team.colors"], ["sports.sports_team_colors.team"]], "ground_paths": [["sports.sports_team.colors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the official colors of the dallas cowboys [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.colors </PATH>", "<PATH> sports.sports_team.colors <SEP> sports.sports_team.colors </PATH>", "<PATH> sports.sports_team_colors.team </PATH>"], "scores": [-0.00010732505324995145, -0.09581319987773895, -0.11410976946353912], "norm_scores": [0.35701701045036316, 0.32443252205848694, 0.3185504972934723]}}
{"id": "WebQTest-303", "question": "where did margaret hoover go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did margaret hoover go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.009736672043800354, -0.024259185418486595, -0.02786705270409584], "norm_scores": [0.33697089552879333, 0.332112580537796, 0.33091652393341064]}}
{"id": "WebQTest-304", "question": "what is jamie oliver children names", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.parent"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is jamie oliver children names [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.011714499443769455, -0.016079867258667946, -0.1542162150144577], "norm_scores": [0.3493046760559082, 0.3477831780910492, 0.3029121458530426]}}
{"id": "WebQTest-305", "question": "where does bradley walsh live", "prediction": [["people.person.places_lived", "people.place_lived.location"], ["people.place_lived.person", "people.place_lived.location"], ["location.location.people_born_here"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does bradley walsh live [/INST]", "raw_output": {"paths": ["<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.017508935183286667, -0.017803847789764404, -0.03624444827437401], "norm_scores": [0.3354417085647583, 0.3353428244590759, 0.32921555638313293]}}
{"id": "WebQTest-306", "question": "who is the head coach of inter milan", "prediction": [["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"], ["base.schemastaging.sports_team_extra.managers", "base.schemastaging.sports_team_manager_tenure.manager"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"]], "ground_paths": [["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"], ["base.schemastaging.sports_team_manager_tenure.team", "base.schemastaging.sports_team_manager_tenure.manager"], ["soccer.football_team_management_tenure.team", "soccer.football_team_manager.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the head coach of inter milan [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> base.schemastaging.sports_team_extra.managers <SEP> base.schemastaging.sports_team_manager_tenure.manager </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach.teams_coached </PATH>"], "scores": [-0.019926516339182854, -0.022126799449324608, -0.024740813300013542], "norm_scores": [0.3341130018234253, 0.3333786427974701, 0.3325083553791046]}}
{"id": "WebQTest-308", "question": "who did reese witherspoon get married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["people.person.parents", "people.person.children"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did reese witherspoon get married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.017281239852309227, -0.02879677340388298, -0.03171283006668091], "norm_scores": [0.3362223207950592, 0.3323727548122406, 0.3314049541950226]}}
{"id": "WebQTest-310", "question": "what kind of money should i take to costa rica", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money should i take to costa rica [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>"], "scores": [-0.007656293921172619, -0.019025197252631187, -0.11780443042516708], "norm_scores": [0.34669291973114014, 0.3427737355232239, 0.310533344745636]}}
{"id": "WebQTest-311", "question": "where is the ufc headquarters", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["sports.sports_league.sport", "sports.sports_team.location"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"]], "ground_paths": [["common.topic.subject_of", "people.person.place_of_birth"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the ufc headquarters [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> sports.sports_league.sport <SEP> sports.sports_team.location </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>"], "scores": [-0.009698281064629555, -0.03801201283931732, -0.045649148523807526], "norm_scores": [0.3405100405216217, 0.33100414276123047, 0.3284858465194702]}}
{"id": "WebQTest-312", "question": "who plays captain kirk in star trek", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.quotations", "media_common.quotation.author"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays captain kirk in star trek [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.025805329903960228, -0.02801639214158058, -0.02940620854496956], "norm_scores": [0.33397936820983887, 0.3332417607307434, 0.3327789306640625]}}
{"id": "WebQTest-313", "question": "who does albert pujols play for 2012", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does albert pujols play for 2012 [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>"], "scores": [-0.010993437841534615, -0.023907486349344254, -0.02612382173538208], "norm_scores": [0.3364565074443817, 0.33213940262794495, 0.33140408992767334]}}
{"id": "WebQTest-314", "question": "what kind of government is sweden", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government is sweden [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.011401250958442688, -0.013234429992735386, -0.144999697804451], "norm_scores": [0.3480549156665802, 0.3474174439907074, 0.3045276403427124]}}
{"id": "WebQTest-315", "question": "where was country singer george jones born", "prediction": [["location.location.people_born_here"], ["music.artist.origin"], ["people.person.place_of_birth"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was country singer george jones born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.010390212759375572, -0.01914956048130989, -0.050056956708431244], "norm_scores": [0.3387088179588318, 0.3357549011707306, 0.32553631067276]}}
{"id": "WebQTest-316", "question": "what did shakespeare become famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["fictional_universe.fictional_character.occupation"]], "ground_paths": [["fictional_universe.fictional_character.occupation"], ["people.person.profession"], ["fictional_universe.character_occupation.characters_with_this_occupation"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did shakespeare become famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>"], "scores": [-0.005112789571285248, -0.03419650346040726, -0.0558304637670517], "norm_scores": [0.3422451615333557, 0.332434743642807, 0.3253200948238373]}}
{"id": "WebQTest-317", "question": "where does russia import from", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does russia import from [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>"], "scores": [-0.02779652550816536, -0.02851368673145771, -0.031081920489668846], "norm_scores": [0.3337780237197876, 0.3335387408733368, 0.3326832056045532]}}
{"id": "WebQTest-318", "question": "what is the money of switzerland called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the money of switzerland called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency </PATH>"], "scores": [-0.004133892711251974, -0.03059389628469944, -0.16585376858711243], "norm_scores": [0.3540366590023041, 0.3447916805744171, 0.3011716902256012]}}
{"id": "WebQTest-321", "question": "what do people in australia speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people in australia speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.006794396787881851, -0.022720882669091225, -0.07101521641016006], "norm_scores": [0.3422316312789917, 0.3368242681026459, 0.3209441006183624]}}
{"id": "WebQTest-322", "question": "what has ian somerhalder acted in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["film.actor.film", "film.film.starring"], ["film.film.produced_by"], ["film.producer.film"], ["film.actor.film", "film.performance.film"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["film.performance.actor", "film.film.starring"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has ian somerhalder acted in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.018833285197615623, -0.030924342572689056, -0.03449208289384842], "norm_scores": [0.3364233672618866, 0.33238014578819275, 0.3311964273452759]}}
{"id": "WebQTest-323", "question": "what movies did ron howard director", "prediction": [["film.film.directed_by"], ["film.film.produced_by"], ["film.director.film"]], "ground_paths": [["film.film.produced_by"], ["film.producer.film"], ["film.film.directed_by"], ["film.director.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did ron howard director [/INST]", "raw_output": {"paths": ["<PATH> film.film.directed_by </PATH>", "<PATH> film.film.produced_by </PATH>", "<PATH> film.director.film </PATH>"], "scores": [-0.022234858945012093, -0.031262416392564774, -0.03342268615961075], "norm_scores": [0.3355831503868103, 0.3325672745704651, 0.331849604845047]}}
{"id": "WebQTest-326", "question": "what did gerald r ford die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did gerald r ford die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>"], "scores": [-0.0012327624717727304, -0.048777174204587936, -0.10339435935020447], "norm_scores": [0.35008469223976135, 0.3338296115398407, 0.31608572602272034]}}
{"id": "WebQTest-327", "question": "what two continents is turkey on", "prediction": [["base.locations.countries.continent"], ["location.location.containedby"], ["location.location.partially_containedby"]], "ground_paths": [["base.locations.countries.continent"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what two continents is turkey on [/INST]", "raw_output": {"paths": ["<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.012319378554821014, -0.02918929047882557, -0.045880381017923355], "norm_scores": [0.33895230293273926, 0.3332821726799011, 0.3277655243873596]}}
{"id": "WebQTest-328", "question": "where did kevin love go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kevin love go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.015798449516296387, -0.02111457660794258, -0.027252130210399628], "norm_scores": [0.33519819378852844, 0.3334209620952606, 0.33138081431388855]}}
{"id": "WebQTest-329", "question": "what type of government was formed when italy unified", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["government.form_of_government.countries", "government.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government was formed when italy unified [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>"], "scores": [-0.01026402972638607, -0.012886357493698597, -0.11067429184913635], "norm_scores": [0.3446080684661865, 0.34370559453964233, 0.31168636679649353]}}
{"id": "WebQTest-331", "question": "where was the temple of karnak built", "prediction": [["location.location.containedby"], ["architecture.building.building_complex"], ["architecture.structure.architect"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the temple of karnak built [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> architecture.building.building_complex </PATH>", "<PATH> architecture.structure.architect </PATH>"], "scores": [-0.0017743565840646625, -0.06535134464502335, -0.07441749423742294], "norm_scores": [0.34863436222076416, 0.3271591365337372, 0.32420647144317627]}}
{"id": "WebQTest-333", "question": "what movies did morgan freeman star in", "prediction": [["people.person.nationality", "film.film.country"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["film.person_or_entity_appearing_in_film.films", "film.personal_film_appearance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["film.personal_film_appearance.person", "film.personal_film_appearance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.producer.film"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.producer.films_executive_produced"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.film.executive_produced_by"], ["theater.theater_actor.theater_roles", "theater.theater_role.play"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["common.topic.notable_types", "common.topic.notable_types"], ["tv.tv_actor.guest_roles", "tv.tv_guest_role.episodes_appeared_in"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did morgan freeman star in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.018669823184609413, -0.03744593635201454, -0.03819112852215767], "norm_scores": [0.33760207891464233, 0.3313223421573639, 0.3310755491256714]}}
{"id": "WebQTest-334", "question": "who did john kennedy have affairs with", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["music.recording.featured_artists", "music.recording.artist"], ["people.person.nationality", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did john kennedy have affairs with [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>"], "scores": [-0.02053324691951275, -0.033263932913541794, -0.047915998846292496], "norm_scores": [0.33779919147491455, 0.33352604508399963, 0.3286748230457306]}}
{"id": "WebQTest-335", "question": "what type of money to take to cuba", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["location.country.currency_used"], ["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of money to take to cuba [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.01085135992616415, -0.015274307690560818, -0.10930287092924118], "norm_scores": [0.3446105122566223, 0.3430896997451782, 0.3122997283935547]}}
{"id": "WebQTest-336", "question": "what currency should you take to morocco", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should you take to morocco [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.011810339987277985, -0.014108975417912006, -0.108219675719738], "norm_scores": [0.3441397547721863, 0.3433496057987213, 0.31251060962677]}}
{"id": "WebQTest-337", "question": "who is mary mcleod bethune for kids", "prediction": [["people.person.profession"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is mary mcleod bethune for kids [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.0005282887141220272, -0.08392184227705002, -0.08556630462408066], "norm_scores": [0.3523029386997223, 0.3241148293018341, 0.3235822916030884]}}
{"id": "WebQTest-338", "question": "what do they call money in japan", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do they call money in japan [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.013272475451231003, -0.017615439370274544, -0.0391286239027977], "norm_scores": [0.33668428659439087, 0.3352252244949341, 0.32809048891067505]}}
{"id": "WebQTest-339", "question": "what nationality is mother teresa", "prediction": [["people.person.nationality"], ["people.person.ethnicity"], ["people.ethnicity.people"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what nationality is mother teresa [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>"], "scores": [-0.005914417095482349, -0.036555759608745575, -0.06338616460561752], "norm_scores": [0.3431742191314697, 0.3328183591365814, 0.32400745153427124]}}
{"id": "WebQTest-340", "question": "who is richard pryor son", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.gender"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is richard pryor son [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.0033844399731606245, -0.0379374697804451, -0.0968412309885025], "norm_scores": [0.34760671854019165, 0.33580100536346436, 0.31659233570098877]}}
{"id": "WebQTest-341", "question": "what countries share borders with spain", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"]], "ground_paths": [["location.location.primarily_containedby", "location.location.contains_major_portion_of"], ["location.country.currency_formerly_used", "finance.currency.countries_formerly_used"], ["location.country.form_of_government", "government.form_of_government.countries"], ["finance.currency.countries_used", "location.country.currency_used"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "location.location.primarily_containedby"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["film.film.country", "film.film.country"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["freebase.valuenotation.has_value", "freebase.valuenotation.is_reviewed"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.primarily_containedby", "location.location.containedby"], ["location.location.primarily_containedby", "base.locations.continents.countries_within"], ["biology.animal_breed.place_of_origin", "biology.animal_breed.place_of_origin"], ["freebase.valuenotation.has_no_value", "freebase.valuenotation.has_no_value"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.containedby", "location.location.contains_major_portion_of"], ["location.location.containedby", "location.location.partially_containedby"], ["geography.river.basin_countries", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_contains"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_contains", "location.location.partially_contains"], ["organization.organization.founders", "organization.organization.founders"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.primarily_containedby", "location.location.primarily_containedby"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["location.location.partially_contains", "location.location.partially_containedby"], ["royalty.royal_line.kingdom_s_ruled", "people.family.country"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.country.internet_tld", "location.country.internet_tld"], ["location.location.partially_contains", "location.location.contains"], ["biology.breed_origin.breeds_originating_here", "biology.breed_origin.breeds_originating_here"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["location.country.form_of_government", "location.country.form_of_government"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.location.partially_containedby", "location.location.containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["government.form_of_government.countries", "location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries share borders with spain [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>"], "scores": [-0.03565997630357742, -0.04073954001069069, -0.045066092163324356], "norm_scores": [0.3349442481994629, 0.33324721455574036, 0.33180850744247437]}}
{"id": "WebQTest-342", "question": "what countries have english as their official language", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries have english as their official language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.017114851623773575, -0.020297350361943245, -0.022485485300421715], "norm_scores": [0.3342842161655426, 0.33322206139564514, 0.33249372243881226]}}
{"id": "WebQTest-343", "question": "what international organizations is china part of", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization_member.member_of", "organization.organization_membership.organization"], ["organization.organization_membership.member", "organization.organization_membership.organization"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"], ["organization.organization_membership.member", "organization.organization_membership.organization"], ["organization.organization_member.member_of", "organization.organization_membership.organization"], ["organization.organization_membership.member", "organization.membership_organization.members"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what international organizations is china part of [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization_member.member_of <SEP> organization.organization_membership.organization </PATH>", "<PATH> organization.organization_membership.member <SEP> organization.organization_membership.organization </PATH>"], "scores": [-0.02448263019323349, -0.02562107890844345, -0.028987593948841095], "norm_scores": [0.3339603543281555, 0.3335803747177124, 0.3324592709541321]}}
{"id": "WebQTest-344", "question": "what county is west st paul in", "prediction": [["location.location.containedby"], ["location.us_county.county_seat"], ["location.hud_county_place.county"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is west st paul in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.013332962058484554, -0.02529735676944256, -0.04148838669061661], "norm_scores": [0.3377985656261444, 0.33378109335899353, 0.32842034101486206]}}
{"id": "WebQTest-346", "question": "who does donnie wahlberg play in the sixth sense", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.film.starring", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does donnie wahlberg play in the sixth sense [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.film.starring <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.020309604704380035, -0.030516652390360832, -0.03766433894634247], "norm_scores": [0.3364013433456421, 0.33298513293266296, 0.33061352372169495]}}
{"id": "WebQTest-347", "question": "what did baron de montesquie influence", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did baron de montesquie influence [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>"], "scores": [-0.004884485155344009, -0.02305918000638485, -0.10803956538438797], "norm_scores": [0.34674346446990967, 0.3404984176158905, 0.31275811791419983]}}
{"id": "WebQTest-349", "question": "who did carlos boozer play for", "prediction": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"]], "ground_paths": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did carlos boozer play for [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.009554837830364704, -0.015354669652879238, -0.05301973596215248], "norm_scores": [0.33878976106643677, 0.3368305265903473, 0.32437974214553833]}}
{"id": "WebQTest-351", "question": "who owns the portland press herald", "prediction": [["book.newspaper.owner"], ["periodicals.newspaper_circulation_area.newspapers", "business.business_operation.industry"], ["book.newspaper_owner.newspapers_owned"]], "ground_paths": [["book.newspaper_owner.newspapers_owned"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who owns the portland press herald [/INST]", "raw_output": {"paths": ["<PATH> book.newspaper.owner </PATH>", "<PATH> periodicals.newspaper_circulation_area.newspapers <SEP> business.business_operation.industry </PATH>", "<PATH> book.newspaper_owner.newspapers_owned </PATH>"], "scores": [-0.022662386298179626, -0.0374530665576458, -0.038603026419878006], "norm_scores": [0.3367565870285034, 0.33181238174438477, 0.3314310312271118]}}
{"id": "WebQTest-352", "question": "what did albert speer design", "prediction": [["architecture.architect.structures_designed"], ["architecture.structure.architect"], ["architecture.architecture.architect"]], "ground_paths": [["architecture.structure.architect"], ["architecture.architect.structures_designed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did albert speer design [/INST]", "raw_output": {"paths": ["<PATH> architecture.architect.structures_designed </PATH>", "<PATH> architecture.structure.architect </PATH>", "<PATH> architecture.architecture.architect </PATH>"], "scores": [-0.0006086868233978748, -0.08461945503950119, -0.10758986324071884], "norm_scores": [0.3548661172389984, 0.3262714743614197, 0.31886231899261475]}}
{"id": "WebQTest-354", "question": "what art movement did leonardo da vinci belong to", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what art movement did leonardo da vinci belong to [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.004193174187093973, -0.025125423446297646, -0.060799021273851395], "norm_scores": [0.3419678211212158, 0.33488404750823975, 0.32314810156822205]}}
{"id": "WebQTest-356", "question": "what airport is near arlington tx", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["location.location.containedby"]], "ground_paths": [["aviation.airport.serves"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what airport is near arlington tx [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.010937867686152458, -0.02148442715406418, -0.0432734452188015], "norm_scores": [0.3381016254425049, 0.3345545828342438, 0.32734382152557373]}}
{"id": "WebQTest-359", "question": "what team does heskey play for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does heskey play for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.02769387513399124, -0.029150741174817085, -0.03263232484459877], "norm_scores": [0.334043949842453, 0.3335576355457306, 0.33239835500717163]}}
{"id": "WebQTest-360", "question": "where did emperor hirohito live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.places_lived", "people.place_lived.location"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did emperor hirohito live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>"], "scores": [-0.023452216759324074, -0.025387460365891457, -0.02848578430712223], "norm_scores": [0.33410781621932983, 0.3334618806838989, 0.33243027329444885]}}
{"id": "WebQTest-361", "question": "what to do and see in vienna austria", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["base.arthist.int_zm_nyek.helynev"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do and see in vienna austria [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.013291239738464355, -0.018958680331707, -0.02474900335073471], "norm_scores": [0.33523792028427124, 0.33334335684776306, 0.3314187526702881]}}
{"id": "WebQTest-362", "question": "when did conflict start in ireland", "prediction": [["time.event.locations"], ["base.culturalevent.event.entity_involved"], ["location.location.events"]], "ground_paths": [["time.event.locations"], ["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did conflict start in ireland [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.0051033589988946915, -0.043101001530885696, -0.0489068478345871], "norm_scores": [0.34248247742652893, 0.3297131061553955, 0.3278043866157532]}}
{"id": "WebQTest-365", "question": "what is there to do in peoria illinois", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is there to do in peoria illinois [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.010564420372247696, -0.017342524603009224, -0.04305807128548622], "norm_scores": [0.3376925587654114, 0.33541139960289, 0.3268960416316986]}}
{"id": "WebQTest-366", "question": "what language do british speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do british speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.016170956194400787, -0.020524196326732635, -0.03716466575860977], "norm_scores": [0.33614784479141235, 0.3346877098083496, 0.32916444540023804]}}
{"id": "WebQTest-367", "question": "what makes elvis presley famous", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["music.artist.genre"]], "ground_paths": [["people.person.profession"], ["fictional_universe.fictional_character.occupation"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what makes elvis presley famous [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> music.artist.genre </PATH>"], "scores": [-0.0044001382775604725, -0.04253363236784935, -0.06733261793851852], "norm_scores": [0.344638466835022, 0.33174362778663635, 0.3236179053783417]}}
{"id": "WebQTest-370", "question": "what language do people speak in the netherlands", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people speak in the netherlands [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.013577265664935112, -0.016609296202659607, -0.041059158742427826], "norm_scores": [0.3367156684398651, 0.335696280002594, 0.3275880813598633]}}
{"id": "WebQTest-371", "question": "what are the major languages spoken in greece", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the major languages spoken in greece [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.016748247668147087, -0.018837781623005867, -0.02642778493463993], "norm_scores": [0.3346406817436218, 0.333942174911499, 0.33141714334487915]}}
{"id": "WebQTest-374", "question": "what type of artist is henri matisse", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_art_form.artists"], ["visual_art.art_form.artists"]], "ground_paths": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_art_form.artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of artist is henri matisse [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>", "<PATH> visual_art.art_form.artists </PATH>"], "scores": [-0.0006287326104938984, -0.07363693416118622, -0.08787191659212112], "norm_scores": [0.35136449337005615, 0.3266260325908661, 0.32200947403907776]}}
{"id": "WebQTest-375", "question": "what was the first book charles dickens wrote", "prediction": [["book.written_work.author"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["fictional_universe.fictional_character.character_created_by"]], "ground_paths": [["fictional_universe.fictional_character_creator.fictional_characters_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the first book charles dickens wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>"], "scores": [-0.0029553393833339214, -0.0354347750544548, -0.05702592432498932], "norm_scores": [0.3430052101612091, 0.3320435583591461, 0.3249512314796448]}}
{"id": "WebQTest-376", "question": "what books did agatha christie wrote", "prediction": [["book.book_edition.author_editor"], ["book.written_work.author"], ["book.author.works_written"]], "ground_paths": [["film.film_story_contributor.film_story_credits"], ["film.film.story_by"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what books did agatha christie wrote [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.015111665241420269, -0.01833033375442028, -0.04005734995007515], "norm_scores": [0.336456835269928, 0.33537566661834717, 0.32816752791404724]}}
{"id": "WebQTest-377", "question": "what school did michael jordan attend", "prediction": [["people.person.education", "education.education.institution"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did michael jordan attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.018491094931960106, -0.022586237639188766, -0.02560584433376789], "norm_scores": [0.3345797657966614, 0.3332124352455139, 0.3322077691555023]}}
{"id": "WebQTest-379", "question": "what year does hitler die", "prediction": [["people.deceased_person.cause_of_death"], ["people.deceased_person.place_of_death"], ["base.activism.activist.area_of_activism"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year does hitler die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>"], "scores": [-0.012875072658061981, -0.021329911425709724, -0.0389559231698513], "norm_scores": [0.3371729552745819, 0.33433422446250916, 0.3284928798675537]}}
{"id": "WebQTest-380", "question": "where is mount st helens volcano", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.mountain.mountain_type"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mount st helens volcano [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.mountain.mountain_type </PATH>"], "scores": [-0.007643796503543854, -0.028888018801808357, -0.04651571065187454], "norm_scores": [0.3400372862815857, 0.3328896760940552, 0.32707300782203674]}}
{"id": "WebQTest-381", "question": "what shows are shot in new york", "prediction": [["film.film.featured_film_locations"], ["tv.tv_program.filming_locations"], ["tv.tv_location.tv_shows_filmed_here"]], "ground_paths": [["tv.tv_program.filming_locations"], ["film.film.featured_film_locations"], ["tv.tv_location.tv_shows_filmed_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what shows are shot in new york [/INST]", "raw_output": {"paths": ["<PATH> film.film.featured_film_locations </PATH>", "<PATH> tv.tv_program.filming_locations </PATH>", "<PATH> tv.tv_location.tv_shows_filmed_here </PATH>"], "scores": [-0.015756579115986824, -0.02207080088555813, -0.02208617329597473], "norm_scores": [0.33473965525627136, 0.3326326906681061, 0.3326275944709778]}}
{"id": "WebQTest-382", "question": "what book did tupac write", "prediction": [["book.book_edition.author_editor"], ["book.written_work.author"], ["book.author.works_written"]], "ground_paths": [["book.author.works_written"], ["book.book_edition.author_editor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what book did tupac write [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.01447666808962822, -0.018704092130064964, -0.03956814482808113], "norm_scores": [0.33658677339553833, 0.33516690135002136, 0.3282463848590851]}}
{"id": "WebQTest-383", "question": "what language do chinese people write in", "prediction": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.language_writing_system.languages"], ["language.human_language.writing_system"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do chinese people write in [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.008318732492625713, -0.03391772508621216, -0.039472468197345734], "norm_scores": [0.3396678566932678, 0.3310830593109131, 0.3292490839958191]}}
{"id": "WebQTest-384", "question": "what was robert burns famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["influence.influence_node.influenced_by", "people.person.profession"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was robert burns famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>"], "scores": [-0.005972651764750481, -0.027442285791039467, -0.07153638452291489], "norm_scores": [0.3430180251598358, 0.33573204278945923, 0.3212498724460602]}}
{"id": "WebQTest-385", "question": "when did william mckinley died", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.cause_of_death"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did william mckinley died [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.003680391004309058, -0.0369090810418129, -0.05259253457188606], "norm_scores": [0.3425147831439972, 0.33132046461105347, 0.32616475224494934]}}
{"id": "WebQTest-386", "question": "how deep is lake merritt oakland", "prediction": [["geography.lake.basin_countries"], ["location.location.containedby"], ["geography.lake.depth"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how deep is lake merritt oakland [/INST]", "raw_output": {"paths": ["<PATH> geography.lake.basin_countries </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> geography.lake.depth </PATH>"], "scores": [-0.011632065288722515, -0.039257366210222244, -0.04619939252734184], "norm_scores": [0.3402777314186096, 0.33100610971450806, 0.3287162184715271]}}
{"id": "WebQTest-388", "question": "what are the important holidays of islam", "prediction": [["time.holiday.featured_in_religions"], ["religion.religion.holidays"], ["location.location.events"]], "ground_paths": [["religion.religion.holidays"], ["time.holiday.featured_in_religions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the important holidays of islam [/INST]", "raw_output": {"paths": ["<PATH> time.holiday.featured_in_religions </PATH>", "<PATH> religion.religion.holidays </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.007992222905158997, -0.016868554055690765, -0.14494353532791138], "norm_scores": [0.349262535572052, 0.3461760878562927, 0.3045613765716553]}}
{"id": "WebQTest-389", "question": "what type of music did claude debussy play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"]], "ground_paths": [["music.artist.genre"], ["music.genre.artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did claude debussy play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.00409657321870327, -0.030625272542238235, -0.09643863886594772], "norm_scores": [0.3465467691421509, 0.33747419714927673, 0.3159789741039276]}}
{"id": "WebQTest-390", "question": "when was the last time the toronto maple leafs were in the stanley cup finals", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the toronto maple leafs were in the stanley cup finals [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.010715234093368053, -0.014799337834119797, -0.02549787424504757], "norm_scores": [0.33542972803115845, 0.3340626060962677, 0.33050766587257385]}}
{"id": "WebQTest-392", "question": "what team is hank baskett on 2010", "prediction": [["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"]], "ground_paths": [["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is hank baskett on 2010 [/INST]", "raw_output": {"paths": ["<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>"], "scores": [-0.015590895898640156, -0.019332418218255043, -0.02061508782207966], "norm_scores": [0.3343079686164856, 0.33305948972702026, 0.33263254165649414]}}
{"id": "WebQTest-393", "question": "what are the two official languages of paraguay", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the two official languages of paraguay [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.014910119585692883, -0.025800399482250214, -0.02813049964606762], "norm_scores": [0.3360174894332886, 0.33237800002098083, 0.3316044509410858]}}
{"id": "WebQTest-395", "question": "what state is the steelers from", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.location"], ["sports.sports_team.location", "location.location.containedby"]], "ground_paths": [["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state is the steelers from [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team.location <SEP> location.location.containedby </PATH>"], "scores": [-0.0040659112855792046, -0.03125879168510437, -0.09501360356807709], "norm_scores": [0.3464716374874115, 0.337177038192749, 0.3163512945175171]}}
{"id": "WebQTest-396", "question": "what college did sir isaac newton go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did sir isaac newton go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.010305116884410381, -0.018620161339640617, -0.02582572214305401], "norm_scores": [0.33598557114601135, 0.33320337533950806, 0.33081111311912537]}}
{"id": "WebQTest-397", "question": "what are the school colors for harvard university", "prediction": [["education.educational_institution.colors"], ["education.school_mascot.school"], ["education.educational_institution.colors", "education.educational_institution.colors"]], "ground_paths": [["education.educational_institution.colors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the school colors for harvard university [/INST]", "raw_output": {"paths": ["<PATH> education.educational_institution.colors </PATH>", "<PATH> education.school_mascot.school </PATH>", "<PATH> education.educational_institution.colors <SEP> education.educational_institution.colors </PATH>"], "scores": [-0.0032948742154985666, -0.047872740775346756, -0.06349409371614456], "norm_scores": [0.34506818652153015, 0.3300235867500305, 0.3249082565307617]}}
{"id": "WebQTest-398", "question": "where is the university of the rockies located", "prediction": [["location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["education.educational_institution.parent_institution", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the university of the rockies located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>"], "scores": [-0.002763462020084262, -0.04972553625702858, -0.06320098787546158], "norm_scores": [0.34536635875701904, 0.32952216267585754, 0.3251115083694458]}}
{"id": "WebQTest-399", "question": "what disease does robin roberts have", "prediction": [["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.notable_people_with_this_condition"], ["people.person.medical_condition"]], "ground_paths": [["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.notable_people_with_this_condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what disease does robin roberts have [/INST]", "raw_output": {"paths": ["<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.person.medical_condition </PATH>"], "scores": [-0.010585594922304153, -0.01219676062464714, -0.09693203121423721], "norm_scores": [0.3429747521877289, 0.34242260456085205, 0.31460264325141907]}}
{"id": "WebQTest-400", "question": "what was the capital city of the east roman empire", "prediction": [["location.location.containedby"], ["location.country.capital"], ["location.location.contains"]], "ground_paths": [["location.country.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the capital city of the east roman empire [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.country.capital </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0067526246421039104, -0.029158111661672592, -0.051309626549482346], "norm_scores": [0.3408008813858032, 0.3332499861717224, 0.32594916224479675]}}
{"id": "WebQTest-401", "question": "who plays edward scissorhands", "prediction": [["film.film.country", "people.person.nationality"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays edward scissorhands [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.016883162781596184, -0.042455218732357025, -0.04937080293893814], "norm_scores": [0.3398139476776123, 0.3312343955039978, 0.3289516270160675]}}
{"id": "WebQTest-402", "question": "what does joey jordison play in slipknot", "prediction": [["music.group_membership.group", "music.group_membership.role"], ["music.group_member.instruments_played"], ["music.group_member.instruments_played", "music.group_member.instruments_played"]], "ground_paths": [["music.group_member.instruments_played"], ["music.group_membership.group", "music.group_membership.role"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does joey jordison play in slipknot [/INST]", "raw_output": {"paths": ["<PATH> music.group_membership.group <SEP> music.group_membership.role </PATH>", "<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.group_member.instruments_played <SEP> music.group_member.instruments_played </PATH>"], "scores": [-0.017913799732923508, -0.018838590011000633, -0.04251423478126526], "norm_scores": [0.3361598551273346, 0.3358491063117981, 0.3279910385608673]}}
{"id": "WebQTest-403", "question": "what kind of language does turkey speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language does turkey speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.006846837233752012, -0.03016379103064537, -0.04786333069205284], "norm_scores": [0.34051060676574707, 0.33266279101371765, 0.3268266022205353]}}
{"id": "WebQTest-404", "question": "what was lebron james first team", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was lebron james first team [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>"], "scores": [-0.013744913972914219, -0.016459310427308083, -0.029486743733286858], "norm_scores": [0.3353824317455292, 0.33447331190109253, 0.3301442265510559]}}
{"id": "WebQTest-405", "question": "who fought the battle of gettysburg", "prediction": [["base.culturalevent.event.entity_involved"], ["military.military_conflict.military_personnel_involved"], ["military.military_person.participated_in_conflicts"]], "ground_paths": [["base.culturalevent.event.entity_involved"], ["location.location.events", "location.administrative_division.second_level_division_of"], ["military.military_conflict.combatants", "military.military_combatant_group.combatants"], ["military.military_person.participated_in_conflicts", "people.person.nationality"], ["base.culturalevent.event.entity_involved", "people.person.nationality"], ["military.military_conflict.military_personnel_involved", "people.person.nationality"], ["time.event.included_in_event", "time.event.locations"], ["military.military_person.participated_in_conflicts"], ["book.written_work.subjects", "book.written_work.subjects"], ["military.military_command.military_conflict", "military.military_combatant.military_commanders"], ["location.location.events", "location.administrative_division.first_level_division_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who fought the battle of gettysburg [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>"], "scores": [-0.0067170667462050915, -0.02701595425605774, -0.04617634788155556], "norm_scores": [0.33999553322792053, 0.3331635892391205, 0.3268408179283142]}}
{"id": "WebQTest-406", "question": "who is the coach of the sf giants", "prediction": [["baseball.baseball_team.current_coaches"], ["baseball.baseball_team.current_manager"], ["baseball.baseball_team.current_coach"]], "ground_paths": [["baseball.current_coaching_tenure.baseball_team", "baseball.current_coaching_tenure.baseball_coach"], ["baseball.current_coaching_tenure.baseball_team", "baseball.baseball_coach.current_team_coaching"], ["baseball.baseball_team.current_coaches", "baseball.current_coaching_tenure.baseball_coach"], ["baseball.baseball_team.current_coaches", "baseball.baseball_coach.current_team_coaching"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the coach of the sf giants [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_team.current_coaches </PATH>", "<PATH> baseball.baseball_team.current_manager </PATH>", "<PATH> baseball.baseball_team.current_coach </PATH>"], "scores": [-0.011096078902482986, -0.028867485001683235, -0.048093222081661224], "norm_scores": [0.339435875415802, 0.3334569036960602, 0.32710719108581543]}}
{"id": "WebQTest-408", "question": "who was the first president of the afl", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["organization.organization.geographic_scope", "people.person.nationality"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the first president of the afl [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.geographic_scope <SEP> people.person.nationality </PATH>"], "scores": [-0.0005953415529802442, -0.08141498267650604, -0.08634015172719955], "norm_scores": [0.3520893156528473, 0.3247531056404114, 0.32315757870674133]}}
{"id": "WebQTest-409", "question": "where did giuliana rancic grow up", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did giuliana rancic grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.003435660619288683, -0.0413985475897789, -0.04789077863097191], "norm_scores": [0.34255170822143555, 0.32979121804237366, 0.3276570737361908]}}
{"id": "WebQTest-410", "question": "what are the major cities in ukraine", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.country.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the major cities in ukraine [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.020692935213446617, -0.0230100154876709, -0.032121263444423676], "norm_scores": [0.33486002683639526, 0.33408501744270325, 0.3310549259185791]}}
{"id": "WebQTest-414", "question": "who does brian dawkins play for 2011", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does brian dawkins play for 2011 [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.012652147561311722, -0.012792007997632027, -0.051416948437690735], "norm_scores": [0.3376282751560211, 0.337581068277359, 0.3247905969619751]}}
{"id": "WebQTest-415", "question": "what movies did tupac star in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["film.actor.film", "film.film.starring"], ["music.featured_artist.recordings", "music.recording.releases"], ["film.actor.film", "film.performance.film"], ["people.person.languages", "film.film.language"], ["music.recording.artist", "music.recording.releases"], ["film.performance.actor", "film.film.starring"], ["music.producer.releases_produced"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did tupac star in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.021145695820450783, -0.0234041940420866, -0.039404820650815964], "norm_scores": [0.3356098234653473, 0.3348527252674103, 0.3295374810695648]}}
{"id": "WebQTest-416", "question": "what form of currency does china have", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of currency does china have [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.01115405559539795, -0.015324068255722523, -0.12000841647386551], "norm_scores": [0.345697820186615, 0.34425926208496094, 0.3100428879261017]}}
{"id": "WebQTest-417", "question": "who plays kenneth", "prediction": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays kenneth [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>"], "scores": [-0.02294158563017845, -0.024907026439905167, -0.031820569187402725], "norm_scores": [0.3345380425453186, 0.3338811993598938, 0.3315808176994324]}}
{"id": "WebQTest-418", "question": "what league are the indianapolis colts in", "prediction": [["american_football.football_team.division"], ["sports.sports_team.league"], ["american_football.football_team.current_division"]], "ground_paths": [["sports.sports_team.sport", "sports.sports_league.sport"], ["american_football.football_team.division"], ["sports.sports_league_participation.team", "sports.sports_league_participation.league"], ["american_football.football_team.conference"], ["american_football.football_team.conference", "american_football.football_conference.league"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what league are the indianapolis colts in [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_team.division </PATH>", "<PATH> sports.sports_team.league </PATH>", "<PATH> american_football.football_team.current_division </PATH>"], "scores": [-0.017094500362873077, -0.03792033717036247, -0.048838693648576736], "norm_scores": [0.33919650316238403, 0.33220547437667847, 0.3285980820655823]}}
{"id": "WebQTest-419", "question": "what country did magellan represent", "prediction": [["people.person.nationality"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "people.person.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did magellan represent [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> people.person.nationality </PATH>"], "scores": [-0.0033263592049479485, -0.04592984914779663, -0.07754435390233994], "norm_scores": [0.34640905261039734, 0.3319608271121979, 0.3216301500797272]}}
{"id": "WebQTest-420", "question": "where is perpignan located", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.capital"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is perpignan located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.007518914993852377, -0.026212360709905624, -0.05821116268634796], "norm_scores": [0.3410581350326538, 0.3347418010234833, 0.3242000341415405]}}
{"id": "WebQTest-421", "question": "who did kim richards marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.spouse_s", "people.marriage.spouse"]], "ground_paths": [["people.person.parents", "people.person.children"], ["people.person.parents", "people.person.parents"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did kim richards marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>"], "scores": [-0.01723751612007618, -0.025993023067712784, -0.046691376715898514], "norm_scores": [0.33758029341697693, 0.33463749289512634, 0.32778221368789673]}}
{"id": "WebQTest-422", "question": "what happened in bosnia in the 90s", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["time.event.locations"], ["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what happened in bosnia in the 90s [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.006308111362159252, -0.03520330414175987, -0.05279311537742615], "norm_scores": [0.34175214171409607, 0.3320184648036957, 0.32622939348220825]}}
{"id": "WebQTest-423", "question": "when 's the last time the steelers won the superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when 's the last time the steelers won the superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.010200967080891132, -0.013715388253331184, -0.0525965541601181], "norm_scores": [0.33841150999069214, 0.3372242748737335, 0.32436421513557434]}}
{"id": "WebQTest-425", "question": "where english is spoken", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where english is spoken [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.013805422000586987, -0.024898234754800797, -0.029458940029144287], "norm_scores": [0.3363111615180969, 0.3326011300086975, 0.33108770847320557]}}
{"id": "WebQTest-426", "question": "where is giza pyramids located", "prediction": [["location.location.containedby"], ["architecture.building.building_complex"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is giza pyramids located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> architecture.building.building_complex </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.002468070713803172, -0.046646539121866226, -0.0797562301158905], "norm_scores": [0.3469323515892029, 0.33193904161453247, 0.32112860679626465]}}
{"id": "WebQTest-427", "question": "what is the zip code for midland tx", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for midland tx [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-7.62101772124879e-05, -0.10958004742860794, -0.1500234454870224], "norm_scores": [0.3627088665962219, 0.325088232755661, 0.31220290064811707]}}
{"id": "WebQTest-428", "question": "what states make up the midwest us", "prediction": [["location.location.contains"], ["location.location.containedby"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what states make up the midwest us [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.011610044166445732, -0.01597072370350361, -0.12944500148296356], "norm_scores": [0.34668153524398804, 0.34517306089401245, 0.3081453740596771]}}
{"id": "WebQTest-430", "question": "what instruments does justin bieber use", "prediction": [["music.artist.track_contributions", "music.track_contribution.role"], ["music.track_contribution.contributor", "music.track_contribution.role"], ["music.group_member.instruments_played"]], "ground_paths": [["music.artist.track_contributions", "music.track_contribution.role"], ["music.group_member.membership", "music.group_membership.role"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what instruments does justin bieber use [/INST]", "raw_output": {"paths": ["<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>", "<PATH> music.track_contribution.contributor <SEP> music.track_contribution.role </PATH>", "<PATH> music.group_member.instruments_played </PATH>"], "scores": [-0.004594853147864342, -0.03857315331697464, -0.050977401435375214], "norm_scores": [0.34231698513031006, 0.3308810293674469, 0.3268020451068878]}}
{"id": "WebQTest-431", "question": "what language does australians speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does australians speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.0077167293056845665, -0.02249966748058796, -0.06178998947143555], "norm_scores": [0.340984046459198, 0.3359803855419159, 0.3230355679988861]}}
{"id": "WebQTest-432", "question": "who did annie oakley married", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.marriage.spouse", "people.marriage.spouse"]], "ground_paths": [["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did annie oakley married [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>"], "scores": [-0.025701479986310005, -0.03739403560757637, -0.04016472026705742], "norm_scores": [0.3362456262111664, 0.332336962223053, 0.331417441368103]}}
{"id": "WebQTest-433", "question": "who has played lex luthor", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has played lex luthor [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.019643131643533707, -0.0255135428160429, -0.028081128373742104], "norm_scores": [0.3349248766899109, 0.3329644799232483, 0.3321106433868408]}}
{"id": "WebQTest-434", "question": "where does the zambezi river start", "prediction": [["geography.river.origin"], ["location.location.containedby"], []], "ground_paths": [["geography.river.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the zambezi river start [/INST]", "raw_output": {"paths": ["<PATH> geography.river.origin </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> </PATH>"], "scores": [-0.0003601202042773366, -0.10317210108041763, -0.11889640986919403], "norm_scores": [0.35835665464401245, 0.32334399223327637, 0.31829941272735596]}}
{"id": "WebQTest-436", "question": "what is the nigeria time", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the nigeria time [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.00017937632219400257, -0.0832986831665039, -0.11922356486320496], "norm_scores": [0.35612407326698303, 0.32772010564804077, 0.3161557912826538]}}
{"id": "WebQTest-437", "question": "where did they find jenni rivera 's body", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did they find jenni rivera 's body [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.0005041033728048205, -0.08307810127735138, -0.08407308161258698], "norm_scores": [0.35204190015792847, 0.3241402506828308, 0.3238178789615631]}}
{"id": "WebQTest-438", "question": "what university did romney graduated from", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.religion", "religion.religion.organizations"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what university did romney graduated from [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.014629162847995758, -0.02156251296401024, -0.023773470893502235], "norm_scores": [0.3351220190525055, 0.3328065276145935, 0.3320714831352234]}}
{"id": "WebQTest-439", "question": "where is the time zone line in south dakota", "prediction": [["location.location.time_zones"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the time zone line in south dakota [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0019108984852209687, -0.04685705527663231, -0.07985080778598785], "norm_scores": [0.347093403339386, 0.3318382799625397, 0.32106831669807434]}}
{"id": "WebQTest-440", "question": "where is jason mraz from", "prediction": [["music.artist.origin"], ["location.location.people_born_here"], ["people.person.place_of_birth"]], "ground_paths": [["music.artist.origin"], ["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is jason mraz from [/INST]", "raw_output": {"paths": ["<PATH> music.artist.origin </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.004775037989020348, -0.039337679743766785, -0.051635827869176865], "norm_scores": [0.34243667125701904, 0.33080336451530457, 0.3267599940299988]}}
{"id": "WebQTest-441", "question": "what killed john bonham", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["music.group_member.instruments_played", "people.deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what killed john bonham [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> music.group_member.instruments_played <SEP> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.0018225880339741707, -0.043928101658821106, -0.09394481778144836], "norm_scores": [0.3483395278453827, 0.33397701382637024, 0.31768345832824707]}}
{"id": "WebQTest-442", "question": "what industry does walmart operate in", "prediction": [["business.business_operation.industry"], ["business.industry.companies"], ["business.business_operation.industry", "business.business_operation.industry"]], "ground_paths": [["organization.organization.sectors"], ["business.industry.companies"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what industry does walmart operate in [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry </PATH>", "<PATH> business.industry.companies </PATH>", "<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>"], "scores": [-0.005585544742643833, -0.04838852211833, -0.0516115240752697], "norm_scores": [0.34327489137649536, 0.32889172434806824, 0.3278334140777588]}}
{"id": "WebQTest-443", "question": "what county is texarkana arkansas in", "prediction": [["location.location.containedby"], ["location.us_county.county_seat"], ["location.hud_county_place.county"]], "ground_paths": [["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is texarkana arkansas in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.008257328532636166, -0.021199051290750504, -0.0577751025557518], "norm_scores": [0.3402714729309082, 0.3358961343765259, 0.3238323926925659]}}
{"id": "WebQTest-446", "question": "what music period did beethoven live in", "prediction": [["music.artist.genre"], ["music.artist.period"], ["music.artist.periods"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what music period did beethoven live in [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.artist.period </PATH>", "<PATH> music.artist.periods </PATH>"], "scores": [-0.001676196581684053, -0.06845041364431381, -0.08986903727054596], "norm_scores": [0.3507552444934845, 0.32809871435165405, 0.32114601135253906]}}
{"id": "WebQTest-447", "question": "what did laura ingalls wilder wrote", "prediction": [["book.written_work.author"], ["book.book_edition.author_editor"], ["book.author.works_written"]], "ground_paths": [["book.author.works_written"], ["book.written_work.author"], ["book.book_edition.author_editor"], ["book.literary_series.author_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did laura ingalls wilder wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.010758320800960064, -0.02529451623558998, -0.03667794167995453], "norm_scores": [0.33783984184265137, 0.3329644501209259, 0.32919567823410034]}}
{"id": "WebQTest-448", "question": "what type of government system does italy have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government system does italy have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.009394899010658264, -0.0148248840123415, -0.14421184360980988], "norm_scores": [0.3486190438270569, 0.34673118591308594, 0.3046497702598572]}}
{"id": "WebQTest-449", "question": "what did john irving wrote", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.book_edition.author_editor"]], "ground_paths": [["book.book_edition.author_editor"], ["film.film.story_by"], ["film.film_story_contributor.film_story_credits"], ["book.written_work.author"], ["book.author.book_editions_published"], ["book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did john irving wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.015085561200976372, -0.025520801544189453, -0.029867377132177353], "norm_scores": [0.33614054322242737, 0.3326510787010193, 0.33120831847190857]}}
{"id": "WebQTest-450", "question": "what is the national flower of hawaii", "prediction": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"]], "ground_paths": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the national flower of hawaii [/INST]", "raw_output": {"paths": ["<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.010330606251955032, -0.01970769464969635, -0.02265641838312149], "norm_scores": [0.3357488811016083, 0.3326152265071869, 0.33163589239120483]}}
{"id": "WebQTest-451", "question": "what did alice paul accomplish", "prediction": [["base.activism.activist.area_of_activism"], ["base.activism.activism_issue.activists"], ["common.topic.notable_types"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did alice paul accomplish [/INST]", "raw_output": {"paths": ["<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> base.activism.activism_issue.activists </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.005655687768012285, -0.02732418291270733, -0.07263582199811935], "norm_scores": [0.3431970179080963, 0.3358404338359833, 0.3209625780582428]}}
{"id": "WebQTest-453", "question": "what languages do they speak in costa rica", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what languages do they speak in costa rica [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.004385855048894882, -0.028140943497419357, -0.08879608660936356], "norm_scores": [0.3453540802001953, 0.337246835231781, 0.31739911437034607]}}
{"id": "WebQTest-454", "question": "where did hank marvin come from", "prediction": [["people.person.place_of_birth"], ["music.artist.origin"], ["music.recording.artist"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did hank marvin come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.011421146802604198, -0.024943655356764793, -0.05726546421647072], "norm_scores": [0.3399326503276825, 0.3353668749332428, 0.3247005343437195]}}
{"id": "WebQTest-455", "question": "when did chipper jones get drafted", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.season"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did chipper jones get drafted [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.season </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>"], "scores": [-0.02166057378053665, -0.022901304066181183, -0.02491213195025921], "norm_scores": [0.3338325619697571, 0.3334186375141144, 0.3327488601207733]}}
{"id": "WebQTest-456", "question": "what part did winona ryder play in star trek", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.film_character.portrayed_in_films"]], "ground_paths": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.performance.character"], ["fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["common.topic.notable_types", "common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part did winona ryder play in star trek [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.02034490369260311, -0.02954901196062565, -0.029871387407183647], "norm_scores": [0.3354177474975586, 0.3323446810245514, 0.33223757147789]}}
{"id": "WebQTest-459", "question": "where did jovan belcher kill himself", "prediction": [["people.deceased_person.place_of_death"], ["location.location.events"], ["location.location.people_born_here", "location.location.containedby"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did jovan belcher kill himself [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.events </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>"], "scores": [-0.0019210056634619832, -0.0690130665898323, -0.08033011108636856], "norm_scores": [0.3496876060962677, 0.3269960582256317, 0.3233163058757782]}}
{"id": "WebQTest-461", "question": "where did clay matthews go to school", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did clay matthews go to school [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.012557762674987316, -0.014718769118189812, -0.03433471918106079], "norm_scores": [0.3359876573085785, 0.33526238799095154, 0.32874998450279236]}}
{"id": "WebQTest-462", "question": "what time is it in texas houston right now", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.containedby"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time is it in texas houston right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-1.621693627384957e-05, -0.1306682527065277, -0.15826644003391266], "norm_scores": [0.36614489555358887, 0.32130059599876404, 0.3125545084476471]}}
{"id": "WebQTest-463", "question": "who is princess leia in star wars", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is princess leia in star wars [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.011706431396305561, -0.029116030782461166, -0.0356716588139534], "norm_scores": [0.3379450738430023, 0.33211252093315125, 0.3299424350261688]}}
{"id": "WebQTest-464", "question": "what is the political structure of china", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["organization.organization.founders"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the political structure of china [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> organization.organization.founders </PATH>"], "scores": [-0.006934930104762316, -0.019585732370615005, -0.1643386334180832], "norm_scores": [0.3518911600112915, 0.34746748208999634, 0.30064138770103455]}}
{"id": "WebQTest-465", "question": "what is new york city airport", "prediction": [["aviation.airport.serves"], ["location.location.containedby"], ["location.location.nearby_airports"]], "ground_paths": [["aviation.airport.serves"], ["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is new york city airport [/INST]", "raw_output": {"paths": ["<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.nearby_airports </PATH>"], "scores": [-0.00717509351670742, -0.03126385435461998, -0.046517714858055115], "norm_scores": [0.34041157364845276, 0.3323094844818115, 0.32727891206741333]}}
{"id": "WebQTest-466", "question": "what language brazil speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language brazil speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.00019225401047151536, -0.10112764686346054, -0.11019641906023026], "norm_scores": [0.3571655750274658, 0.32287463545799255, 0.3199597895145416]}}
{"id": "WebQTest-467", "question": "who played obi wan in episode 2", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played obi wan in episode 2 [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.02897479198873043, -0.032967470586299896, -0.033994752913713455], "norm_scores": [0.33433547616004944, 0.3330032527446747, 0.33266133069992065]}}
{"id": "WebQTest-468", "question": "where did venus williams come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did venus williams come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.004190159495919943, -0.03816669061779976, -0.05041644722223282], "norm_scores": [0.34229928255081177, 0.3308645188808441, 0.3268361985683441]}}
{"id": "WebQTest-469", "question": "what currency is used in panama", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"], ["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency is used in panama [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.009434141218662262, -0.018212463706731796, -0.1550333946943283], "norm_scores": [0.35016897320747375, 0.34710854291915894, 0.3027225136756897]}}
{"id": "WebQTest-470", "question": "where does the zambezi river originate", "prediction": [["geography.river.origin"], ["location.location.containedby"], ["location.location.partial_containedby"]], "ground_paths": [["geography.river.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the zambezi river originate [/INST]", "raw_output": {"paths": ["<PATH> geography.river.origin </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partial_containedby </PATH>"], "scores": [-0.000545742514077574, -0.09386380016803741, -0.09420482814311981], "norm_scores": [0.35442179441452026, 0.32284414768218994, 0.3227340579032898]}}
{"id": "WebQTest-472", "question": "what team did ronaldo play for in 2003", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did ronaldo play for in 2003 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.01952534168958664, -0.02387656643986702, -0.037439536303281784], "norm_scores": [0.3358066976070404, 0.3343487083911896, 0.32984456419944763]}}
{"id": "WebQTest-473", "question": "where does robin williams live 2011", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.children", "people.person.place_of_birth"]], "ground_paths": [["film.film.produced_by", "film.film_location.featured_in_films"], ["people.person.nationality", "location.administrative_division.second_level_division_of"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.parents", "people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does robin williams live 2011 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.014634932391345501, -0.023101573809981346, -0.03013286553323269], "norm_scores": [0.335999995470047, 0.3331671953201294, 0.330832839012146]}}
{"id": "WebQTest-474", "question": "who played jacob black in twilight", "prediction": [["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_honor.honored_for", "award.award_winner.awards_won"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["film.film.starring", "film.actor.film"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["film.performance.character", "film.actor.film"], ["award.award_honor.honored_for", "award.award_winner.awards_won"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_honor.honored_for", "award.award_honor.award_winner"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played jacob black in twilight [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_honor.honored_for <SEP> award.award_winner.awards_won </PATH>"], "scores": [-0.02974703349173069, -0.031046142801642418, -0.032399773597717285], "norm_scores": [0.3337725102901459, 0.33333921432495117, 0.33288830518722534]}}
{"id": "WebQTest-477", "question": "which airport to fly into in buenos aires", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["location.location.containedby"]], "ground_paths": [["aviation.airport.serves"], ["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which airport to fly into in buenos aires [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.007395249791443348, -0.02605152316391468, -0.04576344043016434], "norm_scores": [0.33968833088874817, 0.33340975642204285, 0.32690197229385376]}}
{"id": "WebQTest-479", "question": "what team did adrian peterson play for in college", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did adrian peterson play for in college [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>"], "scores": [-0.010014893487095833, -0.017079366371035576, -0.03872496262192726], "norm_scores": [0.3373069763183594, 0.33493247628211975, 0.32776057720184326]}}
{"id": "WebQTest-480", "question": "where does name pennsylvania come from", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes", "symbols.namesake.named_after"]], "ground_paths": [["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does name pennsylvania come from [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes <SEP> symbols.namesake.named_after </PATH>"], "scores": [-0.004502827767282724, -0.0311250239610672, -0.09445296972990036], "norm_scores": [0.34629565477371216, 0.3371981680393219, 0.31650620698928833]}}
{"id": "WebQTest-481", "question": "what kind of money do they use in germany", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money do they use in germany [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.005204873625189066, -0.025629974901676178, -0.11272688955068588], "norm_scores": [0.34748297929763794, 0.34045758843421936, 0.3120594322681427]}}
{"id": "WebQTest-482", "question": "who plays blaine in batman", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays blaine in batman [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.014850403182208538, -0.02540040947496891, -0.0332179069519043], "norm_scores": [0.33655238151550293, 0.33302041888237, 0.3304271996021271]}}
{"id": "WebQTest-484", "question": "what language does australia use", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does australia use [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.008339019492268562, -0.021140139549970627, -0.06453285366296768], "norm_scores": [0.3409901261329651, 0.3366529047489166, 0.32235702872276306]}}
{"id": "WebQTest-487", "question": "who did veronica lake mary", "prediction": [["people.person.children"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"]], "ground_paths": [["people.person.parents", "people.person.parents"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did veronica lake mary [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>"], "scores": [-0.028338143602013588, -0.030297664925456047, -0.03800146281719208], "norm_scores": [0.33462437987327576, 0.3339693248271942, 0.3314063549041748]}}
{"id": "WebQTest-488", "question": "who did sir francis drake marry", "prediction": [["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.spouse_s", "people.marriage.spouse"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did sir francis drake marry [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.02629460208117962, -0.02912207692861557, -0.04045071080327034], "norm_scores": [0.33521947264671326, 0.33427301049232483, 0.3305075168609619]}}
{"id": "WebQTest-489", "question": "who plays stephanie plum in one for the money", "prediction": [["film.film.country", "people.person.nationality"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.film", "film.performance.actor"]], "ground_paths": [["film.film.executive_produced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays stephanie plum in one for the money [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>"], "scores": [-0.022833971306681633, -0.03387155756354332, -0.035767409950494766], "norm_scores": [0.3360019624233246, 0.3323137164115906, 0.33168429136276245]}}
{"id": "WebQTest-490", "question": "what did the ancient romans speak", "prediction": [["fictional_universe.fictional_setting.languages"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did the ancient romans speak [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_setting.languages </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.012675329111516476, -0.03106716088950634, -0.04124341532588005], "norm_scores": [0.33856847882270813, 0.3323984742164612, 0.3290330469608307]}}
{"id": "WebQTest-491", "question": "what does ringo sing", "prediction": [["music.recording.artist"], ["music.artist.track"], ["music.composition.composer"]], "ground_paths": [["music.featured_artist.recordings"], ["music.composer.compositions"], ["music.lyricist.lyrics_written"], ["theater.theatrical_composer.plays_composed", "film.film.sequel"], ["music.recording.featured_artists"], ["music.album.artist", "music.album.compositions"], ["music.album.artist"], ["music.artist.label", "music.record_label.releases"], ["music.artist.album"], ["people.person.nationality", "film.film.country"], ["music.recording.artist"], ["music.composition.composer"], ["organization.organization.founders", "film.film.production_companies"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does ringo sing [/INST]", "raw_output": {"paths": ["<PATH> music.recording.artist </PATH>", "<PATH> music.artist.track </PATH>", "<PATH> music.composition.composer </PATH>"], "scores": [-0.020408544689416885, -0.02318555861711502, -0.06021752208471298], "norm_scores": [0.3380434215068817, 0.33710598945617676, 0.32485058903694153]}}
{"id": "WebQTest-492", "question": "where was st. lucy born", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["symbols.namesake.named_after"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was st. lucy born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.00609616981819272, -0.025432096794247627, -0.08478451520204544], "norm_scores": [0.3442130386829376, 0.33762127161026, 0.31816571950912476]}}
{"id": "WebQTest-493", "question": "what type of government does australia have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does australia have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.007043755147606134, -0.019130757078528404, -0.13443616032600403], "norm_scores": [0.34862959384918213, 0.3444410562515259, 0.306929349899292]}}
{"id": "WebQTest-497", "question": "who was the father of king george vi", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.gender"]], "ground_paths": [["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the father of king george vi [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.012016339227557182, -0.018687110394239426, -0.0521034337580204], "norm_scores": [0.3385174870491028, 0.3362668454647064, 0.3252156972885132]}}
{"id": "WebQTest-500", "question": "what is the zip code for trenton", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for trenton [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-9.65113504207693e-05, -0.1041448637843132, -0.17004790902137756], "norm_scores": [0.36431363224983215, 0.32831281423568726, 0.3073735237121582]}}
{"id": "WebQTest-501", "question": "what has ashley greene been in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["film.actor.film", "film.film.starring"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.performance.film"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has ashley greene been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.017422569915652275, -0.032162927091121674, -0.03538615256547928], "norm_scores": [0.33697664737701416, 0.33204594254493713, 0.3309774100780487]}}
{"id": "WebQTest-502", "question": "where did al qaeda attack", "prediction": [["base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["organization.organization_founder.organizations_founded", "base.culturalevent.event.entity_involved"]], "ground_paths": [["film.film.subjects", "film.film.subjects"], ["military.military_combatant.military_conflicts", "military.military_combatant_group.conflict"], ["organization.organization_founder.organizations_founded", "base.culturalevent.event.entity_involved"], ["base.disaster2.attack_process.attacker", "base.disaster2.attack.attack"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["base.terrorism.terrorist_organization.involved_in_attacks"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did al qaeda attack [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.018745992332696915, -0.032135576009750366, -0.03491029143333435], "norm_scores": [0.33662495017051697, 0.3321477174758911, 0.3312273621559143]}}
{"id": "WebQTest-503", "question": "what language is spoken in switzerland", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is spoken in switzerland [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.017537806183099747, -0.021039258688688278, -0.023244021460413933], "norm_scores": [0.33435705304145813, 0.33318835496902466, 0.3324545621871948]}}
{"id": "WebQTest-504", "question": "who was philip in acts chapter 8", "prediction": [["people.person.gender", "people.person.gender"], ["people.person.profession"], ["people.person.gender", "fictional_universe.fictional_character.gender"]], "ground_paths": [["fictional_universe.person_in_fiction.representations_in_fiction"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was philip in acts chapter 8 [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>"], "scores": [-0.01419199351221323, -0.017012957483530045, -0.056918319314718246], "norm_scores": [0.33836865425109863, 0.3374154567718506, 0.3242158889770508]}}
{"id": "WebQTest-506", "question": "what the time zone in england", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.country.form_of_government"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the time zone in england [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.0012709078146144748, -0.04616142064332962, -0.1482958346605301], "norm_scores": [0.3546885848045349, 0.3391185402870178, 0.30619290471076965]}}
{"id": "WebQTest-508", "question": "what is the best currency to take to turkey", "prediction": [["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"], ["location.country.currency_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the best currency to take to turkey [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.01676393672823906, -0.01750136911869049, -0.03737249597907066], "norm_scores": [0.33569830656051636, 0.3354508578777313, 0.3288508355617523]}}
{"id": "WebQTest-509", "question": "what system of government is practiced in nigeria", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.continent"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what system of government is practiced in nigeria [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.0054328409023582935, -0.021335691213607788, -0.14262661337852478], "norm_scores": [0.3501370847225189, 0.3446129262447357, 0.305249959230423]}}
{"id": "WebQTest-512", "question": "who were mary shelley", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["symbols.namesake.named_after"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were mary shelley [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.0017999231349676847, -0.0705452412366867, -0.0744781643152237], "norm_scores": [0.34922733902931213, 0.32602620124816895, 0.3247464895248413]}}
{"id": "WebQTest-513", "question": "what language do most australians speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do most australians speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.006627146154642105, -0.024129396304488182, -0.06339050084352493], "norm_scores": [0.34159213304519653, 0.3356655240058899, 0.3227422833442688]}}
{"id": "WebQTest-514", "question": "where is denmark situated", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is denmark situated [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.019825030118227005, -0.021443122997879982, -0.03488868102431297], "norm_scores": [0.33518439531326294, 0.3346424698829651, 0.3301731050014496]}}
{"id": "WebQTest-515", "question": "where was teddy roosevelt 's house", "prediction": [["symbols.name_source.namesakes", "location.location.containedby"], ["symbols.namesake.named_after", "location.location.containedby"], ["people.person.parents", "people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth", "base.aareas.schema.administrative_area.administrative_children"], ["symbols.namesake.named_after", "location.location.containedby"], ["symbols.name_source.namesakes", "location.location.containedby"], ["book.book_edition.author_editor", "book.book_edition.place_of_publication"], ["book.written_work.subjects", "location.location.containedby"], ["book.written_work.author", "base.biblioness.bibs_location.state"], ["base.inaugurations.inauguration.president", "time.event.locations"], ["people.place_lived.person", "people.place_lived.location"], ["book.written_work.author", "people.deceased_person.place_of_death"], ["symbols.namesake.named_after", "transportation.bridge.locale"], ["people.person.children", "people.deceased_person.place_of_death"], ["people.person.parents", "people.deceased_person.place_of_death"], ["people.person.parents", "people.person.place_of_birth"], ["people.person.children", "people.person.place_of_birth"], ["base.usnris.nris_listing.significant_person", "travel.travel_destination.tourist_attractions"], ["book.written_work.subjects", "location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was teddy roosevelt 's house [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes <SEP> location.location.containedby </PATH>", "<PATH> symbols.namesake.named_after <SEP> location.location.containedby </PATH>", "<PATH> people.person.parents <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.02365756966173649, -0.028267500922083855, -0.03772950917482376], "norm_scores": [0.33540982007980347, 0.3338671624660492, 0.33072301745414734]}}
{"id": "WebQTest-516", "question": "what is the government system of malaysia", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the government system of malaysia [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>"], "scores": [-0.01032552495598793, -0.013612259179353714, -0.1121995747089386], "norm_scores": [0.3448440134525299, 0.3437124490737915, 0.311443567276001]}}
{"id": "WebQTest-517", "question": "what do people speak in canada", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people speak in canada [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.016119424253702164, -0.017587387934327126, -0.038267187774181366], "norm_scores": [0.3359505236148834, 0.3354577422142029, 0.3285917639732361]}}
{"id": "WebQTest-518", "question": "what type of money do they use in costa rica", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["government.form_of_government.countries"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of money do they use in costa rica [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> government.form_of_government.countries </PATH>"], "scores": [-0.006067474372684956, -0.022487636655569077, -0.11820274591445923], "norm_scores": [0.3475073277950287, 0.34184780716896057, 0.3106449246406555]}}
{"id": "WebQTest-519", "question": "what timezone is nashville tn", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is nashville tn [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-8.656466889078729e-06, -0.18942493200302124, -0.1901627779006958], "norm_scores": [0.3767508864402771, 0.3117395341396332, 0.3115095794200897]}}
{"id": "WebQTest-521", "question": "who was anakin skywalker", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was anakin skywalker [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.004094772506505251, -0.03970737010240555, -0.04812466725707054], "norm_scores": [0.3422384560108185, 0.3302649259567261, 0.32749664783477783]}}
{"id": "WebQTest-522", "question": "where did adolf hitler die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["people.place_of_death.people"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did adolf hitler die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people </PATH>"], "scores": [-0.0005323864170350134, -0.08333651721477509, -0.10281848907470703], "norm_scores": [0.35419508814811707, 0.32604771852493286, 0.3197571635246277]}}
{"id": "WebQTest-523", "question": "what is the largest nation in europe", "prediction": [["location.location.containedby"], ["base.locations.continents.countries_within"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the largest nation in europe [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.01246381364762783, -0.024038271978497505, -0.04103918746113777], "norm_scores": [0.33780115842819214, 0.3339138329029083, 0.32828497886657715]}}
{"id": "WebQTest-524", "question": "where was the first gulf war fought", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["time.event.locations"], ["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the first gulf war fought [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.004495725501328707, -0.03209081292152405, -0.114790178835392], "norm_scores": [0.34863218665122986, 0.33914315700531006, 0.31222468614578247]}}
{"id": "WebQTest-525", "question": "who are the senators of virginia 2013", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.governmental_body.jurisdiction", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"]], "ground_paths": [["location.administrative_division.country", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.election.district", "government.election.winner"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the senators of virginia 2013 [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_body.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.01639176532626152, -0.02467559091746807, -0.02682606130838394], "norm_scores": [0.3354162275791168, 0.3326491415500641, 0.3319346010684967]}}
{"id": "WebQTest-526", "question": "what band was george clinton in", "prediction": [["music.artist.genre", "music.artist.genre"], ["music.artist.label", "music.artist.label"], ["music.recording.artist", "music.recording.artist"]], "ground_paths": [["music.recording.featured_artists", "music.recording.featured_artists"], ["music.artist.album", "music.artist.album"], ["music.artist.label", "music.artist.label"], ["music.record_label.artist", "music.record_label.artist"], ["music.composer.compositions", "music.artist.track"], ["music.recording.featured_artists", "music.featured_artist.recordings"], ["music.producer.tracks_produced", "music.recording.artist"], ["music.artist.genre", "music.genre.artists"], ["music.album.artist", "music.artist.album"], ["music.group_membership.member", "music.musical_group.member"], ["music.composition.composer", "music.recording.artist"], ["music.recording.artist", "music.recording.artist"], ["music.recording.featured_artists", "music.artist.album"], ["base.saturdaynightlive.snl_episode.musical_guest", "base.saturdaynightlive.snl_musical_guest.episodes_as_musical_guest"], ["music.artist.album", "music.album.artist"], ["music.recording.producer", "music.recording.artist"], ["music.producer.releases_produced", "music.album.artist"], ["music.group_member.membership", "music.musical_group.member"], ["music.featured_artist.recordings", "music.recording.featured_artists"], ["music.artist.origin", "music.artist.origin"], ["music.composer.compositions", "music.recording.artist"], ["common.topic.notable_types", "common.topic.notable_types"], ["music.album.artist", "music.album.artist"], ["broadcast.artist.content", "broadcast.artist.content"], ["music.recording.artist", "music.album.artist"], ["music.artist.genre", "music.artist.genre"], ["music.group_membership.member", "music.group_membership.group"], ["music.composition.composer", "music.artist.track"], ["music.featured_artist.recordings", "music.recording.artist"], ["music.recording.producer", "music.album.artist"], ["people.person.nationality", "music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what band was george clinton in [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> music.artist.label <SEP> music.artist.label </PATH>", "<PATH> music.recording.artist <SEP> music.recording.artist </PATH>"], "scores": [-0.014950672164559364, -0.036402035504579544, -0.05203599855303764], "norm_scores": [0.3398619592189789, 0.3326491117477417, 0.3274889290332794]}}
{"id": "WebQTest-527", "question": "what country borders slovakia", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_containedby", "location.location.partially_contains"]], "ground_paths": [["location.location.partially_contains", "geography.river.basin_countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.sport_country.multi_event_tournaments_participated_in"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["base.uncommon.topic.exceptions", "base.uncommon.topic.exceptions"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.partially_containedby", "geography.river.origin"], ["location.location.containedby", "location.location.contains"], ["location.location.partially_containedby", "location.location.partially_contains"], ["base.locations.countries.continent", "base.locations.continents.countries_within"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.partially_contains", "location.location.partially_contains"], ["organization.organization.founders", "organization.organization.founders"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.partially_contains", "location.location.partially_containedby"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["base.locations.countries.continent", "base.locations.countries.continent"], ["location.location.events", "location.location.events"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.containedby", "location.location.containedby"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["base.datedlocationtest.dated_location_break_up.new_locations", "base.datedlocationtest.dated_location_break_up.new_locations"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["military.military_combatant_group.combatants", "military.military_combatant.military_conflicts"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["finance.currency.countries_formerly_used", "location.country.currency_formerly_used"], ["location.location.partially_containedby", "location.location.containedby"], ["location.country.official_language", "language.human_language.countries_spoken_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["location.country.currency_used", "location.country.currency_used"], ["government.form_of_government.countries", "location.country.form_of_government"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country borders slovakia [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>"], "scores": [-0.0365394726395607, -0.041095830500125885, -0.04187565669417381], "norm_scores": [0.334433376789093, 0.3329130709171295, 0.33265355229377747]}}
{"id": "WebQTest-528", "question": "what was james garfield most known for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was james garfield most known for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0011556519893929362, -0.058810025453567505, -0.08500930666923523], "norm_scores": [0.34921786189079285, 0.3296533226966858, 0.32112881541252136]}}
{"id": "WebQTest-529", "question": "where did paula deen go to school", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did paula deen go to school [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.017919296398758888, -0.018461689352989197, -0.02740241028368473], "norm_scores": [0.334445983171463, 0.3342646360397339, 0.3312893509864807]}}
{"id": "WebQTest-530", "question": "where does robert irvine come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does robert irvine come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.00410914421081543, -0.037752822041511536, -0.048888545483350754], "norm_scores": [0.34209972620010376, 0.3307816684246063, 0.3271186351776123]}}
{"id": "WebQTest-531", "question": "what style of music does michael jackson perform", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"]], "ground_paths": [["music.artist.genre"], ["music.genre.artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what style of music does michael jackson perform [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.0009023013990372419, -0.059975650161504745, -0.13313473761081696], "norm_scores": [0.35476404428482056, 0.33441394567489624, 0.3108220398426056]}}
{"id": "WebQTest-532", "question": "what does david beckham play", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.position"]], "ground_paths": [["book.written_work.subjects", "book.book_subject.works"], ["soccer.football_player.position_s", "sports.sport.positions"], ["sports.pro_sports_played.athlete", "sports.pro_sports_played.sport"], ["symbols.namesake.named_after", "cvg.computer_videogame.cvg_genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does david beckham play [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.position </PATH>"], "scores": [-0.027795899659395218, -0.03027881309390068, -0.04329218715429306], "norm_scores": [0.3353292942047119, 0.33449772000312805, 0.33017298579216003]}}
{"id": "WebQTest-533", "question": "what does julian lennon do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["music.artist.genre"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does julian lennon do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> music.artist.genre </PATH>"], "scores": [-0.0010432281997054815, -0.06229322403669357, -0.1160537451505661], "norm_scores": [0.35311421751976013, 0.33213502168655396, 0.3147507309913635]}}
{"id": "WebQTest-534", "question": "what is the milwaukee brewers mascot", "prediction": [["sports.mascot.team"], ["sports.sports_team.team_mascot"], ["sports.mascot.team", "sports.mascot.team"]], "ground_paths": [["sports.sports_team.team_mascot"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the milwaukee brewers mascot [/INST]", "raw_output": {"paths": ["<PATH> sports.mascot.team </PATH>", "<PATH> sports.sports_team.team_mascot </PATH>", "<PATH> sports.mascot.team <SEP> sports.mascot.team </PATH>"], "scores": [-0.0011970337945967913, -0.04518003761768341, -0.14129120111465454], "norm_scores": [0.3538261353969574, 0.33860108256340027, 0.30757272243499756]}}
{"id": "WebQTest-535", "question": "what is the second biggest state in the united states", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_containedby"]], "ground_paths": [["location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the second biggest state in the united states [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.0016457225428894162, -0.05698885768651962, -0.07686836272478104], "norm_scores": [0.34798377752304077, 0.32924845814704895, 0.32276779413223267]}}
{"id": "WebQTest-537", "question": "what has charlie hunnam been in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["film.actor.film", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.film.starring"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["people.person.nationality", "film.film.country"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has charlie hunnam been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.022535020485520363, -0.02646590583026409, -0.03196713700890541], "norm_scores": [0.33481892943382263, 0.3335053622722626, 0.3316757082939148]}}
{"id": "WebQTest-538", "question": "what are all the movies taylor swift has been in", "prediction": [["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["music.album.genre", "media_common.netflix_title.netflix_genres"], ["film.person_or_entity_appearing_in_film.films", "film.film.personal_appearances"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["people.person.languages", "film.film.language"], ["film.performance.actor", "film.film.starring"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are all the movies taylor swift has been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.026880361139774323, -0.03786630183458328, -0.03879374638199806], "norm_scores": [0.3358825147151947, 0.33221274614334106, 0.3319047689437866]}}
{"id": "WebQTest-540", "question": "what college did brett favre", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did brett favre [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.016575949266552925, -0.019104156643152237, -0.02812103182077408], "norm_scores": [0.33489659428596497, 0.3340510129928589, 0.3310524523258209]}}
{"id": "WebQTest-542", "question": "where did alex chilton die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["location.location.people_born_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did alex chilton die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0002940802660305053, -0.09555463492870331, -0.10046947747468948], "norm_scores": [0.35538941621780396, 0.323097288608551, 0.32151323556900024]}}
{"id": "WebQTest-543", "question": "what college did martin luther king jr go to", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did martin luther king jr go to [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.01450387667864561, -0.01794631965458393, -0.027406712993979454], "norm_scores": [0.33514946699142456, 0.3339977264404297, 0.3308528661727905]}}
{"id": "WebQTest-545", "question": "what kind of cancer did farrah fawcett die of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["medicine.disease.notable_people_with_this_condition"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of cancer did farrah fawcett die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>"], "scores": [-0.00816661398857832, -0.018297182396054268, -0.04300292953848839], "norm_scores": [0.3383311331272125, 0.3349209725856781, 0.326747864484787]}}
{"id": "WebQTest-546", "question": "who is michael j fox 's wife", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.marriage.spouse", "people.marriage.spouse"]], "ground_paths": [["award.award_winner.awards_won", "award.award_honor.award_winner"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["people.person.spouse_s", "people.marriage.spouse"], ["common.image.appears_in_topic_gallery", "common.topic.image"], ["people.person.profession", "people.person.profession"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is michael j fox 's wife [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>"], "scores": [-0.021139610558748245, -0.03335091844201088, -0.04227582737803459], "norm_scores": [0.33704662322998047, 0.33295586705207825, 0.3299975097179413]}}
{"id": "WebQTest-547", "question": "what money do i use in costa rica", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["government.form_of_government.countries"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money do i use in costa rica [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> government.form_of_government.countries </PATH>"], "scores": [-0.005965555086731911, -0.024182038381695747, -0.12781742215156555], "norm_scores": [0.3487689197063446, 0.3424730896949768, 0.3087579905986786]}}
{"id": "WebQTest-548", "question": "what guitar did joe walsh play", "prediction": [["music.guitarist.guitars_played"], ["music.group_member.instruments_played"], ["music.guitar.guitarists"]], "ground_paths": [["music.guitar.guitarists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what guitar did joe walsh play [/INST]", "raw_output": {"paths": ["<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.guitar.guitarists </PATH>"], "scores": [-0.01877441816031933, -0.02058018371462822, -0.03205792233347893], "norm_scores": [0.3350083529949188, 0.33440396189689636, 0.3305876851081848]}}
{"id": "WebQTest-549", "question": "what are the official languages of the eu", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization.geographic_scope", "language.human_language.countries_spoken_in"], ["organization.organization.geographic_scope", "location.country.languages_spoken"]], "ground_paths": [["organization.organization_founder.organizations_founded", "location.country.official_language"], ["organization.organization.founders", "location.country.official_language"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_language"], ["organization.organization.founders", "location.country.languages_spoken"], ["organization.organization.geographic_scope", "language.human_language.region"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the official languages of the eu [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.geographic_scope <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> organization.organization.geographic_scope <SEP> location.country.languages_spoken </PATH>"], "scores": [-0.024180831387639046, -0.029832273721694946, -0.03760850429534912], "norm_scores": [0.33545491099357605, 0.333564430475235, 0.3309805989265442]}}
{"id": "WebQTest-550", "question": "what is ryan seacrest radio show", "prediction": [["broadcast.content.producer"], ["internet.website.owner"], ["broadcast.producer.content", "broadcast.content.producer"]], "ground_paths": [["tv.tv_program_creator.programs_created"], ["broadcast.artist.content"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is ryan seacrest radio show [/INST]", "raw_output": {"paths": ["<PATH> broadcast.content.producer </PATH>", "<PATH> internet.website.owner </PATH>", "<PATH> broadcast.producer.content <SEP> broadcast.content.producer </PATH>"], "scores": [-0.03604408726096153, -0.05435674637556076, -0.05784870684146881], "norm_scores": [0.33780527114868164, 0.3316754400730133, 0.33051928877830505]}}
{"id": "WebQTest-552", "question": "what language do most italians speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do most italians speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.016650231555104256, -0.020585568621754646, -0.02757798507809639], "norm_scores": [0.3349854648113251, 0.3336697816848755, 0.33134475350379944]}}
{"id": "WebQTest-554", "question": "what money do they use in spain", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money do they use in spain [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.004435236100107431, -0.029845932498574257, -0.13879689574241638], "norm_scores": [0.3509777784347534, 0.3421715199947357, 0.30685070157051086]}}
{"id": "WebQTest-555", "question": "what did drew barrymore go to rehab for", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.person.profession", "people.profession.specializations"], ["people.person.profession", "people.profession.specializations_of"]], "ground_paths": [["celebrities.celebrity.substance_abuse_problems", "celebrities.abused_substance.abused_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did drew barrymore go to rehab for [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.person.profession <SEP> people.profession.specializations </PATH>", "<PATH> people.person.profession <SEP> people.profession.specializations_of </PATH>"], "scores": [-0.021766407415270805, -0.025498133152723312, -0.04094650223851204], "norm_scores": [0.33587729930877686, 0.3346262276172638, 0.32949650287628174]}}
{"id": "WebQTest-556", "question": "when is saint george day celebrated", "prediction": [["time.recurring_event.current_frequency"], ["time.day_of_year.holidays"], ["time.holiday.featured_in_religions"]], "ground_paths": [["time.holiday.day_of_year"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is saint george day celebrated [/INST]", "raw_output": {"paths": ["<PATH> time.recurring_event.current_frequency </PATH>", "<PATH> time.day_of_year.holidays </PATH>", "<PATH> time.holiday.featured_in_religions </PATH>"], "scores": [-0.015490195713937283, -0.026116929948329926, -0.03400008752942085], "norm_scores": [0.33657681941986084, 0.33301904797554016, 0.3304041028022766]}}
{"id": "WebQTest-557", "question": "what government did the roman empire have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["fictional_universe.fictional_setting.languages", "location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what government did the roman empire have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> fictional_universe.fictional_setting.languages <SEP> location.country.form_of_government </PATH>"], "scores": [-0.005004467908293009, -0.02506103739142418, -0.08295610547065735], "norm_scores": [0.3442160189151764, 0.3373810052871704, 0.3184029757976532]}}
{"id": "WebQTest-559", "question": "what instrument did robin gibb play", "prediction": [["music.group_member.instruments_played"], ["music.instrument.family"], ["music.group_membership.group", "music.group_membership.role"]], "ground_paths": [["music.track_contribution.contributor", "music.track_contribution.role"], ["music.group_membership.member", "music.performance_role.regular_performances"], ["music.group_membership.member", "music.group_membership.role"], ["music.artist.track_contributions", "music.track_contribution.role"], ["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what instrument did robin gibb play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.group_membership.group <SEP> music.group_membership.role </PATH>"], "scores": [-0.0008591226069256663, -0.07503201812505722, -0.081016905605793], "norm_scores": [0.35069483518600464, 0.3256240785121918, 0.3236810564994812]}}
{"id": "WebQTest-560", "question": "who voiced darth vader", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.special_performance_type", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who voiced darth vader [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.025654872879385948, -0.028495239093899727, -0.030626079067587852], "norm_scores": [0.33420172333717346, 0.3332538306713104, 0.3325444757938385]}}
{"id": "WebQTest-561", "question": "when was abe lincoln president", "prediction": [["government.us_president.vice_president"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_holder"]], "ground_paths": [["base.inaugurations.inauguration.president"], ["base.inaugurations.inauguration_speaker.inauguration"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was abe lincoln president [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.02584437094628811, -0.027573898434638977, -0.03982703015208244], "norm_scores": [0.3350772261619568, 0.33449819684028625, 0.33042457699775696]}}
{"id": "WebQTest-562", "question": "where are the texas rangers playing", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_team_location.teams"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the texas rangers playing [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team_location.teams </PATH>"], "scores": [-0.0012109453091397882, -0.053213465958833694, -0.09068693220615387], "norm_scores": [0.34919410943984985, 0.33149924874305725, 0.3193066716194153]}}
{"id": "WebQTest-564", "question": "what is arkansas state capitol", "prediction": [["location.location.containedby"], ["base.biblioness.bibs_location.state"], ["location.us_state.capital"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is arkansas state capitol [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> location.us_state.capital </PATH>"], "scores": [-0.020318441092967987, -0.02443951554596424, -0.027858439832925797], "norm_scores": [0.33462992310523987, 0.33325374126434326, 0.3321163058280945]}}
{"id": "WebQTest-567", "question": "what is the zip code for nogales az", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for nogales az [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0003421722212806344, -0.08194489032030106, -0.11180196702480316], "norm_scores": [0.35509276390075684, 0.32726699113845825, 0.3176402151584625]}}
{"id": "WebQTest-568", "question": "when did michael vick start playing for the eagles", "prediction": [["american_football.football_player.rushing", "american_football.player_rushing_statistics.season"], ["american_football.football_player.games", "american_football.player_game_statistics.season"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.season"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did michael vick start playing for the eagles [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.rushing <SEP> american_football.player_rushing_statistics.season </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.season </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.season </PATH>"], "scores": [-0.020662056282162666, -0.022116705775260925, -0.027236465364694595], "norm_scores": [0.3342253267765045, 0.3337395191192627, 0.3320351839065552]}}
{"id": "WebQTest-570", "question": "what is william taft famous for", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is william taft famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.009796676225960255, -0.0402982197701931, -0.04061932861804962], "norm_scores": [0.3401815891265869, 0.32996219396591187, 0.3298562467098236]}}
{"id": "WebQTest-572", "question": "where did robin cook die", "prediction": [["people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["people.place_of_death.people_deceased_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did robin cook die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>"], "scores": [-0.000322487554512918, -0.11139075458049774, -0.11235035210847855], "norm_scores": [0.35856467485427856, 0.3208715319633484, 0.32056376338005066]}}
{"id": "WebQTest-573", "question": "where did melba beals live", "prediction": [["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth", "location.us_state.capital"], ["people.person.nationality", "location.country.first_level_divisions"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did melba beals live [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.01716918684542179, -0.027213841676712036, -0.03571704402565956], "norm_scores": [0.3365158140659332, 0.33315253257751465, 0.3303316831588745]}}
{"id": "WebQTest-575", "question": "who is louisiana state senator", "prediction": [["location.administrative_division.country", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"]], "ground_paths": [["location.location.containedby", "people.deceased_person.place_of_death"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.election.district", "government.election.winner"], ["location.location.containedby", "people.person.place_of_birth"], ["location.location.containedby", "symbols.name_source.namesakes"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["location.location.partially_containedby", "people.deceased_person.place_of_death"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["base.locations.countries.states_provinces_within", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is louisiana state senator [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.country <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.022577593103051186, -0.02581598423421383, -0.029670054093003273], "norm_scores": [0.334481805562973, 0.3334003686904907, 0.33211788535118103]}}
{"id": "WebQTest-576", "question": "what are the 7 countries that are part of central america", "prediction": [["location.location.contains"], ["location.location.containedby"], ["common.topic.notable_types"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the 7 countries that are part of central america [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.007606408093124628, -0.02817348577082157, -0.03492886573076248], "norm_scores": [0.3386741578578949, 0.33177974820137024, 0.3295460343360901]}}
{"id": "WebQTest-577", "question": "where did derek fisher go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.education.institution"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did derek fisher go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.017966095358133316, -0.023602992296218872, -0.024358225986361504], "norm_scores": [0.3346711993217468, 0.3327900171279907, 0.33253875374794006]}}
{"id": "WebQTest-578", "question": "who was kobe bryant drafted by", "prediction": [["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was kobe bryant drafted by [/INST]", "raw_output": {"paths": ["<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.team </PATH>"], "scores": [-0.025066232308745384, -0.027260025963187218, -0.030585991218686104], "norm_scores": [0.33419063687324524, 0.3334583044052124, 0.33235105872154236]}}
{"id": "WebQTest-579", "question": "where are the new orleans hornets moving to", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.location"], ["sports.sports_team.previously_known_as"]], "ground_paths": [["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the new orleans hornets moving to [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team.previously_known_as </PATH>"], "scores": [-0.01110200397670269, -0.029874809086322784, -0.04432637244462967], "norm_scores": [0.339129775762558, 0.33282271027565, 0.328047513961792]}}
{"id": "WebQTest-580", "question": "what language does algerian people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does algerian people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.011965290643274784, -0.017699427902698517, -0.051990896463394165], "norm_scores": [0.3384040892124176, 0.3364691734313965, 0.3251267671585083]}}
{"id": "WebQTest-581", "question": "where was anne boleyn executed", "prediction": [["people.deceased_person.place_of_death"], ["location.location.people_executed"], ["location.location.events"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was anne boleyn executed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.people_executed </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.001210313057526946, -0.0827237218618393, -0.08391652256250381], "norm_scores": [0.35182255506515503, 0.3242819905281067, 0.3238954246044159]}}
{"id": "WebQTest-582", "question": "what did galileo do to become famous", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did galileo do to become famous [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0017521665431559086, -0.05866624787449837, -0.07363779842853546], "norm_scores": [0.3477882742881775, 0.3285469710826874, 0.32366475462913513]}}
{"id": "WebQTest-583", "question": "what school did martin luther king jr received his doctorate degree from", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did martin luther king jr received his doctorate degree from [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.006771208718419075, -0.023561500012874603, -0.029832882806658745], "norm_scores": [0.3377748429775238, 0.3321508467197418, 0.33007434010505676]}}
{"id": "WebQTest-584", "question": "where buddha come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where buddha come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.0045995451509952545, -0.04415743052959442, -0.04643450677394867], "norm_scores": [0.3424373269081116, 0.32915565371513367, 0.32840701937675476]}}
{"id": "WebQTest-585", "question": "what did king henry viii rule", "prediction": [["royalty.kingdom.rulers"], ["royalty.monarch.kingdom"], ["people.person.nationality"]], "ground_paths": [["royalty.monarch.kingdom"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did king henry viii rule [/INST]", "raw_output": {"paths": ["<PATH> royalty.kingdom.rulers </PATH>", "<PATH> royalty.monarch.kingdom </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.015329336747527122, -0.029669949784874916, -0.030442148447036743], "norm_scores": [0.33661389350891113, 0.33182111382484436, 0.33156493306159973]}}
{"id": "WebQTest-586", "question": "what type of cancer did larry hagman have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of cancer did larry hagman have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.014796207658946514, -0.015162418596446514, -0.032207515090703964], "norm_scores": [0.3353034257888794, 0.33518069982528687, 0.32951590418815613]}}
{"id": "WebQTest-588", "question": "what wars did robert e lee served in", "prediction": [["base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"], ["military.military_person.participated_in_conflicts"]], "ground_paths": [["base.culturalevent.event.entity_involved"], ["military.military_person.participated_in_conflicts"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"], ["military.military_conflict.military_personnel_involved"], ["military.military_command.military_commander", "military.military_conflict.commanders"], ["military.military_commander.military_commands", "military.military_conflict.commanders"], ["people.person.nationality", "base.culturalevent.event.entity_involved"], ["military.military_commander.military_commands", "military.military_command.military_conflict"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what wars did robert e lee served in [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.included_in_event </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>"], "scores": [-0.00637128995731473, -0.025786563754081726, -0.04791247844696045], "norm_scores": [0.34012657403945923, 0.3335866332054138, 0.32628676295280457]}}
{"id": "WebQTest-589", "question": "what russian language called", "prediction": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what russian language called [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.014625939540565014, -0.025494838133454323, -0.02678540162742138], "norm_scores": [0.33589690923690796, 0.33226582407951355, 0.3318372964859009]}}
{"id": "WebQTest-590", "question": "who is in charge of libya now", "prediction": [["people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is in charge of libya now [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.010177681222558022, -0.024116098880767822, -0.03649352863430977], "norm_scores": [0.33781665563583374, 0.33314070105552673, 0.3290426731109619]}}
{"id": "WebQTest-592", "question": "when did shays rebellion start", "prediction": [["time.event.included_in_event"], ["time.event.includes_event"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did shays rebellion start [/INST]", "raw_output": {"paths": ["<PATH> time.event.included_in_event </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.014285682700574398, -0.020820291712880135, -0.0337858609855175], "norm_scores": [0.3362276554107666, 0.3340376913547516, 0.3297346532344818]}}
{"id": "WebQTest-593", "question": "what kind of disease does montel williams have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["people.person.medical_condition"]], "ground_paths": [["medicine.notable_person_with_medical_condition.condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of disease does montel williams have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.person.medical_condition </PATH>"], "scores": [-0.009553301148116589, -0.012873661704361439, -0.08285141736268997], "norm_scores": [0.3417624831199646, 0.34062957763671875, 0.3176079988479614]}}
{"id": "WebQTest-595", "question": "when is the last time the ravens won a superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is the last time the ravens won a superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.010314413346350193, -0.020991722121834755, -0.0332171805202961], "norm_scores": [0.33707064390182495, 0.3334908187389374, 0.32943853735923767]}}
{"id": "WebQTest-596", "question": "where did kansas city royals come from", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.location"], ["sports.sports_team.previously_known_as"]], "ground_paths": [["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kansas city royals come from [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team.previously_known_as </PATH>"], "scores": [-0.014977348037064075, -0.02198927104473114, -0.036493055522441864], "norm_scores": [0.3365046977996826, 0.334153413772583, 0.3293418884277344]}}
{"id": "WebQTest-597", "question": "what style of music does john mayer play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["music.album.genre"]], "ground_paths": [["music.artist.genre"], ["music.genre.artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what style of music does john mayer play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> music.album.genre </PATH>"], "scores": [-0.0006485030171461403, -0.06409524381160736, -0.1495654433965683], "norm_scores": [0.35712185502052307, 0.33516743779182434, 0.3077107071876526]}}
{"id": "WebQTest-599", "question": "what inventions did thomas edison invent list", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["law.inventor.us_patents"]], "ground_paths": [["law.invention.inventor"], ["law.inventor.us_patents"], ["law.inventor.inventions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inventions did thomas edison invent list [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.us_patents </PATH>"], "scores": [-0.00901404581964016, -0.02257557213306427, -0.05353885516524315], "norm_scores": [0.3397914171218872, 0.33521443605422974, 0.32499411702156067]}}
{"id": "WebQTest-600", "question": "where did djokovic come from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did djokovic come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.008772270753979683, -0.021360233426094055, -0.055506832897663116], "norm_scores": [0.33992427587509155, 0.33567216992378235, 0.3244035840034485]}}
{"id": "WebQTest-602", "question": "what part of the country is new england", "prediction": [["location.location.contains"], ["location.location.containedby"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part of the country is new england [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.0068350182846188545, -0.023381104692816734, -0.13816292583942413], "norm_scores": [0.34958675503730774, 0.3438500463962555, 0.3065631687641144]}}
{"id": "WebQTest-603", "question": "what money to take to turkey", "prediction": [["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"], ["location.country.currency_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money to take to turkey [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.013776089064776897, -0.023364556953310966, -0.035842448472976685], "norm_scores": [0.336855411529541, 0.3336409330368042, 0.3295036554336548]}}
{"id": "WebQTest-604", "question": "what kind of music did jimi hendrix play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of music did jimi hendrix play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.0008984430460259318, -0.05603495240211487, -0.1422618329524994], "norm_scores": [0.35529911518096924, 0.33623945713043213, 0.30846136808395386]}}
{"id": "WebQTest-605", "question": "what degrees did obama get in college", "prediction": [["education.education.student", "education.education.degree"], ["people.person.education", "education.education.degree"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.degree"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what degrees did obama get in college [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.011276802979409695, -0.01962844654917717, -0.026914406567811966], "norm_scores": [0.3360026478767395, 0.3332081437110901, 0.3307892382144928]}}
{"id": "WebQTest-606", "question": "who were alexander graham bell 's siblings", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.parents", "people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.parents"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["people.person.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were alexander graham bell 's siblings [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.02080276608467102, -0.022690851241350174, -0.04168444871902466], "norm_scores": [0.3358580470085144, 0.33522453904151917, 0.3289174437522888]}}
{"id": "WebQTest-608", "question": "when did liverpool fc last win the champions league", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["time.event.instance_of_recurring_event"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did liverpool fc last win the champions league [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>"], "scores": [-0.004604071844369173, -0.029191115871071815, -0.057157937437295914], "norm_scores": [0.3419369161128998, 0.3336322009563446, 0.3244308531284332]}}
{"id": "WebQTest-609", "question": "who played jeannie", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "ground_paths": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["award.award_winning_work.awards_won", "award.award_honor.award_winner"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played jeannie [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.015833780169487, -0.031092587858438492, -0.03267698734998703], "norm_scores": [0.3369096517562866, 0.33180782198905945, 0.33128252625465393]}}
{"id": "WebQTest-611", "question": "where was st peter killed", "prediction": [["people.deceased_person.place_of_death"], ["location.location.events"], ["location.location.people_born_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was st peter killed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.events </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0009060965385288, -0.08072558790445328, -0.09236259013414383], "norm_scores": [0.35262373089790344, 0.3255714774131775, 0.32180479168891907]}}
{"id": "WebQTest-612", "question": "who does jeremy shockey play for in 2012", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does jeremy shockey play for in 2012 [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.013644042424857616, -0.017428267747163773, -0.02696705050766468], "norm_scores": [0.33523428440093994, 0.33396807312965393, 0.33079758286476135]}}
{"id": "WebQTest-613", "question": "what college did deion sanders jr go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did deion sanders jr go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.01966901868581772, -0.023785635828971863, -0.023950884118676186], "norm_scores": [0.33426713943481445, 0.3328939378261566, 0.33283892273902893]}}
{"id": "WebQTest-614", "question": "when did the battle of alamo start", "prediction": [["time.event.locations"], ["time.event.included_in_event"], ["time.event.includes_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the battle of alamo start [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> time.event.included_in_event </PATH>", "<PATH> time.event.includes_event </PATH>"], "scores": [-0.011237319558858871, -0.02824152074754238, -0.04429323226213455], "norm_scores": [0.33891135454177856, 0.33319714665412903, 0.32789146900177]}}
{"id": "WebQTest-615", "question": "what were the names of henry viii 's three children", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.parent"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what were the names of henry viii 's three children [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.012268188409507275, -0.012885384261608124, -0.1304885596036911], "norm_scores": [0.34627440571784973, 0.34606072306632996, 0.3076649010181427]}}
{"id": "WebQTest-616", "question": "what was queen elizabeth ii childhood nickname", "prediction": [["people.person.children"], ["people.person.parents"], ["royalty.monarch.kingdom"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was queen elizabeth ii childhood nickname [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> royalty.monarch.kingdom </PATH>"], "scores": [-0.023858508095145226, -0.029525240883231163, -0.034129489213228226], "norm_scores": [0.3351059556007385, 0.33321237564086914, 0.3316816985607147]}}
{"id": "WebQTest-617", "question": "where did tim tebow grow up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did tim tebow grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.010077930055558681, -0.01937348209321499, -0.061092689633369446], "norm_scores": [0.34001898765563965, 0.33687296509742737, 0.3231080174446106]}}
{"id": "WebQTest-618", "question": "where is chesapeake bay bridge", "prediction": [["transportation.bridge.locale"], ["transportation.bridge.body_of_water_spanned"], ["location.location.containedby"]], "ground_paths": [["transportation.bridge.locale"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is chesapeake bay bridge [/INST]", "raw_output": {"paths": ["<PATH> transportation.bridge.locale </PATH>", "<PATH> transportation.bridge.body_of_water_spanned </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0009907109197229147, -0.0649930089712143, -0.08464605361223221], "norm_scores": [0.3499254882335663, 0.3282311260700226, 0.32184335589408875]}}
{"id": "WebQTest-619", "question": "who played victoria barkley", "prediction": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played victoria barkley [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.020122326910495758, -0.02100551128387451, -0.023858651518821716], "norm_scores": [0.3338465988636017, 0.3335518538951874, 0.33260154724121094]}}
{"id": "WebQTest-620", "question": "where did kennedy 's inaugural address take place", "prediction": [["time.event.locations"], ["location.location.events"], ["architecture.architect.structures_designed"]], "ground_paths": [["symbols.name_source.namesakes", "location.location.containedby"], ["base.inaugurations.inauguration.president", "time.event.locations"], ["symbols.name_source.namesakes", "people.person.place_of_birth"], ["base.inaugurations.inauguration.president", "location.location.events"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.administrative_division.country"], ["common.image.appears_in_topic_gallery", "common.image.image_location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kennedy 's inaugural address take place [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> architecture.architect.structures_designed </PATH>"], "scores": [-0.0007088493439368904, -0.06875277310609818, -0.09883536398410797], "norm_scores": [0.3520192801952362, 0.3288632333278656, 0.3191175162792206]}}
{"id": "WebQTest-621", "question": "what countries have german as the official language", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries have german as the official language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.015387557446956635, -0.02068805694580078, -0.024197865277528763], "norm_scores": [0.33490270376205444, 0.33313223719596863, 0.33196505904197693]}}
{"id": "WebQTest-622", "question": "where did the taliban began", "prediction": [["organization.organization.place_founded"], ["organization.organization.founders"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [["organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the taliban began [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.place_founded </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.019547535106539726, -0.025348421186208725, -0.03011944331228733], "norm_scores": [0.33515435457229614, 0.3332158029079437, 0.33162981271743774]}}
{"id": "WebQTest-624", "question": "who plays elena gilbert on vampire diaries", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"]], "ground_paths": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["common.image.appears_in_topic_gallery", "common.image.appears_in_topic_gallery"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays elena gilbert on vampire diaries [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.02565513737499714, -0.03101031854748726, -0.03211623430252075], "norm_scores": [0.3346475064754486, 0.33286020159721375, 0.33249229192733765]}}
{"id": "WebQTest-626", "question": "what does the missouri river bisect", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.containedby"]], "ground_paths": [["location.location.partially_contains"], ["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does the missouri river bisect [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.011170751415193081, -0.01959383860230446, -0.057579539716243744], "norm_scores": [0.3394129276275635, 0.3365660309791565, 0.3240211009979248]}}
{"id": "WebQTest-627", "question": "what nationality is taylor lautner jacob black", "prediction": [["people.person.nationality"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what nationality is taylor lautner jacob black [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.nationality </PATH>"], "scores": [-0.02117200195789337, -0.022466374561190605, -0.04926717281341553], "norm_scores": [0.33658677339553833, 0.3361513912677765, 0.32726189494132996]}}
{"id": "WebQTest-628", "question": "what animal represents california", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"]], "ground_paths": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what animal represents california [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.01897604763507843, -0.027162160724401474, -0.03815168887376785], "norm_scores": [0.3363770544528961, 0.3336346745491028, 0.3299882411956787]}}
{"id": "WebQTest-629", "question": "what company does nike own", "prediction": [["business.business_operation.industry"], ["business.business_operation.industry", "business.business_operation.industry"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"]], "ground_paths": [["business.acquisition.acquiring_company", "business.acquisition.company_acquired"], ["business.business_operation.industry", "business.business_operation.industry"], ["business.industry.companies", "business.industry.companies"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what company does nike own [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry </PATH>", "<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> base.schemastaging.organization_extra.phone_number <SEP> base.schemastaging.phone_sandbox.service_location </PATH>"], "scores": [-0.029264789074659348, -0.02950058877468109, -0.03174390271306038], "norm_scores": [0.3336349129676819, 0.33355626463890076, 0.33280882239341736]}}
{"id": "WebQTest-630", "question": "what language do colombian people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do colombian people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.0087277265265584, -0.01833619736135006, -0.07148829847574234], "norm_scores": [0.34134283661842346, 0.33807873725891113, 0.32057836651802063]}}
{"id": "WebQTest-631", "question": "what year did ray allen join the nba", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did ray allen join the nba [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>"], "scores": [-0.01812800019979477, -0.022553935647010803, -0.02377529814839363], "norm_scores": [0.3344534933567047, 0.33297649025917053, 0.33257004618644714]}}
{"id": "WebQTest-633", "question": "what is claire danes famous for", "prediction": [["people.person.profession"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.film_character.portrayed_in_films"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is claire danes famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.0009784498251974583, -0.07513954490423203, -0.07552946358919144], "norm_scores": [0.3500564396381378, 0.32503512501716614, 0.32490840554237366]}}
{"id": "WebQTest-634", "question": "what county is san antonio in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"]], "ground_paths": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is san antonio in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.008434277959167957, -0.026745565235614777, -0.03924962505698204], "norm_scores": [0.33880963921546936, 0.33266204595565796, 0.3285283148288727]}}
{"id": "WebQTest-635", "question": "what characters does matt stone do", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["film.actor.film", "film.film_character.portrayed_in_films"]], "ground_paths": [["fictional_universe.fictional_character_creator.fictional_characters_created"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["base.popstra.friendship.participant", "base.popstra.friendship.participant"], ["people.person.gender", "people.person.gender"], ["tv.tv_actor.guest_roles", "tv.tv_character.appeared_in_tv_episodes"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.has_value", "freebase.valuenotation.has_no_value"], ["film.performance.actor", "film.performance.character"], ["people.person.nationality", "people.person.nationality"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["tv.tv_guest_role.actor", "tv.tv_character.appeared_in_tv_episodes"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["people.person.profession", "people.person.profession"], ["people.person.ethnicity", "people.person.ethnicity"], ["film.actor.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what characters does matt stone do [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.016965188086032867, -0.0363236665725708, -0.04023626446723938], "norm_scores": [0.33808621764183044, 0.33160433173179626, 0.3303093910217285]}}
{"id": "WebQTest-636", "question": "what timezone is the state of michigan and", "prediction": [["location.location.time_zones"], ["base.locations.states_and_provences.country", "location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is the state of michigan and [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> base.locations.states_and_provences.country <SEP> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>"], "scores": [-1.1891102076333482e-05, -0.14552687108516693, -0.15417230129241943], "norm_scores": [0.3674158453941345, 0.31765928864479065, 0.31492480635643005]}}
{"id": "WebQTest-637", "question": "what type of government did soviet union have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government did soviet union have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.007543246727436781, -0.017810167744755745, -0.14534109830856323], "norm_scores": [0.3495207726955414, 0.3459506034851074, 0.3045286536216736]}}
{"id": "WebQTest-638", "question": "what languages are there in japan", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what languages are there in japan [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.014465482905507088, -0.02195703238248825, -0.03206653520464897], "norm_scores": [0.33612436056137085, 0.33361566066741943, 0.33025994896888733]}}
{"id": "WebQTest-639", "question": "what year did the bulls get rodman", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["sports.sports_team.championships"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the bulls get rodman [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_team.championships </PATH>"], "scores": [-0.01736699789762497, -0.033854544162750244, -0.04862475395202637], "norm_scores": [0.33865320682525635, 0.3331153988838196, 0.3282313942909241]}}
{"id": "WebQTest-640", "question": "who was saint paul the apostle", "prediction": [["people.person.profession"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was saint paul the apostle [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.0031001283787190914, -0.050818610936403275, -0.06936228275299072], "norm_scores": [0.34610608220100403, 0.3299782872200012, 0.32391566038131714]}}
{"id": "WebQTest-642", "question": "where did vince scully go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did vince scully go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.014705643989145756, -0.020569704473018646, -0.023650357499718666], "norm_scores": [0.3349805176258087, 0.33302193880081177, 0.3319975733757019]}}
{"id": "WebQTest-643", "question": "where does princess leia live", "prediction": [["fictional_universe.fictional_character.places_lived"], ["fictional_universe.fictional_character.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["fictional_universe.fictional_character.places_lived"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does princess leia live [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.places_lived </PATH>", "<PATH> fictional_universe.fictional_character.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.003922908566892147, -0.03686829283833504, -0.06759171187877655], "norm_scores": [0.3441266119480133, 0.3329739570617676, 0.3228994309902191]}}
{"id": "WebQTest-644", "question": "what famous people are from el salvador", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what famous people are from el salvador [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.006420486140996218, -0.03561738133430481, -0.04844178631901741], "norm_scores": [0.3412882685661316, 0.33146774768829346, 0.32724401354789734]}}
{"id": "WebQTest-645", "question": "what type of art did leonardo da vinci make", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_art_form.artists"]], "ground_paths": [["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art did leonardo da vinci make [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>"], "scores": [-0.00020056807261426002, -0.09801841527223587, -0.10311289876699448], "norm_scores": [0.3559960424900055, 0.3228222131729126, 0.3211817741394043]}}
{"id": "WebQTest-646", "question": "where did mary bell live", "prediction": [["people.person.place_of_birth"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did mary bell live [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.016013626009225845, -0.03339378163218498, -0.03501984477043152], "norm_scores": [0.33738836646080017, 0.3315751850605011, 0.33103644847869873]}}
{"id": "WebQTest-648", "question": "who did kimberly williams marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.marriage.spouse", "people.marriage.spouse"]], "ground_paths": [["people.person.parents", "people.person.parents"], ["film.film.produced_by", "film.music_contributor.film"], ["people.person.nationality", "people.person.nationality"], ["people.person.children", "people.person.children"], ["people.person.profession", "people.person.profession"], ["people.person.spouse_s", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did kimberly williams marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>"], "scores": [-0.019981015473604202, -0.021730763837695122, -0.046135786920785904], "norm_scores": [0.33642441034317017, 0.335836261510849, 0.3277393579483032]}}
{"id": "WebQTest-649", "question": "what other movies has ryan gosling been in", "prediction": [["people.person.languages", "film.film.language"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["film.film.executive_produced_by"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["people.person.languages", "film.film.language"], ["film.performance.actor", "film.film.starring"], ["people.person.languages", "tv.tv_program.languages"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.featured_film_locations"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other movies has ryan gosling been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.025493280962109566, -0.028147660195827484, -0.03333044797182083], "norm_scores": [0.334499329328537, 0.3336126208305359, 0.3318880498409271]}}
{"id": "WebQTest-650", "question": "what political party does julia gillard belong to", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"]], "ground_paths": [["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party does julia gillard belong to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>"], "scores": [-0.008216336369514465, -0.022001812234520912, -0.02250988967716694], "norm_scores": [0.33646050095558167, 0.33185404539108276, 0.33168545365333557]}}
{"id": "WebQTest-651", "question": "what is the currency of germany now", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency of germany now [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.009070811793208122, -0.02076783962547779, -0.059692010283470154], "norm_scores": [0.3402506411075592, 0.33629390597343445, 0.32345542311668396]}}
{"id": "WebQTest-652", "question": "what kind of hispanic is selena gomez", "prediction": [["people.ethnicity.included_in_group"], ["people.person.ethnicity"], ["people.ethnicity.people"]], "ground_paths": [["people.person.ethnicity"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of hispanic is selena gomez [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.included_in_group </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>"], "scores": [-0.01173541508615017, -0.02707449533045292, -0.03295113146305084], "norm_scores": [0.33740630745887756, 0.3322702944278717, 0.33032339811325073]}}
{"id": "WebQTest-653", "question": "who did denise van outen married", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["people.person.profession", "people.person.profession"], ["people.person.children", "people.person.parents"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["people.person.spouse_s", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did denise van outen married [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.01667213626205921, -0.0342736653983593, -0.038332607597112656], "norm_scores": [0.3377094864845276, 0.33181726932525635, 0.3304732143878937]}}
{"id": "WebQTest-654", "question": "what are egyptian money called", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are egyptian money called [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.008534948341548443, -0.028449848294258118, -0.05766592547297478], "norm_scores": [0.3410247564315796, 0.3343004584312439, 0.3246748149394989]}}
{"id": "WebQTest-655", "question": "what is the zip code for wyoming michigan", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for wyoming michigan [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0014561457792297006, -0.05209410563111305, -0.08343257009983063], "norm_scores": [0.3481995761394501, 0.3310064375400543, 0.3207940459251404]}}
{"id": "WebQTest-656", "question": "who is the president of costa rica in 2012", "prediction": [["people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the president of costa rica in 2012 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.0034181629307568073, -0.03252687677741051, -0.04190616309642792], "norm_scores": [0.34088343381881714, 0.33110377192497253, 0.3280127942562103]}}
{"id": "WebQTest-657", "question": "where did martin luther king got shot at", "prediction": [["people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["symbols.name_source.namesakes"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did martin luther king got shot at [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.00021780595125164837, -0.112953320145607, -0.11972168833017349], "norm_scores": [0.3596155345439911, 0.32127583026885986, 0.31910863518714905]}}
{"id": "WebQTest-658", "question": "who wrote the gospel according to john", "prediction": [["book.book_subject.works"], ["book.author.works_written"], ["book.written_work.author"]], "ground_paths": [["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who wrote the gospel according to john [/INST]", "raw_output": {"paths": ["<PATH> book.book_subject.works </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.013504564762115479, -0.02377907745540142, -0.033928874880075455], "norm_scores": [0.3367500901222229, 0.3333078920841217, 0.329941987991333]}}
{"id": "WebQTest-659", "question": "what form of government is the uk", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["form.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government is the uk [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> form.form_of_government.countries </PATH>"], "scores": [-0.007536822929978371, -0.018730180338025093, -0.15146516263484955], "norm_scores": [0.3502849340438843, 0.3463858962059021, 0.303329199552536]}}
{"id": "WebQTest-661", "question": "who did nasri play for before arsenal", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did nasri play for before arsenal [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_player_stats.team </PATH>"], "scores": [-0.0260701272636652, -0.03552915155887604, -0.038346726447343826], "norm_scores": [0.33575254678726196, 0.33259162306785583, 0.3316558301448822]}}
{"id": "WebQTest-662", "question": "what currency does the czech republic use now", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does the czech republic use now [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.013452542014420033, -0.022131161764264107, -0.027796009555459023], "norm_scores": [0.3358953297138214, 0.332992821931839, 0.3311118185520172]}}
{"id": "WebQTest-663", "question": "what age did william penn get married", "prediction": [["people.person.spouse_s", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what age did william penn get married [/INST]", "raw_output": {"paths": ["<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.013981842435896397, -0.03508142754435539, -0.03696444258093834], "norm_scores": [0.3382491171360016, 0.331186980009079, 0.3305639624595642]}}
{"id": "WebQTest-664", "question": "what was firefox programmed in", "prediction": [["computer.software_developer.software"], ["film.film.story_by"], ["computer.software.language"]], "ground_paths": [["computer.software.languages_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was firefox programmed in [/INST]", "raw_output": {"paths": ["<PATH> computer.software_developer.software </PATH>", "<PATH> film.film.story_by </PATH>", "<PATH> computer.software.language </PATH>"], "scores": [-0.05291840061545372, -0.05332730710506439, -0.0578133724629879], "norm_scores": [0.3339223563671112, 0.33378586173057556, 0.3322918117046356]}}
{"id": "WebQTest-665", "question": "what government does new zealand have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.main_country", "location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what government does new zealand have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>"], "scores": [-0.005962053779512644, -0.02262616530060768, -0.07787729799747467], "norm_scores": [0.34316104650497437, 0.33748993277549744, 0.3193490207195282]}}
{"id": "WebQTest-666", "question": "where is isthmus of panama located on the map", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.countries.continent"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is isthmus of panama located on the map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.0011289361864328384, -0.05935879424214363, -0.09128383547067642], "norm_scores": [0.34999024868011475, 0.33019235730171204, 0.319817453622818]}}
{"id": "WebQTest-667", "question": "what is the capital of republic of ireland", "prediction": [["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["location.location.contains"]], "ground_paths": [["location.country.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of republic of ireland [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00733603211119771, -0.0283702090382576, -0.056033410131931305], "norm_scores": [0.34110426902770996, 0.33400437235832214, 0.3248913288116455]}}
{"id": "WebQTest-668", "question": "what has kristen stewart acted in", "prediction": [["people.person.languages", "film.film.language"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"]], "ground_paths": [["film.actor.film", "film.film.starring"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["people.person.place_of_birth", "film.film.subjects"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.actor.film", "film.performance.film"], ["people.person.languages", "film.film.language"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has kristen stewart acted in [/INST]", "raw_output": {"paths": ["<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>"], "scores": [-0.0269625186920166, -0.02706925757229328, -0.03469926863908768], "norm_scores": [0.3342037498950958, 0.33416810631752014, 0.33162808418273926]}}
{"id": "WebQTest-669", "question": "what did dr josef mengele do", "prediction": [["people.person.profession"], ["people.profession.people_with_this_profession"], ["base.crime.crime.victim_s"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did dr josef mengele do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> base.crime.crime.victim_s </PATH>"], "scores": [-0.002241231733933091, -0.05709962546825409, -0.1076355129480362], "norm_scores": [0.35129767656326294, 0.33254513144493103, 0.3161572515964508]}}
{"id": "WebQTest-670", "question": "what kind of language does egypt speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language does egypt speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.018841197714209557, -0.02446257509291172, -0.026259703561663628], "norm_scores": [0.3347837030887604, 0.3329070210456848, 0.3323092758655548]}}
{"id": "WebQTest-671", "question": "who are the senators of hawaii 2012", "prediction": [["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the senators of hawaii 2012 [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.023340629413723946, -0.02547212317585945, -0.031614940613508224], "norm_scores": [0.33448946475982666, 0.3337772786617279, 0.33173319697380066]}}
{"id": "WebQTest-672", "question": "what form of government does north korea have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does north korea have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.010173652321100235, -0.013394811190664768, -0.1315567046403885], "norm_scores": [0.3469236493110657, 0.3458079397678375, 0.3072684109210968]}}
{"id": "WebQTest-673", "question": "where did robert kardashian go to college", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did robert kardashian go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>"], "scores": [-0.012927503325045109, -0.01826195977628231, -0.029321303591132164], "norm_scores": [0.3357485234737396, 0.33396226167678833, 0.33028921484947205]}}
{"id": "WebQTest-675", "question": "what county is st james city fl in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st james city fl in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.02020690031349659, -0.023410985246300697, -0.029221344739198685], "norm_scores": [0.3346914052963257, 0.33362072706222534, 0.33168789744377136]}}
{"id": "WebQTest-676", "question": "what did the akkadian empire include", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.country.languages_spoken"]], "ground_paths": [["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did the akkadian empire include [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.01199058536440134, -0.029768921434879303, -0.034743960946798325], "norm_scores": [0.33785128593444824, 0.33189791440963745, 0.3302508294582367]}}
{"id": "WebQTest-677", "question": "after how many years are the olympic games held", "prediction": [["time.recurring_event.instances"], ["time.event.instance_of_recurring_event"], ["time.recurring_event.current_frequency"]], "ground_paths": [["time.recurring_event.current_frequency"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: after how many years are the olympic games held [/INST]", "raw_output": {"paths": ["<PATH> time.recurring_event.instances </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>"], "scores": [-0.01064289640635252, -0.016728129237890244, -0.04739522933959961], "norm_scores": [0.3380836248397827, 0.33603256940841675, 0.3258838355541229]}}
{"id": "WebQTest-678", "question": "what system of government does guyana have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what system of government does guyana have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.007546996232122183, -0.01781396009027958, -0.1430477350950241], "norm_scores": [0.3492761552333832, 0.3457084894180298, 0.30501535534858704]}}
{"id": "WebQTest-680", "question": "what team did aguero play for", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["people.person.nationality", "sports.sports_team_location.teams"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did aguero play for [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.matches_played <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.024128427729010582, -0.02585649862885475, -0.030332371592521667], "norm_scores": [0.3342147171497345, 0.33363765478134155, 0.33214765787124634]}}
{"id": "WebQTest-681", "question": "who is the president of the european union 2012", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["organization.organization_membership.organization", "organization.organization_membership.member"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the president of the european union 2012 [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> organization.organization_membership.organization <SEP> organization.organization_membership.member </PATH>"], "scores": [-0.010892204940319061, -0.017702044919133186, -0.029286371544003487], "norm_scores": [0.3361358940601349, 0.33385464549064636, 0.33000946044921875]}}
{"id": "WebQTest-683", "question": "what year was kenya moore crowned miss usa", "prediction": [["people.person.nationality", "location.location.events"], ["people.person.nationality", "time.event.locations"], ["award.competitor.competitions", "award.competitor.competitions"]], "ground_paths": [["time.recurring_event.instances"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was kenya moore crowned miss usa [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.events </PATH>", "<PATH> people.person.nationality <SEP> time.event.locations </PATH>", "<PATH> award.competitor.competitions <SEP> award.competitor.competitions </PATH>"], "scores": [-0.016358382999897003, -0.03161145746707916, -0.04677891358733177], "norm_scores": [0.33842089772224426, 0.33329811692237854, 0.3282809555530548]}}
{"id": "WebQTest-684", "question": "what sort of government does nigeria have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.continent"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sort of government does nigeria have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.00599465798586607, -0.021114468574523926, -0.1452856957912445], "norm_scores": [0.3502665162086487, 0.3450103998184204, 0.3047230541706085]}}
{"id": "WebQTest-685", "question": "where does chef ina garten live", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does chef ina garten live [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.01964620128273964, -0.022303910925984383, -0.02947372756898403], "norm_scores": [0.33472058176994324, 0.3338322043418884, 0.33144721388816833]}}
{"id": "WebQTest-686", "question": "when the atlanta braves won the world series", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.mascot.team"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when the atlanta braves won the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.mascot.team </PATH>"], "scores": [-0.010104747489094734, -0.015460094437003136, -0.05177612975239754], "norm_scores": [0.3385419249534607, 0.33673375844955444, 0.32472431659698486]}}
{"id": "WebQTest-688", "question": "what kind of language do they speak in iran", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language do they speak in iran [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01352161169052124, -0.019287869334220886, -0.03469609469175339], "norm_scores": [0.33632680773735046, 0.3343930244445801, 0.32928013801574707]}}
{"id": "WebQTest-689", "question": "what was the first language spoken in spain", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the first language spoken in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.018104005604982376, -0.021135205402970314, -0.021183842793107033], "norm_scores": [0.3340126574039459, 0.3330017626285553, 0.3329855501651764]}}
{"id": "WebQTest-690", "question": "what did scott joplin play", "prediction": [["music.group_member.instruments_played"], ["people.person.profession"], ["music.instrument.family"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did scott joplin play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> music.instrument.family </PATH>"], "scores": [-0.00200501992367208, -0.07554985582828522, -0.08499529212713242], "norm_scores": [0.3509443402290344, 0.3260604739189148, 0.3229951858520508]}}
{"id": "WebQTest-691", "question": "what school did miley cyrus", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did miley cyrus [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.019848866388201714, -0.0246206596493721, -0.02599337138235569], "norm_scores": [0.3345473110675812, 0.3329547345638275, 0.3324980139732361]}}
{"id": "WebQTest-693", "question": "what language do most people speak in afghanistan", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do most people speak in afghanistan [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.015697376802563667, -0.018511110916733742, -0.03248434141278267], "norm_scores": [0.3355093002319336, 0.33456656336784363, 0.3299241065979004]}}
{"id": "WebQTest-694", "question": "who is the prime minister of jamaica west indies", "prediction": [["people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the prime minister of jamaica west indies [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.010957852005958557, -0.027236025780439377, -0.03119506500661373], "norm_scores": [0.33740243315696716, 0.33195459842681885, 0.330642968416214]}}
{"id": "WebQTest-695", "question": "what role did paul mccartney play in the beatles", "prediction": [["music.musical_group.member", "music.group_membership.role"], ["music.musical_group.member", "music.performance_role.regular_performances"], ["music.producer.releases_produced", "music.group_membership.member", "music.group_membership.role"]], "ground_paths": [["music.group_member.instruments_played", "music.instrument.family"], ["music.group_member.membership", "music.performance_role.regular_performances"], ["music.group_membership.member", "music.group_membership.role"], ["music.group_member.membership", "music.group_membership.role"], ["music.group_member.instruments_played", "music.instrument.variation"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what role did paul mccartney play in the beatles [/INST]", "raw_output": {"paths": ["<PATH> music.musical_group.member <SEP> music.group_membership.role </PATH>", "<PATH> music.musical_group.member <SEP> music.performance_role.regular_performances </PATH>", "<PATH> music.producer.releases_produced <SEP> music.group_membership.member <SEP> music.group_membership.role </PATH>"], "scores": [-0.013366755098104477, -0.021111438050866127, -0.02528172731399536], "norm_scores": [0.33552083373069763, 0.33293235301971436, 0.3315468430519104]}}
{"id": "WebQTest-696", "question": "when did juventus win the champions league", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did juventus win the champions league [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.005385310389101505, -0.023862313479185104, -0.0871908888220787], "norm_scores": [0.34445419907569885, 0.3381481468677521, 0.31739768385887146]}}
{"id": "WebQTest-698", "question": "who is robin roberts father", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.gender"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is robin roberts father [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.008357315324246883, -0.022559387609362602, -0.10087078809738159], "norm_scores": [0.345120906829834, 0.3402540981769562, 0.31462496519088745]}}
{"id": "WebQTest-699", "question": "what is the capital of australia victoria state", "prediction": [["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["base.biblioness.bibs_location.state"]], "ground_paths": [["base.biblioness.bibs_location.state"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of australia victoria state [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>"], "scores": [-0.015566122718155384, -0.037992071360349655, -0.03851284086704254], "norm_scores": [0.3383936583995819, 0.3308893144130707, 0.3307170271873474]}}
{"id": "WebQTest-700", "question": "where is french spoken most", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["base.rosetta.local_name.locale"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is french spoken most [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.016710050404071808, -0.02326531894505024, -0.023900143802165985], "norm_scores": [0.3348623216152191, 0.33267441391944885, 0.33246326446533203]}}
{"id": "WebQTest-701", "question": "what team is reggie bush on 2011", "prediction": [["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"]], "ground_paths": [["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is reggie bush on 2011 [/INST]", "raw_output": {"paths": ["<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.team </PATH>"], "scores": [-0.020354704931378365, -0.02271716296672821, -0.02396136149764061], "norm_scores": [0.33399683237075806, 0.33320873975753784, 0.3327943980693817]}}
{"id": "WebQTest-702", "question": "what highschool did emily osment go to", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what highschool did emily osment go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.014979643747210503, -0.02157081849873066, -0.022357523441314697], "norm_scores": [0.33488723635673523, 0.3326871991157532, 0.3324255645275116]}}
{"id": "WebQTest-703", "question": "what currency should i take to mexico", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should i take to mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>"], "scores": [-0.005498549900949001, -0.03282034397125244, -0.058476489037275314], "norm_scores": [0.34229588508605957, 0.3330703377723694, 0.32463371753692627]}}
{"id": "WebQTest-704", "question": "who is thor heyerdahl", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is thor heyerdahl [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0012992046540603042, -0.05769803747534752, -0.09904750436544418], "norm_scores": [0.3506263196468353, 0.33139872550964355, 0.3179749846458435]}}
{"id": "WebQTest-706", "question": "where did troy aikman play football", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did troy aikman play football [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.011529073119163513, -0.014136475510895252, -0.058632154017686844], "norm_scores": [0.33882397413253784, 0.3379416763782501, 0.3232343792915344]}}
{"id": "WebQTest-707", "question": "who played cletus hogg", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played cletus hogg [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.020509034395217896, -0.027119874954223633, -0.03131664916872978], "norm_scores": [0.3352710008621216, 0.3330618739128113, 0.3316670358181]}}
{"id": "WebQTest-708", "question": "who won fa cup 1976", "prediction": [["sports.sports_award_type.winners", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_team.championships"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.championship", "sports.sports_team.championships"], ["sports.sports_award.award", "sports.sports_award_winner.awards"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won fa cup 1976 [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_type.winners <SEP> sports.sports_award.award_winner </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.018995631486177444, -0.023372096940875053, -0.02418917417526245], "norm_scores": [0.33439749479293823, 0.33293721079826355, 0.33266526460647583]}}
{"id": "WebQTest-709", "question": "what did general robert e lee die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["base.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did general robert e lee die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> base.cause_of_death.people </PATH>"], "scores": [-0.0017222246387973428, -0.04064415767788887, -0.11419560760259628], "norm_scores": [0.3502078652381897, 0.336838960647583, 0.3129531443119049]}}
{"id": "WebQTest-710", "question": "what style of art did henri matisse do", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what style of art did henri matisse do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.0018672870937734842, -0.03690488636493683, -0.08214996010065079], "norm_scores": [0.34620949625968933, 0.33428919315338135, 0.3195013105869293]}}
{"id": "WebQTest-711", "question": "who plays chuck bass in gossip girl", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"]], "ground_paths": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["base.gossipgirl.character.played_by"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_winning_work.awards_won", "award.award_honor.award_winner"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays chuck bass in gossip girl [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.021758537739515305, -0.031019557267427444, -0.03407673165202141], "norm_scores": [0.3357350826263428, 0.3326401710510254, 0.3316247761249542]}}
{"id": "WebQTest-712", "question": "what countries have territories in oceania", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries have territories in oceania [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.008839831687510014, -0.034623514860868454, -0.04424438625574112], "norm_scores": [0.3401637375354767, 0.33150514960289, 0.3283311128616333]}}
{"id": "WebQTest-713", "question": "what county is st. louis park in", "prediction": [["location.location.containedby"], ["location.us_county.county_seat"], ["location.us_county.hud_county_place"]], "ground_paths": [["location.hud_county_place.county"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st. louis park in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.us_county.hud_county_place </PATH>"], "scores": [-0.0064353118650615215, -0.03955712914466858, -0.04698815569281578], "norm_scores": [0.3415674865245819, 0.3304394483566284, 0.3279930651187897]}}
{"id": "WebQTest-714", "question": "what is the political system of south africa", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the political system of south africa [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.011553341522812843, -0.01271105743944645, -0.07641848921775818], "norm_scores": [0.34059518575668335, 0.34020110964775085, 0.3192037045955658]}}
{"id": "WebQTest-716", "question": "what language group does germany belong to", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.language_family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language group does germany belong to [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.01265970803797245, -0.027773259207606316, -0.03587673604488373], "norm_scores": [0.33760398626327515, 0.3325399160385132, 0.32985609769821167]}}
{"id": "WebQTest-717", "question": "what timezone is tucson arizona in right now", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is tucson arizona in right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-8.95030279934872e-06, -0.1741858571767807, -0.18192873895168304], "norm_scores": [0.3739972412586212, 0.3142131567001343, 0.3117896318435669]}}
{"id": "WebQTest-718", "question": "when was lucy lawless born", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was lucy lawless born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.0007279200945049524, -0.07613353431224823, -0.09051038324832916], "norm_scores": [0.35192716121673584, 0.32636570930480957, 0.32170718908309937]}}
{"id": "WebQTest-719", "question": "which political party does barack obama represent", "prediction": [["government.politician.party", "government.political_party_tenure.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "ground_paths": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["government.politician.election_campaigns", "government.election_campaign.party"], ["government.election.winner", "government.primary_election.party"], ["government.politician.party", "government.political_party_tenure.party"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which political party does barack obama represent [/INST]", "raw_output": {"paths": ["<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.party </PATH>", "<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>"], "scores": [-0.019200488924980164, -0.021458497270941734, -0.021977392956614494], "norm_scores": [0.3338930010795593, 0.33313992619514465, 0.3329671025276184]}}
{"id": "WebQTest-721", "question": "what kind of leukemia did jill clayburgh have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["people.person.medical_condition"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of leukemia did jill clayburgh have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.person.medical_condition </PATH>"], "scores": [-0.011952407658100128, -0.012784077785909176, -0.058813005685806274], "norm_scores": [0.3385940492153168, 0.338312566280365, 0.32309335470199585]}}
{"id": "WebQTest-723", "question": "what is there to see in sydney australia", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is there to see in sydney australia [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.014345821924507618, -0.016088426113128662, -0.028463268652558327], "norm_scores": [0.3350936472415924, 0.3345102071762085, 0.33039620518684387]}}
{"id": "WebQTest-724", "question": "where was marilyn monroe found dead", "prediction": [["people.deceased_person.place_of_death"], ["people.cause_of_death.people"], ["people.place_of_death.people_found_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was marilyn monroe found dead [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.place_of_death.people_found_here </PATH>"], "scores": [-0.0003017555281985551, -0.08563786000013351, -0.1169452890753746], "norm_scores": [0.35611191391944885, 0.3269832730293274, 0.31690484285354614]}}
{"id": "WebQTest-725", "question": "what states does the sierra nevada run through", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.containedby"]], "ground_paths": [["location.location.partially_contains"], ["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what states does the sierra nevada run through [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0070489137433469296, -0.02747514843940735, -0.04324377328157425], "norm_scores": [0.3396470248699188, 0.3327796459197998, 0.32757335901260376]}}
{"id": "WebQTest-726", "question": "what language is spoken in greek", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is spoken in greek [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.014315604232251644, -0.018231963738799095, -0.042921777814626694], "norm_scores": [0.33693966269493103, 0.3356226682662964, 0.3274376392364502]}}
{"id": "WebQTest-728", "question": "what is the name of the speaker of the house of representatives now 2011", "prediction": [["common.topic.image", "common.topic.image"], ["government.government_office_or_title.office_holders", "government.government_position_held.office_holder"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"]], "ground_paths": [["common.topic.image", "common.image.appears_in_topic_gallery"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the speaker of the house of representatives now 2011 [/INST]", "raw_output": {"paths": ["<PATH> common.topic.image <SEP> common.topic.image </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.010127134621143341, -0.016514398157596588, -0.026052121073007584], "norm_scores": [0.33581453561782837, 0.33367642760276794, 0.3305090367794037]}}
{"id": "WebQTest-729", "question": "who plays stanley hudson", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays stanley hudson [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.021961769089102745, -0.030552472919225693, -0.032546866685152054], "norm_scores": [0.33546727895736694, 0.3325977027416229, 0.3319350481033325]}}
{"id": "WebQTest-730", "question": "what record label is kelly clarkson signed with", "prediction": [["music.artist.label"], ["music.record_label.artist"], ["music.recording.artist"]], "ground_paths": [["music.record_label.artist"], ["music.artist.label"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what record label is kelly clarkson signed with [/INST]", "raw_output": {"paths": ["<PATH> music.artist.label </PATH>", "<PATH> music.record_label.artist </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.0018383885035291314, -0.05861372500658035, -0.07304158806800842], "norm_scores": [0.34769558906555176, 0.3285049796104431, 0.32379937171936035]}}
{"id": "WebQTest-731", "question": "where does tim roth come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does tim roth come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.006862818729132414, -0.032563529908657074, -0.04958527535200119], "norm_scores": [0.34097060561180115, 0.33231908082962036, 0.3267103135585785]}}
{"id": "WebQTest-733", "question": "what currency should you take to vietnam", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"]], "ground_paths": [["base.monetarydebase.debased_currency.issuing_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should you take to vietnam [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>"], "scores": [-0.0113655561581254, -0.016248611733317375, -0.07141692191362381], "norm_scores": [0.34050145745277405, 0.33884280920028687, 0.3206557333469391]}}
{"id": "WebQTest-734", "question": "who did roger federer married", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.nationality", "people.person.nationality"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did roger federer married [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.dated.participant </PATH>"], "scores": [-0.024533620104193687, -0.034820523113012314, -0.041018132120370865], "norm_scores": [0.3363134562969208, 0.33287158608436584, 0.330814927816391]}}
{"id": "WebQTest-735", "question": "what kind of money do i need in costa rica", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["government.form_of_government.countries"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money do i need in costa rica [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> government.form_of_government.countries </PATH>"], "scores": [-0.0064169373363256454, -0.02171608991920948, -0.10477245599031448], "norm_scores": [0.34588387608528137, 0.3406324088573456, 0.31348374485969543]}}
{"id": "WebQTest-737", "question": "what is the name of the capital of belgium", "prediction": [["location.country.capital"], ["base.biblioness.bibs_location.country"], ["location.location.containedby"]], "ground_paths": [["base.biblioness.bibs_location.country"], ["location.country.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the capital of belgium [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.011806539259850979, -0.022224701941013336, -0.03002946451306343], "norm_scores": [0.33652153611183167, 0.33303380012512207, 0.3304446339607239]}}
{"id": "WebQTest-738", "question": "what kind of government does japan have now", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does japan have now [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.009330442175269127, -0.014852622523903847, -0.12175489217042923], "norm_scores": [0.34624147415161133, 0.3443347215652466, 0.3094238340854645]}}
{"id": "WebQTest-739", "question": "what pieces of music did claude debussy compose", "prediction": [["music.composition.composer"], ["music.artist.genre"], ["music.recording.artist"]], "ground_paths": [["opera.opera.librettist"], ["music.lyricist.lyrics_written"], ["book.author.works_written", "book.book.editions"], ["opera.librettist.libretti"], ["base.musiteca.music_teacher.composition"], ["music.recording.artist"], ["book.written_work.author"], ["book.author.works_written"], ["music.composition.composer"], ["music.composition.lyricist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what pieces of music did claude debussy compose [/INST]", "raw_output": {"paths": ["<PATH> music.composition.composer </PATH>", "<PATH> music.artist.genre </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.007458534091711044, -0.03406412899494171, -0.05381351709365845], "norm_scores": [0.34147778153419495, 0.33251234889030457, 0.3260098695755005]}}
{"id": "WebQTest-740", "question": "who was selena gomez in barney and friends", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.series", "tv.tv_character.appeared_in_tv_program"]], "ground_paths": [["tv.tv_guest_role.actor", "tv.tv_guest_role.character"], ["tv.tv_program.regular_cast", "tv.tv_character.appeared_in_tv_program"], ["tv.tv_actor.guest_roles", "tv.tv_character.appeared_in_tv_episodes"], ["tv.tv_guest_role.actor", "tv.tv_character.appeared_in_tv_episodes"], ["tv.tv_actor.guest_roles", "tv.tv_guest_role.character"], ["film.actor.film", "film.performance.character"], ["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was selena gomez in barney and friends [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_character.appeared_in_tv_program </PATH>"], "scores": [-0.024238038808107376, -0.0267014279961586, -0.03356657922267914], "norm_scores": [0.33464351296424866, 0.3338201642036438, 0.33153629302978516]}}
{"id": "WebQTest-741", "question": "who plays caesar flickerman in the hunger games", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["film.film.starring", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays caesar flickerman in the hunger games [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.02648363634943962, -0.03009244240820408, -0.03863149136304855], "norm_scores": [0.33508434891700745, 0.3338772654533386, 0.3310384154319763]}}
{"id": "WebQTest-742", "question": "where did the gettysburg address", "prediction": [["book.written_work.subjects"], ["book.author.book_editions_published"], ["book.book_subject.works"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the gettysburg address [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> book.book_subject.works </PATH>"], "scores": [-0.012053006328642368, -0.04435208439826965, -0.04835714399814606], "norm_scores": [0.3409985303878784, 0.33016055822372437, 0.3288409113883972]}}
{"id": "WebQTest-743", "question": "what was the name of jfk 's brothers", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.profession", "people.person.profession"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.parents", "people.person.children"], ["base.activism.activist.area_of_activism", "base.activism.activist.area_of_activism"], ["film.film_subject.films", "film.film.subjects"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["people.deceased_person.cause_of_death", "people.deceased_person.cause_of_death"], ["people.person.parents", "people.person.parents"], ["people.family.members", "people.family.members"], ["military.military_person.participated_in_conflicts", "military.military_person.participated_in_conflicts"], ["government.government_position_held.appointed_by", "government.politician.government_positions_held"], ["people.family.members", "people.family_member.family"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["base.americancomedy.impersonated_celebrity.impersonated_by", "base.americancomedy.impersonated_celebrity.impersonated_by"], ["people.person.religion", "people.person.religion"], ["people.person.sibling_s", "people.sibling_relationship.sibling"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_no_value"], ["people.person.profession", "people.person.profession"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["people.person.nationality", "people.person.nationality"], ["people.person.ethnicity", "people.person.ethnicity"], ["people.person.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the name of jfk 's brothers [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.03124704584479332, -0.03455531224608421, -0.03676582872867584], "norm_scores": [0.3343147039413452, 0.3332105278968811, 0.3324747681617737]}}
{"id": "WebQTest-744", "question": "who was tupac signed to", "prediction": [["music.artist.label"], ["music.record_label.artist"], ["music.recording.artist"]], "ground_paths": [["music.artist.label"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was tupac signed to [/INST]", "raw_output": {"paths": ["<PATH> music.artist.label </PATH>", "<PATH> music.record_label.artist </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.00931481458246708, -0.0243435800075531, -0.05730060115456581], "norm_scores": [0.34034085273742676, 0.3352642059326172, 0.32439497113227844]}}
{"id": "WebQTest-745", "question": "what was augustus caesar famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was augustus caesar famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.004833129234611988, -0.04478036239743233, -0.05196325108408928], "norm_scores": [0.34307628870010376, 0.3296414613723755, 0.3272821605205536]}}
{"id": "WebQTest-747", "question": "what language does colombia", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does colombia [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.010468009859323502, -0.019446494057774544, -0.06000421568751335], "norm_scores": [0.3398202359676361, 0.3367828130722046, 0.3233969509601593]}}
{"id": "WebQTest-748", "question": "what is the zip code for schenectady new york", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for schenectady new york [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0004785721830558032, -0.06546729803085327, -0.11016916483640671], "norm_scores": [0.3529590964317322, 0.3307501971721649, 0.31629064679145813]}}
{"id": "WebQTest-749", "question": "who was president in 1988 in the united states", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_office_or_title.jurisdiction", "people.person.nationality"]], "ground_paths": [["government.government_office_or_title.jurisdiction", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["freebase.type_profile.equivalent_topic", "common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was president in 1988 in the united states [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_office_or_title.jurisdiction <SEP> people.person.nationality </PATH>"], "scores": [-0.023253662511706352, -0.023628827184438705, -0.0260477252304554], "norm_scores": [0.33368539810180664, 0.3335602283477783, 0.33275434374809265]}}
{"id": "WebQTest-751", "question": "who owns google 2012", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"], ["people.person.gender", "people.person.gender"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["conferences.conference.focus", "time.event.people_involved"], ["organization.organization.leadership", "business.board_member.leader_of"], ["organization.leadership.organization", "organization.leadership.person"], ["organization.leadership.organization", "business.board_member.leader_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who owns google 2012 [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.020626557990908623, -0.022441165521740913, -0.051515381783246994], "norm_scores": [0.3369532823562622, 0.33634239435195923, 0.3267042934894562]}}
{"id": "WebQTest-752", "question": "what drink did john pemberton create", "prediction": [["food.beer_country_region.beers_from_here"], ["food.beer_country_region.beers_from_here", "food.beer_country_region.beers_from_here"], ["food.beer.from_region"]], "ground_paths": [["law.inventor.inventions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what drink did john pemberton create [/INST]", "raw_output": {"paths": ["<PATH> food.beer_country_region.beers_from_here </PATH>", "<PATH> food.beer_country_region.beers_from_here <SEP> food.beer_country_region.beers_from_here </PATH>", "<PATH> food.beer.from_region </PATH>"], "scores": [-0.008564773015677929, -0.026955919340252876, -0.04512413591146469], "norm_scores": [0.3394573926925659, 0.33327141404151917, 0.3272711932659149]}}
{"id": "WebQTest-753", "question": "what position did stanley matthews play", "prediction": [["sports.pro_athlete.teams", "sports.sports_position.teams"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_position.sport", "sports.sports_team.sport"]], "ground_paths": [["soccer.football_player.position_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what position did stanley matthews play [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_position.teams </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>"], "scores": [-0.030759990215301514, -0.03593507036566734, -0.037204016000032425], "norm_scores": [0.3346255421638489, 0.3328983187675476, 0.3324761390686035]}}
{"id": "WebQTest-755", "question": "what was bonnie prince charlie full name", "prediction": [["people.person.nationality"], ["common.topic.notable_types"], ["people.person.children"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was bonnie prince charlie full name [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.007691999431699514, -0.05172765627503395, -0.05538852512836456], "norm_scores": [0.34360212087631226, 0.32879969477653503, 0.3275981843471527]}}
{"id": "WebQTest-756", "question": "who makes the voice of stewie griffin", "prediction": [["fictional_universe.fictional_character_creator.fictional_characters_created"], ["fictional_universe.fictional_character.character_created_by"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"]], "ground_paths": [["fictional_universe.fictional_character.character_created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who makes the voice of stewie griffin [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>"], "scores": [-0.019352922216057777, -0.02199718914926052, -0.022553199902176857], "norm_scores": [0.33398303389549255, 0.3331010639667511, 0.33291590213775635]}}
{"id": "WebQTest-757", "question": "who is angelina jolie mom and dad", "prediction": [["people.person.parents"], ["people.person.children"], ["award.award_nominee.award_nominations", "award.award_nominee.award_nominations"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is angelina jolie mom and dad [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.010541016235947609, -0.015655947849154472, -0.09590858966112137], "norm_scores": [0.3432801365852356, 0.3415287435054779, 0.3151911497116089]}}
{"id": "WebQTest-758", "question": "who is the governor of indiana 2009", "prediction": [["location.location.containedby", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["common.topic.image"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the governor of indiana 2009 [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.019089186564087868, -0.01916583813726902, -0.023718565702438354], "norm_scores": [0.33385586738586426, 0.3338302671909332, 0.3323138952255249]}}
{"id": "WebQTest-759", "question": "who brad pitt has dated", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"]], "ground_paths": [["base.popstra.dated.participant", "base.popstra.dated.participant"], ["people.person.nationality", "people.person.place_of_birth"], ["base.popstra.celebrity.canoodled", "base.popstra.celebrity.canoodled"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["base.popstra.celebrity.insult_victim", "base.popstra.public_insult.perpetrator"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.lived_with.participant", "base.popstra.celebrity.lived_with"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who brad pitt has dated [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>"], "scores": [-0.016363389790058136, -0.030474836006760597, -0.05026821047067642], "norm_scores": [0.33867865800857544, 0.3339329957962036, 0.32738831639289856]}}
{"id": "WebQTest-760", "question": "what party does barack obama belong to", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"], ["government.election_campaign.candidate", "government.election_campaign.party"]], "ground_paths": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["government.politician.election_campaigns", "government.election_campaign.party"], ["government.election.winner", "government.primary_election.party"], ["government.politician.party", "government.political_party_tenure.party"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what party does barack obama belong to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.party </PATH>"], "scores": [-0.017370374873280525, -0.02002875506877899, -0.027711516246199608], "norm_scores": [0.3347776532173157, 0.33388885855674744, 0.3313334882259369]}}
{"id": "WebQTest-761", "question": "what is san francisco zip code", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"], ["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is san francisco zip code [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.000232475736993365, -0.08640420436859131, -0.16173456609249115], "norm_scores": [0.36123234033584595, 0.3314077854156494, 0.30735987424850464]}}
{"id": "WebQTest-762", "question": "who has stephen ireland played for", "prediction": [["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has stephen ireland played for [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.03193158656358719, -0.03210503235459328, -0.03733913600444794], "norm_scores": [0.33395296335220337, 0.3338950574398041, 0.33215197920799255]}}
{"id": "WebQTest-764", "question": "where did john mayer go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did john mayer go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.015168172307312489, -0.01916409842669964, -0.021068129688501358], "norm_scores": [0.3344336748123169, 0.33309996128082275, 0.33246633410453796]}}
{"id": "WebQTest-765", "question": "what celebrities has brody jenner dated", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["base.popstra.celebrity.canoodled", "base.popstra.canoodled.participant"]], "ground_paths": [["base.popstra.dated.participant", "base.popstra.dated.participant"], ["base.popstra.canoodled.participant", "base.popstra.celebrity.canoodled"], ["people.person.gender", "people.person.gender"], ["base.popstra.celebrity.canoodled", "base.popstra.celebrity.canoodled"], ["base.popstra.breakup.participant", "base.popstra.celebrity.breakup"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["base.popstra.celebrity.canoodled", "base.popstra.canoodled.participant"], ["base.popstra.infidelity.perpetrator", "base.popstra.infidelity.participant"], ["people.person.nationality", "people.person.nationality"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.popstra.breakup.participant", "base.popstra.breakup.participant"], ["people.person.profession", "people.person.profession"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["base.popstra.friendship.participant", "base.popstra.friendship.participant"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what celebrities has brody jenner dated [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> base.popstra.celebrity.canoodled <SEP> base.popstra.canoodled.participant </PATH>"], "scores": [-0.016737213358283043, -0.027846025303006172, -0.03835951164364815], "norm_scores": [0.33697691559791565, 0.33325421810150146, 0.3297688961029053]}}
{"id": "WebQTest-766", "question": "what is malaysian currency", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is malaysian currency [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.00694622378796339, -0.023781003430485725, -0.12776511907577515], "norm_scores": [0.34849268198013306, 0.34267500042915344, 0.3088322877883911]}}
{"id": "WebQTest-768", "question": "who owns volkswagen car company", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"], ["organization.organization.board_members", "business.board_member.organization_board_memberships"]], "ground_paths": [["business.business_operation.industry", "business.business_operation.industry"], ["organization.organization.parent", "organization.organization_relationship.parent"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"], ["base.schemastaging.corresponding_entity.entities", "base.schemastaging.entity_hint.corresponds_to"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who owns volkswagen car company [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization.board_members <SEP> business.board_member.organization_board_memberships </PATH>"], "scores": [-0.025511378422379494, -0.030354758724570274, -0.03928098827600479], "norm_scores": [0.3354024291038513, 0.3337818682193756, 0.33081570267677307]}}
{"id": "WebQTest-769", "question": "which is the main train station in rome", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["trains.travel_destination.tourist_attractions"]], "ground_paths": [["travel.travel_destination.how_to_get_here", "travel.transportation.transport_terminus"], ["location.location.containedby", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which is the main train station in rome [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> trains.travel_destination.tourist_attractions </PATH>"], "scores": [-0.002916030352935195, -0.06397848576307297, -0.07238104194402695], "norm_scores": [0.3479886054992676, 0.3273753523826599, 0.3246360421180725]}}
{"id": "WebQTest-770", "question": "what did steve mcnair died from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did steve mcnair died from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0009995201835408807, -0.05038907378911972, -0.11027415096759796], "norm_scores": [0.3510873317718506, 0.3341684937477112, 0.314744234085083]}}
{"id": "WebQTest-771", "question": "who is the newly elected governor of california", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["location.location.containedby", "people.person.nationality"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the newly elected governor of california [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>"], "scores": [-0.020959768444299698, -0.023242061957716942, -0.029395850375294685], "norm_scores": [0.3345242738723755, 0.33376163244247437, 0.33171406388282776]}}
{"id": "WebQTest-772", "question": "who plays roxanne in ghost rider", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"], ["film.film.language", "people.person.languages"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["film.film.language", "people.person.languages"], ["film.performance.film", "film.performance.actor"], ["film.film.starring", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays roxanne in ghost rider [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.film.language <SEP> people.person.languages </PATH>"], "scores": [-0.02120729722082615, -0.024146668612957, -0.029025116935372353], "norm_scores": [0.33452898263931274, 0.33354711532592773, 0.3319239020347595]}}
{"id": "WebQTest-773", "question": "where is the sony ericsson company", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the sony ericsson company [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>"], "scores": [-0.005179275758564472, -0.033610109239816666, -0.045932210981845856], "norm_scores": [0.34105995297431946, 0.331499844789505, 0.32744014263153076]}}
{"id": "WebQTest-774", "question": "what kind of currency does the dominican republic have", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of currency does the dominican republic have [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-6.386948371073231e-05, -0.1085883378982544, -0.13476507365703583], "norm_scores": [0.3608631193637848, 0.3237508535385132, 0.3153860569000244]}}
{"id": "WebQTest-775", "question": "what to do this weekend in manchester with kids", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do this weekend in manchester with kids [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.012516193091869354, -0.016411520540714264, -0.02890888974070549], "norm_scores": [0.3355869948863983, 0.3342823386192322, 0.3301306664943695]}}
{"id": "WebQTest-776", "question": "what sport is popular in italy", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["cricket.cricket_team.country"]], "ground_paths": [["cricket.cricket_team.country"], ["sports.sports_team.location"], ["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sport is popular in italy [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> cricket.cricket_team.country </PATH>"], "scores": [-0.010908142663538456, -0.014788289554417133, -0.12920716404914856], "norm_scores": [0.3466736078262329, 0.3453310430049896, 0.30799534916877747]}}
{"id": "WebQTest-777", "question": "where is the main headquarters of google", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["organization.organization.phone_number", "common.phone_number.service_location"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the main headquarters of google [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.01581057533621788, -0.02581137791275978, -0.03531072661280632], "norm_scores": [0.3366166949272156, 0.33326706290245056, 0.3301162123680115]}}
{"id": "WebQTest-778", "question": "what was jesse owens education", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was jesse owens education [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.012958141043782234, -0.019359499216079712, -0.033604416996240616], "norm_scores": [0.33633968234062195, 0.33419352769851685, 0.32946673035621643]}}
{"id": "WebQTest-779", "question": "where does carlos santana live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.children", "people.person.place_of_birth"]], "ground_paths": [["music.artist.origin", "location.location.containedby"], ["people.person.nationality", "location.country.first_level_divisions"], ["people.place_lived.person", "people.place_lived.location"], ["symbols.namesake.named_after", "music.artist.origin"], ["people.person.nationality", "location.location.containedby"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.administrative_division.country"], ["people.person.children", "people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does carlos santana live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.019688306376338005, -0.022606343030929565, -0.028649844229221344], "norm_scores": [0.3346535861492157, 0.3336784839630127, 0.331667959690094]}}
{"id": "WebQTest-780", "question": "what is the new movie john carter about", "prediction": [["film.film_story_contributor.film_story_credits"], ["film.film.subjects"], ["film.film.story_by"]], "ground_paths": [["film.film.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the new movie john carter about [/INST]", "raw_output": {"paths": ["<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> film.film.subjects </PATH>", "<PATH> film.film.story_by </PATH>"], "scores": [-0.02152634784579277, -0.021731648594141006, -0.0327693447470665], "norm_scores": [0.3346031606197357, 0.33453449606895447, 0.3308623135089874]}}
{"id": "WebQTest-782", "question": "what countries does nafta include", "prediction": [["organization.membership_organization.members", "organization.organization_member.member_of"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["common.topic.notable_types"]], "ground_paths": [["organization.membership_organization.members", "organization.organization_membership.member"], ["organization.organization_founder.organizations_founded", "people.person.nationality"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["organization.membership_organization.members", "organization.organization_member.member_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does nafta include [/INST]", "raw_output": {"paths": ["<PATH> organization.membership_organization.members <SEP> organization.organization_member.member_of </PATH>", "<PATH> organization.organization_membership.organization <SEP> organization.organization_member.member_of </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.010853754356503487, -0.025819404050707817, -0.03816820681095123], "norm_scores": [0.33804330229759216, 0.3330219089984894, 0.32893481850624084]}}
{"id": "WebQTest-783", "question": "what county is houston tx in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"]], "ground_paths": [["location.hud_county_place.county"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is houston tx in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.005550291854888201, -0.03440132364630699, -0.04427963122725487], "norm_scores": [0.3408813178539276, 0.33118706941604614, 0.32793158292770386]}}
{"id": "WebQTest-784", "question": "where do the seattle seahawks play", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_team.location"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the seattle seahawks play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.location </PATH>"], "scores": [-0.009003018029034138, -0.015980171039700508, -0.08450313657522202], "norm_scores": [0.34242743253707886, 0.34004658460617065, 0.3175259828567505]}}
{"id": "WebQTest-785", "question": "who does michael keaton play in cars", "prediction": [["film.performance.actor", "film.performance.character"], ["film.performance.film", "film.performance.character"], ["film.performance.actor", "film.film_character.portrayed_in_films"]], "ground_paths": [["film.performance.film", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["fictional_universe.fictional_universe.works_set_here", "fictional_universe.fictional_universe.characters"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does michael keaton play in cars [/INST]", "raw_output": {"paths": ["<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.02830260433256626, -0.03140652924776077, -0.03307454660534859], "norm_scores": [0.3342089354991913, 0.33317315578460693, 0.3326179087162018]}}
{"id": "WebQTest-787", "question": "where is the riviera casino", "prediction": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the riviera casino [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.008894164115190506, -0.03682596981525421, -0.04652313143014908], "norm_scores": [0.3406546115875244, 0.33127114176750183, 0.32807430624961853]}}
{"id": "WebQTest-788", "question": "what is the holy book of the buddhist religion", "prediction": [["religion.religion.texts"], ["religion.religious_text.religious_text_of"], ["book.written_work.subjects"]], "ground_paths": [["religion.religious_text.religious_text_of"], ["religion.religion.texts"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the holy book of the buddhist religion [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.texts </PATH>", "<PATH> religion.religious_text.religious_text_of </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.004697415512055159, -0.02718096598982811, -0.062269579619169235], "norm_scores": [0.34225231409072876, 0.33464309573173523, 0.323104590177536]}}
{"id": "WebQTest-789", "question": "who became president when henry harrison died", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who became president when henry harrison died [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.02326262556016445, -0.025171509012579918, -0.03203113377094269], "norm_scores": [0.3345194458961487, 0.33388152718544006, 0.33159902691841125]}}
{"id": "WebQTest-790", "question": "where did richard arkwright grow up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did richard arkwright grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0016587943537160754, -0.04484184831380844, -0.1269766241312027], "norm_scores": [0.352118581533432, 0.3372366428375244, 0.31064480543136597]}}
{"id": "WebQTest-791", "question": "what other books did louis sachar write", "prediction": [["book.book_edition.author_editor"], ["book.author.works_written"], ["book.written_work.author"]], "ground_paths": [["book.author.works_written"], ["book.book_edition.author_editor"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other books did louis sachar write [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.012251581996679306, -0.02622288092970848, -0.028831271454691887], "norm_scores": [0.3367363214492798, 0.33206436038017273, 0.3311993479728699]}}
{"id": "WebQTest-792", "question": "when did roth ira originate", "prediction": [["finance.currency.countries_used"], ["common.topic.notable_types"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did roth ira originate [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.013778218068182468, -0.029959237203001976, -0.039539601653814316], "norm_scores": [0.33800721168518066, 0.33258190751075745, 0.3294108510017395]}}
{"id": "WebQTest-794", "question": "who does the voice for darth vader", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.performance.special_performance_type", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does the voice for darth vader [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>"], "scores": [-0.02336173877120018, -0.024202700704336166, -0.028252016752958298], "norm_scores": [0.33396998047828674, 0.33368924260139465, 0.3323407769203186]}}
{"id": "WebQTest-796", "question": "what did thomas malthus do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did thomas malthus do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0018121881876140833, -0.05601053684949875, -0.07032778859138489], "norm_scores": [0.3470989465713501, 0.3287874460220337, 0.324113667011261]}}
{"id": "WebQTest-797", "question": "what is the zip code for newark california", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for newark california [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00013230511103756726, -0.0928792804479599, -0.15121658146381378], "norm_scores": [0.3608545660972595, 0.3288915455341339, 0.3102538287639618]}}
{"id": "WebQTest-798", "question": "what states share a border with tennessee", "prediction": [["meteorology.cyclone_affected_area.cyclones", "meteorology.cyclone_affected_area.cyclones"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.tropical_cyclone.affected_areas"], ["location.location.partially_contains", "location.location.partially_contains"]], "ground_paths": [["time.event.locations", "location.location.events"], ["base.locations.countries.states_provinces_within", "base.aareas.schema.administrative_area.administrative_children"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.containedby", "base.usnationalparks.us_national_park.state"], ["location.location.events", "time.event.locations"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.containedby", "location.location.contains"], ["book.written_work.subjects", "book.written_work.subjects"], ["location.location.partially_containedby", "location.location.partially_contains"], ["base.locations.countries.states_provinces_within", "base.locations.countries.states_provinces_within"], ["location.location.partially_contains", "geography.river.origin"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_contains", "location.location.partially_containedby"], ["base.locations.countries.states_provinces_within", "location.country.first_level_divisions"], ["location.location.events", "location.location.events"], ["common.topic.notable_types", "common.topic.notable_types"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.tropical_cyclone.affected_areas"], ["book.newspaper.circulation_areas", "periodicals.newspaper_circulation_area.newspapers"], ["location.location.containedby", "location.location.containedby"], ["base.locations.countries.states_provinces_within", "location.administrative_division.country"], ["base.locations.countries.states_provinces_within", "base.aareas.schema.administrative_area.administrative_parent"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["base.locations.countries.states_provinces_within", "location.administrative_division.first_level_division_of"], ["time.event.locations", "time.event.locations"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["periodicals.newspaper_circulation_area.newspapers", "book.newspaper.circulation_areas"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.tropical_cyclone.affected_areas"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.cyclone_affected_area.cyclones"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what states share a border with tennessee [/INST]", "raw_output": {"paths": ["<PATH> meteorology.cyclone_affected_area.cyclones <SEP> meteorology.cyclone_affected_area.cyclones </PATH>", "<PATH> meteorology.cyclone_affected_area.cyclones <SEP> meteorology.tropical_cyclone.affected_areas </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>"], "scores": [-0.03322787210345268, -0.033404670655727386, -0.04094681516289711], "norm_scores": [0.3342096209526062, 0.3341505229473114, 0.33163982629776]}}
{"id": "WebQTest-800", "question": "what war did the us lose the most soldiers", "prediction": [["base.culturalevent.event.entity_involved"], ["military.military_conflict.casualties", "military.casualties.military_conflict"], ["military.casualties.combatant", "military.casualties.military_conflict"]], "ground_paths": [["military.military_command.military_combatant", "military.military_conflict.commanders"], ["military.military_combatant.military_commanders", "military.military_conflict.commanders"], ["military.military_command.military_combatant", "military.military_command.military_conflict"], ["military.military_combatant.military_commanders", "military.military_command.military_conflict"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"], ["military.military_combatant.casualties", "military.casualties.military_conflict"], ["location.location.partially_containedby", "location.location.events"], ["people.person.nationality", "military.military_person.participated_in_conflicts"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what war did the us lose the most soldiers [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.casualties.military_conflict </PATH>", "<PATH> military.casualties.combatant <SEP> military.casualties.military_conflict </PATH>"], "scores": [-0.015101325698196888, -0.03033575601875782, -0.03778880834579468], "norm_scores": [0.3375585377216339, 0.33245500922203064, 0.3299863934516907]}}
{"id": "WebQTest-801", "question": "what kind of government is egypt", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government is egypt [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>"], "scores": [-0.006207296159118414, -0.02188745327293873, -0.12182401865720749], "norm_scores": [0.347794771194458, 0.34238380193710327, 0.30982136726379395]}}
{"id": "WebQTest-802", "question": "what musical instruments does justin bieber play", "prediction": [["music.artist.track_contributions", "music.track_contribution.role"], ["music.group_member.instruments_played"], ["music.track_contribution.contributor", "music.track_contribution.role"]], "ground_paths": [["music.artist.track_contributions", "music.track_contribution.role"], ["music.group_member.membership", "music.group_membership.role"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what musical instruments does justin bieber play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>", "<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.track_contribution.contributor <SEP> music.track_contribution.role </PATH>"], "scores": [-0.0036374921910464764, -0.04523085430264473, -0.05501539260149002], "norm_scores": [0.3437395393848419, 0.32973551750183105, 0.326524943113327]}}
{"id": "WebQTest-805", "question": "where do john lennon die", "prediction": [["people.deceased_person.place_of_death"], ["location.deceased_person.place_of_death"], ["location.location.people_born_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do john lennon die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0006607606774196029, -0.07845432311296463, -0.10943947732448578], "norm_scores": [0.35434800386428833, 0.32782697677612305, 0.31782495975494385]}}
{"id": "WebQTest-809", "question": "what is the current time in bangalore india", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the current time in bangalore india [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-0.0001805144129320979, -0.0713479220867157, -0.17657853662967682], "norm_scores": [0.36106425523757935, 0.3362612724304199, 0.30267447233200073]}}
{"id": "WebQTest-810", "question": "what language do people from greece speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people from greece speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.013974985107779503, -0.014381763525307178, -0.04937049746513367], "norm_scores": [0.3372889757156372, 0.33715179562568665, 0.3255592882633209]}}
{"id": "WebQTest-811", "question": "where is the phillies spring training stadium", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["location.location.containedby"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the phillies spring training stadium [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0034679442178457975, -0.046369802206754684, -0.04992067068815231], "norm_scores": [0.3433340787887573, 0.32891589403152466, 0.327750027179718]}}
{"id": "WebQTest-812", "question": "what movies does matt dallas play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [["film.actor.film", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.film.starring"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies does matt dallas play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.012449630536139011, -0.03427013009786606, -0.03702446073293686], "norm_scores": [0.33850792050361633, 0.3312015235424042, 0.3302905261516571]}}
{"id": "WebQTest-813", "question": "who was john f kennedy 's family", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_no_value"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.parents", "people.person.children"], ["base.activism.activist.area_of_activism", "base.activism.activist.area_of_activism"], ["film.film_subject.films", "film.film.subjects"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["people.deceased_person.cause_of_death", "people.deceased_person.cause_of_death"], ["people.person.parents", "people.person.parents"], ["people.person.parents"], ["military.military_person.participated_in_conflicts", "military.military_person.participated_in_conflicts"], ["symbols.name_source.namesakes"], ["government.government_position_held.appointed_by", "government.politician.government_positions_held"], ["people.family.members", "people.family.members"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.family.members", "people.family_member.family"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["base.americancomedy.impersonated_celebrity.impersonated_by", "base.americancomedy.impersonated_celebrity.impersonated_by"], ["people.person.religion", "people.person.religion"], ["people.person.sibling_s", "people.sibling_relationship.sibling"], ["people.sibling_relationship.sibling", "people.person.sibling_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_no_value"], ["people.person.profession", "people.person.profession"], ["people.person.profession", "common.topic.notable_types"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["people.person.nationality", "people.person.nationality"], ["people.person.ethnicity", "people.person.ethnicity"], ["people.person.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was john f kennedy 's family [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_no_value </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.03260942921042442, -0.03285719081759453, -0.034341927617788315], "norm_scores": [0.3335533142089844, 0.3334707021713257, 0.33297595381736755]}}
{"id": "WebQTest-814", "question": "where is harley davidson corporate headquarters", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.place_founded"]], "ground_paths": [["organization.organization.place_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is harley davidson corporate headquarters [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.place_founded </PATH>"], "scores": [-0.005135740619152784, -0.04085679352283478, -0.04335639625787735], "norm_scores": [0.34159886837005615, 0.32961198687553406, 0.3287891149520874]}}
{"id": "WebQTest-818", "question": "where was the ottoman empire based", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.country.capital"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the ottoman empire based [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.country.capital </PATH>"], "scores": [-0.006970958784222603, -0.041267361491918564, -0.06023797765374184], "norm_scores": [0.3431223928928375, 0.3315540552139282, 0.32532352209091187]}}
{"id": "WebQTest-819", "question": "where was bin laden found and killed", "prediction": [["base.culturalevent.event.entity_involved", "time.event.locations"], ["people.deceased_person.place_of_death"], ["base.culturalevent.event.entity_involved", "location.location.events"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was bin laden found and killed [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>"], "scores": [-0.020584797486662865, -0.03150584176182747, -0.03436531871557236], "norm_scores": [0.3360833525657654, 0.33243292570114136, 0.3314836919307709]}}
{"id": "WebQTest-820", "question": "what are the names of harry potter movies in order", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes"], ["fictional_universe.fictional_universe.works_set_here"], ["film.film_series.films_in_series"]], "ground_paths": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes"], ["film.film.film_series"], ["film.film_series.films_in_series"], ["fictional_universe.fictional_universe.works_set_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the names of harry potter movies in order [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes </PATH>", "<PATH> fictional_universe.fictional_universe.works_set_here </PATH>", "<PATH> film.film_series.films_in_series </PATH>"], "scores": [-0.006604698486626148, -0.032994307577610016, -0.03776479512453079], "norm_scores": [0.3397574722766876, 0.3309086561203003, 0.3293338418006897]}}
{"id": "WebQTest-821", "question": "what did pamela courson die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did pamela courson die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0023229585494846106, -0.03738861158490181, -0.11449193954467773], "norm_scores": [0.34971940517425537, 0.337668776512146, 0.31261181831359863]}}
{"id": "WebQTest-822", "question": "what did dr jack kevorkian do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["base.activism.activism_issue.activists"], ["base.activism.activist.area_of_activism"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did dr jack kevorkian do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0017948850290849805, -0.04893884062767029, -0.0717858150601387], "norm_scores": [0.3464580476284027, 0.3305036723613739, 0.3230382800102234]}}
{"id": "WebQTest-823", "question": "what does bolivia border", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.partially_containedby", "location.location.partially_contains"]], "ground_paths": [["location.location.partially_contains", "geography.river.basin_countries"], ["location.country.form_of_government", "government.form_of_government.countries"], ["language.language_family.geographic_distribution", "language.human_language.main_country"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"], ["location.country.languages_spoken", "location.country.official_language"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["time.event.locations", "location.location.events"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["people.deceased_person.place_of_death", "people.person.nationality"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["location.location.events", "time.event.locations"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.containedby", "location.location.contains_major_portion_of"], ["location.country.official_language", "language.human_language.main_country"], ["location.location.partially_containedby", "location.location.partially_contains"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["symbols.name_source.namesakes", "people.person.nationality"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.country.official_language", "location.country.languages_spoken"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["location.location.contains", "location.location.contains"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["location.location.partially_contains", "location.location.partially_containedby"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["location.location.events", "location.location.events"], ["common.topic.notable_types", "common.topic.notable_types"], ["language.language_family.geographic_distribution", "location.country.official_language"], ["location.location.containedby", "location.location.containedby"], ["location.country.form_of_government", "location.country.form_of_government"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["military.military_combatant_group.combatants", "military.military_combatant.military_conflicts"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["location.country.official_language", "language.human_language.countries_spoken_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.contains", "location.location.containedby"], ["time.event.locations", "time.event.locations"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does bolivia border [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>"], "scores": [-0.0362238846719265, -0.04316385090351105, -0.046068307012319565], "norm_scores": [0.33520063757896423, 0.33288243412971497, 0.3319169878959656]}}
{"id": "WebQTest-825", "question": "who does nolan ryan play for", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does nolan ryan play for [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.007934142835438251, -0.018075304105877876, -0.04075007885694504], "norm_scores": [0.33810892701148987, 0.334697425365448, 0.32719364762306213]}}
{"id": "WebQTest-827", "question": "what country does justin bieber come from", "prediction": [["people.person.nationality"], ["music.artist.origin"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country does justin bieber come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.004106791224330664, -0.042495548725128174, -0.05042382329702377], "norm_scores": [0.3428087830543518, 0.3298981785774231, 0.3272930085659027]}}
{"id": "WebQTest-828", "question": "what is the head judge of the supreme court called", "prediction": [["law.judicial_tenure.title", "law.judge.courts"], ["law.court.jurisdiction", "people.person.nationality"], ["law.court.jurisdiction", "government.government_office_or_title.office_holders"]], "ground_paths": [["government.governmental_body.offices_positions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the head judge of the supreme court called [/INST]", "raw_output": {"paths": ["<PATH> law.judicial_tenure.title <SEP> law.judge.courts </PATH>", "<PATH> law.court.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> law.court.jurisdiction <SEP> government.government_office_or_title.office_holders </PATH>"], "scores": [-0.028578806668519974, -0.034157946705818176, -0.04316842928528786], "norm_scores": [0.33557578921318054, 0.3337087631225586, 0.3307153880596161]}}
{"id": "WebQTest-829", "question": "who was frida kahlo influenced by", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"]], "ground_paths": [["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was frida kahlo influenced by [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>"], "scores": [-0.011475499719381332, -0.012281962670385838, -0.07342016696929932], "norm_scores": [0.3402368724346161, 0.33996260166168213, 0.3198004961013794]}}
{"id": "WebQTest-830", "question": "where is taylor swift from", "prediction": [["music.artist.origin"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is taylor swift from [/INST]", "raw_output": {"paths": ["<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.01541227474808693, -0.02148912474513054, -0.03179629519581795], "norm_scores": [0.3358306884765625, 0.33379608392715454, 0.33037325739860535]}}
{"id": "WebQTest-831", "question": "what type of religion does argentina have", "prediction": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"], ["religion.religion.countries"]], "ground_paths": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of religion does argentina have [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> religion.religion.countries </PATH>"], "scores": [-0.009161138907074928, -0.014043476432561874, -0.0748855248093605], "norm_scores": [0.3411201238632202, 0.33945873379707336, 0.31942111253738403]}}
{"id": "WebQTest-832", "question": "when is the last time the giants won the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is the last time the giants won the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.007274655159562826, -0.01738831028342247, -0.08126183599233627], "norm_scores": [0.34262755513191223, 0.33917978405952454, 0.31819260120391846]}}
{"id": "WebQTest-833", "question": "what is the main language used in china", "prediction": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the main language used in china [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.006554975174367428, -0.030807191506028175, -0.042084477841854095], "norm_scores": [0.34000495076179504, 0.33185824751853943, 0.3281368315219879]}}
{"id": "WebQTest-834", "question": "where did the battle of waterloo occur", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the battle of waterloo occur [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.0030861063860356808, -0.03807712718844414, -0.11486868560314178], "norm_scores": [0.34966835379600525, 0.33764466643333435, 0.312686949968338]}}
{"id": "WebQTest-836", "question": "who did vladimir lenin marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.parents"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "ground_paths": [["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.has_no_value", "freebase.valuenotation.has_no_value"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did vladimir lenin marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>"], "scores": [-0.02523914910852909, -0.044302914291620255, -0.04450187459588051], "norm_scores": [0.337605357170105, 0.3312302827835083, 0.3311643898487091]}}
{"id": "WebQTest-837", "question": "what year did the baltimore ravens win superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the baltimore ravens win superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.0133574353531003, -0.014693190343677998, -0.032303906977176666], "norm_scores": [0.33558207750320435, 0.3351341187953949, 0.32928383350372314]}}
{"id": "WebQTest-838", "question": "what team does cristiano ronaldo play for 2011", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"]], "ground_paths": [["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.transfers", "soccer.football_player_transfer.purchasing_team"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does cristiano ronaldo play for 2011 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>"], "scores": [-0.019886497408151627, -0.022151248529553413, -0.024670979008078575], "norm_scores": [0.3341168761253357, 0.33336102962493896, 0.33252209424972534]}}
{"id": "WebQTest-841", "question": "what is money called in italy", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is money called in italy [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.009981591254472733, -0.017607521265745163, -0.10241857916116714], "norm_scores": [0.34433960914611816, 0.3417236804962158, 0.313936710357666]}}
{"id": "WebQTest-843", "question": "where did theodor schwann study", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did theodor schwann study [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.02130955457687378, -0.021660091355443, -0.02390015870332718], "norm_scores": [0.3336600661277771, 0.3335431218147278, 0.3327968418598175]}}
{"id": "WebQTest-845", "question": "who are the virginia senators 2013", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.governmental_body.jurisdiction", "people.person.nationality"], ["government.government_position_held.governmental_body", "government.politician.government_positions_held"]], "ground_paths": [["location.administrative_division.country", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.election.district", "government.election.winner"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the virginia senators 2013 [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_body.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.016125278547406197, -0.02034090831875801, -0.03332516923546791], "norm_scores": [0.33571240305900574, 0.33430013060569763, 0.3299875557422638]}}
{"id": "WebQTest-846", "question": "what was dr seuss 's real name", "prediction": [["book.book.interior_illustrations_by", "people.person.nationality"], ["people.person.profession"], ["book.book_edition.author_editor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was dr seuss 's real name [/INST]", "raw_output": {"paths": ["<PATH> book.book.interior_illustrations_by <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.02241702377796173, -0.031901080161333084, -0.036282699555158615], "norm_scores": [0.3359322249889374, 0.332761287689209, 0.33130645751953125]}}
{"id": "WebQTest-847", "question": "what countries are members of the security council", "prediction": [["base.unitednations.united_nations_body_membership.body", "base.unitednations.united_nations_member_state.member_of"], ["base.unitednations.united_nations_body.members", "base.unitednations.united_nations_member_state.member_of"], ["base.unitednations.united_nations_body_membership.body", "base.unitednations.united_nations_body_membership.member"]], "ground_paths": [["base.unitednations.united_nations_body.members", "base.unitednations.united_nations_member_state.member_of"], ["base.unitednations.united_nations_body.members", "base.unitednations.united_nations_body_membership.member"], ["base.unitednations.united_nations_body_membership.body", "base.unitednations.united_nations_member_state.member_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are members of the security council [/INST]", "raw_output": {"paths": ["<PATH> base.unitednations.united_nations_body_membership.body <SEP> base.unitednations.united_nations_member_state.member_of </PATH>", "<PATH> base.unitednations.united_nations_body.members <SEP> base.unitednations.united_nations_member_state.member_of </PATH>", "<PATH> base.unitednations.united_nations_body_membership.body <SEP> base.unitednations.united_nations_body_membership.member </PATH>"], "scores": [-0.01037608552724123, -0.012874294072389603, -0.02609168365597725], "norm_scores": [0.335355281829834, 0.33451855182647705, 0.33012616634368896]}}
{"id": "WebQTest-848", "question": "where are the nordic countries", "prediction": [["location.location.contains"], ["location.location.containedby"], ["common.topic.notable_types"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the nordic countries [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.008799757808446884, -0.02594689093530178, -0.05323357507586479], "norm_scores": [0.34018951654434204, 0.3344059884548187, 0.32540449500083923]}}
{"id": "WebQTest-849", "question": "what country sponsored ferdinand magellan", "prediction": [["people.person.nationality"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country sponsored ferdinand magellan [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.003615468740463257, -0.057326361536979675, -0.06761500239372253], "norm_scores": [0.34653496742248535, 0.32841330766677856, 0.3250516653060913]}}
{"id": "WebQTest-850", "question": "what did vera ellen die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did vera ellen die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0023767617531120777, -0.037503931671381, -0.1308056116104126], "norm_scores": [0.35149893164634705, 0.3393661379814148, 0.30913496017456055]}}
{"id": "WebQTest-851", "question": "what is the capital of modern day egypt", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of modern day egypt [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.010380088351666927, -0.024681610986590385, -0.03896543011069298], "norm_scores": [0.33810973167419434, 0.3333086669445038, 0.3285815715789795]}}
{"id": "WebQTest-852", "question": "what province is toronto in ( 7 letters )", "prediction": [["location.province.capital"], ["base.biblioness.bibs_location.state"], ["base.biblioness.bibs_location.province"]], "ground_paths": [["location.province.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what province is toronto in ( 7 letters ) [/INST]", "raw_output": {"paths": ["<PATH> location.province.capital </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> base.biblioness.bibs_location.province </PATH>"], "scores": [-0.002837899373844266, -0.04273250699043274, -0.05916019156575203], "norm_scores": [0.3441007733345032, 0.3306432366371155, 0.3252559304237366]}}
{"id": "WebQTest-855", "question": "what is the currency of brazil now", "prediction": [["location.country.currency_used"], ["location.country.currency_formerly_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency of brazil now [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.001287860213778913, -0.05946121737360954, -0.06764034181833267], "norm_scores": [0.3473081588745117, 0.3276804983615875, 0.32501131296157837]}}
{"id": "WebQTest-856", "question": "what types of art did andy warhol do", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what types of art did andy warhol do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.006776940077543259, -0.02027883194386959, -0.05904050171375275], "norm_scores": [0.34063804149627686, 0.33606967329978943, 0.3232923150062561]}}
{"id": "WebQTest-857", "question": "what other states border florida", "prediction": [["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.partially_contains", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"]], "ground_paths": [["location.administrative_division.first_level_division_of", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.contains", "location.location.contains"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_contains", "location.location.partially_containedby"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.administrative_division.first_level_division_of", "location.administrative_division.first_level_division_of"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["location.location.time_zones", "location.location.time_zones"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.tropical_cyclone.affected_areas"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["common.topic.notable_types", "common.topic.notable_types"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["location.location.events", "time.event.locations"], ["location.location.containedby", "location.location.contains"], ["location.location.partially_containedby", "location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other states border florida [/INST]", "raw_output": {"paths": ["<PATH> meteorology.tropical_cyclone.affected_areas <SEP> meteorology.cyclone_affected_area.cyclones </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization.geographic_scope </PATH>"], "scores": [-0.025859253481030464, -0.03672150894999504, -0.03888389468193054], "norm_scores": [0.33599257469177246, 0.33236268162727356, 0.33164477348327637]}}
{"id": "WebQTest-859", "question": "what are the predominant religions in russia", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["language.human_language.main_country", "location.country.languages_spoken"]], "ground_paths": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the predominant religions in russia [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.languages_spoken </PATH>"], "scores": [-0.008360113017261028, -0.01270620059221983, -0.10925754904747009], "norm_scores": [0.34486454725265503, 0.3433690071105957, 0.3117664158344269]}}
{"id": "WebQTest-861", "question": "who will coach carolina panthers in 2013", "prediction": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["sports.sports_team.sport", "sports.sports_team_coach.sports_coached"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"]], "ground_paths": [["american_football.football_coach.current_team_head_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who will coach carolina panthers in 2013 [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sports_team_coach.sports_coached </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>"], "scores": [-0.0041652098298072815, -0.034761592745780945, -0.03580537438392639], "norm_scores": [0.34028396010398865, 0.3300301730632782, 0.32968586683273315]}}
{"id": "WebQTest-862", "question": "where did louis sachar grow up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did louis sachar grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0043586306273937225, -0.028061049059033394, -0.10126832872629166], "norm_scores": [0.3467148542404175, 0.3385935127735138, 0.31469160318374634]}}
{"id": "WebQTest-865", "question": "what to do for fun in houston", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do for fun in houston [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.013481900095939636, -0.018053021281957626, -0.02772691287100315], "norm_scores": [0.33542466163635254, 0.33389487862586975, 0.3306804299354553]}}
{"id": "WebQTest-866", "question": "where did otto frank go to college", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did otto frank go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.014376946724951267, -0.01828792691230774, -0.026731502264738083], "norm_scores": [0.3351410925388336, 0.33383288979530334, 0.33102601766586304]}}
{"id": "WebQTest-867", "question": "what else has jennifer lawrence played in", "prediction": [["people.person.nationality", "film.film.country"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"]], "ground_paths": [["film.actor.film", "film.film.starring"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what else has jennifer lawrence played in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>"], "scores": [-0.02311822585761547, -0.031568724662065506, -0.03244451805949211], "norm_scores": [0.3353114128112793, 0.33248981833457947, 0.33219873905181885]}}
{"id": "WebQTest-868", "question": "who is the governor of pennsylvania state now", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_children", "people.person.place_of_birth"], ["location.location.containedby", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the governor of pennsylvania state now [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.024108435958623886, -0.024748075753450394, -0.02511482499539852], "norm_scores": [0.3335162401199341, 0.3333030045032501, 0.3331807553768158]}}
{"id": "WebQTest-869", "question": "where is ancient phoenician", "prediction": [["location.location.containedby"], ["language.human_language.countries_spoken_in"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is ancient phoenician [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.023125529289245605, -0.02684045396745205, -0.030451709404587746], "norm_scores": [0.3345608711242676, 0.3333203196525574, 0.33211877942085266]}}
{"id": "WebQTest-870", "question": "where does the celtics practice", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_team.location"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the celtics practice [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.location </PATH>"], "scores": [-0.0021386737935245037, -0.039986759424209595, -0.1404867321252823], "norm_scores": [0.3529011309146881, 0.3397941291332245, 0.3073047399520874]}}
{"id": "WebQTest-872", "question": "where bin laden got killed", "prediction": [["people.deceased_person.place_of_death"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where bin laden got killed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.013368090614676476, -0.02781461365520954, -0.04395097494125366], "norm_scores": [0.3383479416370392, 0.3334951400756836, 0.3281569182872772]}}
{"id": "WebQTest-873", "question": "where does toronto get its water from", "prediction": [["location.location.containedby"], ["base.infrastructure.sewage_treatment_plant.serves_location_s"], ["base.infrastructure.sewage_treatment_plant.serves_location_s", "base.infrastructure.sewage_treatment_plant.serves_location_s"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does toronto get its water from [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.infrastructure.sewage_treatment_plant.serves_location_s </PATH>", "<PATH> base.infrastructure.sewage_treatment_plant.serves_location_s <SEP> base.infrastructure.sewage_treatment_plant.serves_location_s </PATH>"], "scores": [-0.01473915483802557, -0.024945873767137527, -0.04060786962509155], "norm_scores": [0.3373468220233917, 0.3339211344718933, 0.3287320137023926]}}
{"id": "WebQTest-874", "question": "what language do people from bosnia speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people from bosnia speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.014573890715837479, -0.02115228958427906, -0.028626998886466026], "norm_scores": [0.33562809228897095, 0.33342742919921875, 0.3309444785118103]}}
{"id": "WebQTest-875", "question": "where did hugo chavez go to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.contains"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did hugo chavez go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.015572881326079369, -0.02073957957327366, -0.028134725987911224], "norm_scores": [0.3353045582771301, 0.33357658982276917, 0.3311188817024231]}}
{"id": "WebQTest-876", "question": "who does japan import from", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.imports_and_exports.exported_to", "location.imports_and_exports.imported_from"], ["book.newspaper.circulation_areas", "book.newspaper.circulation_areas"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"], ["location.country.form_of_government", "government.form_of_government.countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["location.imports_and_exports.imported_from", "location.imports_and_exports.exported_to"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["fictional_universe.fictional_universe.locations", "fictional_universe.fictional_universe.locations"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.statistical_region.places_imported_from", "location.statistical_region.places_exported_to"], ["base.aareas.schema.administrative_area.administrative_parent", "base.locations.countries.planet"], ["organization.organization.founders", "organization.organization.founders"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["location.statistical_region.places_imported_from", "location.imports_and_exports.imported_from"], ["film.film.featured_film_locations", "film.film.country"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.charities.charity.geographic_scope", "organization.organization.geographic_scope"], ["travel.travel_destination.tour_operators", "organization.organization.geographic_scope"], ["location.country.form_of_government", "location.country.form_of_government"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.imports_and_exports.exported_to", "location.statistical_region.places_exported_to"], ["cvg.computer_game_region.versions_released_in_this_region", "cvg.computer_game_region.versions_released_in_this_region"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does japan import from [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.02682008594274521, -0.035913024097681046, -0.03798823431134224], "norm_scores": [0.3355882465839386, 0.332550585269928, 0.3318612277507782]}}
{"id": "WebQTest-877", "question": "what is the name of sonia gandhi daughter", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.gender"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of sonia gandhi daughter [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.008655708283185959, -0.01891334541141987, -0.1101088896393776], "norm_scores": [0.3456239402294159, 0.3420967757701874, 0.3122793436050415]}}
{"id": "WebQTest-878", "question": "who do islamic people pray to", "prediction": [["religion.religion.deities"], ["religion.deity.deity_of"], ["base.argumentmaps.intellectual_dispute.positions"]], "ground_paths": [["religion.deity.deity_of"], ["religion.religion.deities"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who do islamic people pray to [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.deities </PATH>", "<PATH> religion.deity.deity_of </PATH>", "<PATH> base.argumentmaps.intellectual_dispute.positions </PATH>"], "scores": [-0.013125186786055565, -0.013724385760724545, -0.0641789361834526], "norm_scores": [0.33902573585510254, 0.3388226628303528, 0.32215163111686707]}}
{"id": "WebQTest-879", "question": "where was mary todd lincoln from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was mary todd lincoln from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.0036453844513744116, -0.032560110092163086, -0.10524358600378036], "norm_scores": [0.3478391766548157, 0.337925523519516, 0.31423529982566833]}}
{"id": "WebQTest-880", "question": "where did charles darwin die", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.place_of_burial"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did charles darwin die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.0008678709855303168, -0.07067419588565826, -0.07483816146850586], "norm_scores": [0.34949469566345215, 0.32592979073524475, 0.3245754539966583]}}
{"id": "WebQTest-881", "question": "what movies does nick cannon play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["film.actor.film", "film.film.starring"], ["tv.tv_producer.programs_produced", "tv.tv_program.tv_producer"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["music.artist.track", "music.recording.releases"], ["film.actor.film", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies does nick cannon play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.016494926065206528, -0.03524717688560486, -0.03554306924343109], "norm_scores": [0.33754652738571167, 0.33127573132514954, 0.3311777412891388]}}
{"id": "WebQTest-882", "question": "when did the big earthquake hit san francisco", "prediction": [["time.event.locations"], ["location.location.events"], ["event.disaster.areas_affected"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the big earthquake hit san francisco [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> event.disaster.areas_affected </PATH>"], "scores": [-0.0071236854419112206, -0.02906752936542034, -0.04884137958288193], "norm_scores": [0.3404330015182495, 0.3330439329147339, 0.326523095369339]}}
{"id": "WebQTest-883", "question": "who was king george vi married to", "prediction": [["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"], ["people.person.children", "people.person.children"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.family.members", "people.family.members"], ["film.film.subjects", "film.film_subject.films"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["people.person.religion", "people.person.religion"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was king george vi married to [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.03459756076335907, -0.04013904184103012, -0.04064556583762169], "norm_scores": [0.3346222937107086, 0.3327731192111969, 0.33260461688041687]}}
{"id": "WebQTest-884", "question": "where was jesus after he died on the cross", "prediction": [["people.deceased_person.place_of_death"], ["location.location.events"], ["symbols.name_source.namesakes"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was jesus after he died on the cross [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.events </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.008959620259702206, -0.028332656249403954, -0.07029282301664352], "norm_scores": [0.3423106372356415, 0.3357428312301636, 0.32194650173187256]}}
{"id": "WebQTest-885", "question": "who is sir james dyson", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is sir james dyson [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0006776106311008334, -0.06801614910364151, -0.10326490551233292], "norm_scores": [0.3524380624294281, 0.32948681712150574, 0.31807512044906616]}}
{"id": "WebQTest-886", "question": "who voiced darth vader in the original star wars movies", "prediction": [["film.performance.character", "film.performance.actor"], ["film.performance.special_performance_type", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who voiced darth vader in the original star wars movies [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.02221575565636158, -0.02247755229473114, -0.02945028617978096], "norm_scores": [0.33416542410850525, 0.33407795429229736, 0.3317566215991974]}}
{"id": "WebQTest-887", "question": "what high school did maya angelou go to", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what high school did maya angelou go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.016224997118115425, -0.02054748497903347, -0.022249752655625343], "norm_scores": [0.33448395133018494, 0.3330412805080414, 0.33247482776641846]}}
{"id": "WebQTest-888", "question": "what 5 countries border switzerland", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "location.location.partially_contains"]], "ground_paths": [["location.country.official_language", "location.country.official_language"], ["location.location.partially_contains", "geography.river.basin_countries"], ["book.newspaper.circulation_areas", "book.newspaper.circulation_areas"], ["location.country.form_of_government", "government.form_of_government.countries"], ["freebase.valuenotation.has_no_value", "freebase.valuenotation.is_reviewed"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["time.event.locations", "location.location.events"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["location.location.containedby", "base.nations.supranational_body.member"], ["film.film_location.featured_in_films", "film.film.featured_film_locations"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["film.film.country", "film.film.country"], ["geography.lake.basin_countries", "location.location.partially_contains"], ["geography.lake.basin_countries", "geography.lake.basin_countries"], ["fictional_universe.work_of_fiction.setting", "fictional_universe.fictional_setting.works_set_here"], ["geography.lake.basin_countries", "location.location.partially_containedby"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["base.mystery.cryptid_classification.area_of_occurrence", "base.mystery.cryptid_observation_location.cryptid_s_occurring_here"], ["location.location.events", "time.event.locations"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.containedby", "location.location.contains_major_portion_of"], ["book.written_work.subjects", "book.written_work.subjects"], ["location.country.languages_spoken", "language.human_language.main_country"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.contains", "location.location.partially_containedby"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["sports.multi_event_tournament.participating_countries", "sports.sport_country.multi_event_tournaments_participated_in"], ["location.location.partially_contains", "location.location.partially_containedby"], ["location.location.contains", "location.location.contains"], ["sports.multi_event_tournament.participating_countries", "sports.multi_event_tournament.participating_countries"], ["location.location.events", "location.location.events"], ["common.topic.notable_types", "common.topic.notable_types"], ["book.newspaper.circulation_areas", "periodicals.newspaper_circulation_area.newspapers"], ["location.location.containedby", "location.location.containedby"], ["biology.breed_origin.breeds_originating_here", "biology.breed_origin.breeds_originating_here"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["location.country.form_of_government", "location.country.form_of_government"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["base.aareas.schema.administrative_area.subdivides_place"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.contains", "location.location.containedby"], ["time.event.locations", "time.event.locations"], ["location.location.containedby", "base.locations.countries.continent"], ["people.person.nationality", "people.person.nationality"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["fictional_universe.fictional_setting.universe", "fictional_universe.fictional_setting.universe"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["geography.river.origin", "location.location.partially_contains"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what 5 countries border switzerland [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>"], "scores": [-0.0359351821243763, -0.0400366336107254, -0.04679310694336891], "norm_scores": [0.3349962532520294, 0.33362510800361633, 0.3313785791397095]}}
{"id": "WebQTest-889", "question": "what country did benito mussolini govern", "prediction": [["people.person.nationality"], ["government.government_position_held.office_holder", "government.governmental_jurisdiction.governing_officials"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did benito mussolini govern [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.jurisdiction_of_office </PATH>"], "scores": [-0.016135292127728462, -0.027078256011009216, -0.030373014509677887], "norm_scores": [0.33613672852516174, 0.33247846364974976, 0.3313848078250885]}}
{"id": "WebQTest-891", "question": "who was the leader of the first communist party of china", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"], ["organization.organization.founders", "influence.influence_node.influenced"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the leader of the first communist party of china [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization.founders <SEP> influence.influence_node.influenced </PATH>"], "scores": [-0.00386503036133945, -0.029095785692334175, -0.08370354771614075], "norm_scores": [0.34502387046813965, 0.3364275395870209, 0.31854861974716187]}}
{"id": "WebQTest-892", "question": "what did miles davis died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did miles davis died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>"], "scores": [-0.0022037893068045378, -0.03873355686664581, -0.12638868391513824], "norm_scores": [0.3512042760848999, 0.3386063575744629, 0.3101893961429596]}}
{"id": "WebQTest-893", "question": "what is the zip code for azle texas", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for azle texas [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0001301080919802189, -0.09785573929548264, -0.13076505064964294], "norm_scores": [0.35913920402526855, 0.3257024884223938, 0.31515830755233765]}}
{"id": "WebQTest-894", "question": "what date did sally pearson won gold", "prediction": [["people.person.nationality", "olympics.olympic_participating_country.olympics_participated_in"], ["people.person.profession", "common.topic.notable_types"], ["people.person.nationality", "olympics.olympic_games.participating_countries"]], "ground_paths": [["olympics.olympic_athlete_affiliation.athlete", "olympics.olympic_athlete_affiliation.olympics"], ["olympics.olympic_athlete.medals_won", "olympics.olympic_medal_honor.olympics"], ["people.person.nationality", "olympics.olympic_games.participating_countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what date did sally pearson won gold [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> people.person.profession <SEP> common.topic.notable_types </PATH>", "<PATH> people.person.nationality <SEP> olympics.olympic_games.participating_countries </PATH>"], "scores": [-0.020270636305212975, -0.027534112334251404, -0.03309430554509163], "norm_scores": [0.3355681002140045, 0.3331395387649536, 0.33129236102104187]}}
{"id": "WebQTest-895", "question": "when did the detroit pistons last win the championship", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the detroit pistons last win the championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.011059517972171307, -0.011570696718990803, -0.11428353935480118], "norm_scores": [0.3446595370769501, 0.3444834053516388, 0.31085702776908875]}}
{"id": "WebQTest-896", "question": "when did the philadelphia flyers win the cup", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the philadelphia flyers win the cup [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.00912227388471365, -0.014289983548223972, -0.10606351494789124], "norm_scores": [0.3445359468460083, 0.3427600562572479, 0.31270402669906616]}}
{"id": "WebQTest-898", "question": "what county is st paul nc in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st paul nc in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.01018026564270258, -0.028512421995401382, -0.03602936863899231], "norm_scores": [0.3382587134838104, 0.33211418986320496, 0.32962706685066223]}}
{"id": "WebQTest-899", "question": "where is olympic national park wa", "prediction": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "ground_paths": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is olympic national park wa [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>"], "scores": [-0.008877375163137913, -0.030861206352710724, -0.031118197366595268], "norm_scores": [0.33826515078544617, 0.33090993762016296, 0.33082491159439087]}}
{"id": "WebQTest-900", "question": "where did nat turner 1831 rebellion occur", "prediction": [["location.location.events"], ["time.event.locations"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did nat turner 1831 rebellion occur [/INST]", "raw_output": {"paths": ["<PATH> location.location.events </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.012087469920516014, -0.01405533030629158, -0.0582764595746994], "norm_scores": [0.3386506736278534, 0.3379848897457123, 0.3233644664287567]}}
{"id": "WebQTest-901", "question": "who would play luke skywalker", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who would play luke skywalker [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.020760150626301765, -0.021605201065540314, -0.022932875901460648], "norm_scores": [0.33366867899894714, 0.3333868384361267, 0.33294451236724854]}}
{"id": "WebQTest-903", "question": "where turkish people originate", "prediction": [["people.ethnicity.geographic_distribution"], ["people.ethnicity.geographic_distribution", "people.ethnicity.geographic_distribution"], ["people.ethnicity.included_in_group"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where turkish people originate [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> people.ethnicity.geographic_distribution </PATH>", "<PATH> people.ethnicity.included_in_group </PATH>"], "scores": [-7.37060618121177e-05, -0.12619633972644806, -0.13429422676563263], "norm_scores": [0.3628573417663574, 0.31986120343208313, 0.31728148460388184]}}
{"id": "WebQTest-904", "question": "what state is barack obama from", "prediction": [["people.person.place_of_birth", "base.biblioness.bibs_location.state"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_children"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["people.person.place_of_birth", "base.biblioness.bibs_location.state"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_children"], ["base.popstra.celebrity.vacations_in", "base.popstra.location.vacationers"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state is barack obama from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth <SEP> base.biblioness.bibs_location.state </PATH>", "<PATH> people.person.nationality <SEP> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> people.person.nationality <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.017918573692440987, -0.02300383150577545, -0.04471215605735779], "norm_scores": [0.3368716537952423, 0.3351629078388214, 0.32796546816825867]}}
{"id": "WebQTest-905", "question": "what years did joe montana win super bowl", "prediction": [["sports.sports_championship_event.champion"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_championship_event.champion"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what years did joe montana win super bowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_championship_event.champion </PATH>"], "scores": [-0.006029565818607807, -0.029134240001440048, -0.04886747896671295], "norm_scores": [0.34068915247917175, 0.33290788531303406, 0.3264029324054718]}}
{"id": "WebQTest-908", "question": "what awards gary paulsen won", "prediction": [["award.award_honor.award_winner", "award.award_honor.award"], ["award.award_winner.awards_won", "award.award_honor.award"], ["award.award_nomination.award_nominee", "award.award_nomination.award"]], "ground_paths": [["award.award_winner.awards_won", "award.award_category.winners"], ["award.award_nomination.award_nominee", "award.award_nomination.award"], ["award.award_honor.award_winner", "award.award_honor.award"], ["award.award_winner.awards_won", "award.award_honor.award"], ["award.award_nominee.award_nominations", "award.award_nomination.award"], ["award.award_honor.award_winner", "award.award_category.winners"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what awards gary paulsen won [/INST]", "raw_output": {"paths": ["<PATH> award.award_honor.award_winner <SEP> award.award_honor.award </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.award </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award </PATH>"], "scores": [-0.02038559317588806, -0.024532809853553772, -0.024844100698828697], "norm_scores": [0.3342902362346649, 0.33290672302246094, 0.3328031003475189]}}
{"id": "WebQTest-910", "question": "where did alexander graham bell die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did alexander graham bell die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>"], "scores": [-0.00039020832628011703, -0.09035348147153854, -0.09441670030355453], "norm_scores": [0.35407963395118713, 0.32361629605293274, 0.32230404019355774]}}
{"id": "WebQTest-911", "question": "what year did the golden state warriors win their first nba championship", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the golden state warriors win their first nba championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.010641693137586117, -0.011326254345476627, -0.10160157084465027], "norm_scores": [0.343362957239151, 0.3431279957294464, 0.3135090470314026]}}
{"id": "WebQTest-912", "question": "what currency should i take to turkey", "prediction": [["finance.currency.countries_formerly_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should i take to turkey [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.016908761113882065, -0.016980186104774475, -0.04126263037323952], "norm_scores": [0.3360363245010376, 0.3360123038291931, 0.3279514014720917]}}
{"id": "WebQTest-914", "question": "what countries are in the uk yahoo", "prediction": [["base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["base.biblioness.bibs_location.country"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are in the uk yahoo [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.02053810842335224, -0.026032906025648117, -0.02800871804356575], "norm_scores": [0.3347753584384918, 0.33294087648391724, 0.33228370547294617]}}
{"id": "WebQTest-915", "question": "what superbowl did the baltimore ravens win", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what superbowl did the baltimore ravens win [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.011437476612627506, -0.013676987029612064, -0.04616985097527504], "norm_scores": [0.3374243974685669, 0.33666959404945374, 0.32590600848197937]}}
{"id": "WebQTest-916", "question": "what is the language of russia s spoken", "prediction": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the language of russia s spoken [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.019276220351457596, -0.019823329523205757, -0.02031371183693409], "norm_scores": [0.3335094153881073, 0.3333269953727722, 0.3331635892391205]}}
{"id": "WebQTest-918", "question": "who is the king of cambodia now", "prediction": [["people.person.nationality"], ["royalty.monarch.kingdom"], ["royalty.kingdom.rulers"]], "ground_paths": [["royalty.kingdom.rulers"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the king of cambodia now [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> royalty.monarch.kingdom </PATH>", "<PATH> royalty.kingdom.rulers </PATH>"], "scores": [-0.016215473413467407, -0.01686742715537548, -0.0371028296649456], "norm_scores": [0.33571943640708923, 0.3355006277561188, 0.3287798762321472]}}
{"id": "WebQTest-919", "question": "who are serena williams parents", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.profession"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are serena williams parents [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.008726922795176506, -0.02134304866194725, -0.15842153131961823], "norm_scores": [0.3510701060295105, 0.3466687798500061, 0.3022611141204834]}}
{"id": "WebQTest-920", "question": "what schools did sir isaac newton go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what schools did sir isaac newton go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.01074326690286398, -0.022998731583356857, -0.02880481444299221], "norm_scores": [0.3367094099521637, 0.33260807394981384, 0.3306824862957001]}}
{"id": "WebQTest-921", "question": "what was the cause of death for huell howser", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the cause of death for huell howser [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.00109547795727849, -0.04911131411790848, -0.09249842166900635], "norm_scores": [0.34894654154777527, 0.33258748054504395, 0.31846603751182556]}}
{"id": "WebQTest-922", "question": "what books are written by suzanne collins", "prediction": [["book.book_edition.author_editor"], ["book.author.works_written"], ["book.author.book_editions_published"]], "ground_paths": [["book.author.book_editions_published"], ["book.author.works_written"], ["book.book_edition.author_editor"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what books are written by suzanne collins [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.author.book_editions_published </PATH>"], "scores": [-0.014131621457636356, -0.02097201719880104, -0.0391925610601902], "norm_scores": [0.3368780314922333, 0.33458152413368225, 0.32854047417640686]}}
{"id": "WebQTest-923", "question": "what countries does england border", "prediction": [["location.location.events", "location.location.events"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"]], "ground_paths": [["location.country.languages_spoken", "location.country.official_language"], ["finance.currency.countries_used", "location.country.currency_used"], ["time.event.locations", "location.location.events"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.multi_event_tournament.participating_countries"], ["government.government_agency.jurisdiction", "government.governmental_jurisdiction.agencies"], ["sports.sports_team_location.teams", "cricket.cricket_team.country"], ["law.court_jurisdiction_area.courts", "law.court.jurisdiction"], ["biology.breed_origin.breeds_originating_here", "biology.animal_breed.place_of_origin"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["film.film.featured_film_locations", "film.film_location.featured_in_films"], ["biology.animal_breed.place_of_origin", "biology.animal_breed.place_of_origin"], ["base.biblioness.bibs_location.country", "base.aareas.schema.administrative_area.administrative_children"], ["location.location.events", "time.event.locations"], ["location.location.time_zones", "location.location.time_zones"], ["organization.organization.geographic_scope", "common.topic.subject_of"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["law.court_jurisdiction_area.courts", "law.court_jurisdiction_area.courts"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["periodicals.newspaper_circulation_area.newspapers", "periodicals.newspaper_circulation_area.newspapers"], ["cricket.cricket_tournament_event.host", "time.event.locations"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_contains", "location.location.partially_containedby"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["government.governmental_jurisdiction.government_bodies", "government.governmental_jurisdiction.government_bodies"], ["film.film.featured_film_locations", "film.film.featured_film_locations"], ["location.location.events", "location.location.events"], ["location.country.internet_tld", "location.country.internet_tld"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.charities.charity.geographic_scope", "organization.organization.geographic_scope"], ["book.newspaper.circulation_areas", "periodicals.newspaper_circulation_area.newspapers"], ["biology.breed_origin.breeds_originating_here", "biology.breed_origin.breeds_originating_here"], ["government.government_agency.jurisdiction", "government.government_agency.jurisdiction"], ["location.country.form_of_government", "location.country.form_of_government"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["cricket.cricket_tournament_event.host", "location.location.events"], ["time.event.locations", "time.event.locations"], ["people.person.nationality", "people.person.nationality"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["periodicals.newspaper_circulation_area.newspapers", "book.newspaper.circulation_areas"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does england border [/INST]", "raw_output": {"paths": ["<PATH> location.location.events <SEP> location.location.events </PATH>", "<PATH> organization.organization.geographic_scope <SEP> organization.organization_scope.organizations_with_this_scope </PATH>", "<PATH> organization.organization.geographic_scope <SEP> organization.organization.geographic_scope </PATH>"], "scores": [-0.042881958186626434, -0.04499995708465576, -0.04555673897266388], "norm_scores": [0.33386605978012085, 0.3331596851348877, 0.33297425508499146]}}
{"id": "WebQTest-924", "question": "what team did messi play for in 2010", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"]], "ground_paths": [["sports.sports_award.award_winner", "sports.sports_award.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did messi play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>"], "scores": [-0.02485787682235241, -0.024866025894880295, -0.03831389173865318], "norm_scores": [0.3348259925842285, 0.33482325077056885, 0.33035072684288025]}}
{"id": "WebQTest-925", "question": "what time zone is greensburg indiana", "prediction": [["location.location.time_zones"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is greensburg indiana [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-2.2438283849623986e-05, -0.150740385055542, -0.16150455176830292], "norm_scores": [0.36887139081954956, 0.3172627091407776, 0.31386590003967285]}}
{"id": "WebQTest-927", "question": "who plays voldemort", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["cvg.game_performance.character", "cvg.game_performance.voice_actor"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays voldemort [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.02190573140978813, -0.022862888872623444, -0.023753078654408455], "norm_scores": [0.3336449861526489, 0.3333258032798767, 0.33302921056747437]}}
{"id": "WebQTest-928", "question": "what colleges did harper lee attend", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what colleges did harper lee attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.013986920937895775, -0.02357269637286663, -0.02916637621819973], "norm_scores": [0.3360898196697235, 0.3328835368156433, 0.33102667331695557]}}
{"id": "WebQTest-929", "question": "who founded the kkk", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["base.activism.activist_organization.founders"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who founded the kkk [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> base.activism.activist_organization.founders </PATH>"], "scores": [-0.010038980282843113, -0.018074754625558853, -0.0973830297589302], "norm_scores": [0.34383660554885864, 0.34108468890190125, 0.3150787353515625]}}
{"id": "WebQTest-931", "question": "what jobs did john adams have before he was president", "prediction": [["people.person.profession", "base.onephylogeny.type_of_thing.things_of_this_type"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["people.person.profession"]], "ground_paths": [["common.topic.image", "common.topic.image"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["tv.tv_program.country_of_origin", "government.government_office_or_title.jurisdiction"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what jobs did john adams have before he was president [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.022313207387924194, -0.023432532325387, -0.02401025965809822], "norm_scores": [0.33364632725715637, 0.3332730829715729, 0.33308058977127075]}}
{"id": "WebQTest-932", "question": "what year did super mario bros 2 come out", "prediction": [["film.film.sequel"], ["film.film.country"], ["common.topic.notable_types"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did super mario bros 2 come out [/INST]", "raw_output": {"paths": ["<PATH> film.film.sequel </PATH>", "<PATH> film.film.country </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.022801022976636887, -0.03591950237751007, -0.04632355272769928], "norm_scores": [0.33741387724876404, 0.33301642537117004, 0.32956966757774353]}}
{"id": "WebQTest-933", "question": "what political party did andrew johnson belong to", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["government.us_president.vice_president", "organization.organization.founders"]], "ground_paths": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.us_president.vice_president", "organization.organization.founders"], ["government.politician.party", "government.political_party_tenure.party"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party did andrew johnson belong to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.us_president.vice_president <SEP> organization.organization.founders </PATH>"], "scores": [-0.016183922067284584, -0.025599757209420204, -0.02661553956568241], "norm_scores": [0.335542231798172, 0.3323976397514343, 0.33206015825271606]}}
{"id": "WebQTest-935", "question": "where was osama bin laden killed", "prediction": [["base.culturalevent.event.entity_involved", "time.event.locations"], ["people.deceased_person.place_of_death"], ["base.culturalevent.event.entity_involved", "location.location.events"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was osama bin laden killed [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>"], "scores": [-0.016727352514863014, -0.024748366326093674, -0.035979386419057846], "norm_scores": [0.3363669812679291, 0.33367979526519775, 0.3299531936645508]}}
{"id": "WebQTest-936", "question": "where were greek temples built", "prediction": [["location.location.containedby"], ["location.location.contains"], ["architecture.building.building_function"]], "ground_paths": [["architecture.building_function.buildings", "location.location.contains"], ["architecture.building_function.buildings", "location.location.containedby"], ["architecture.building.building_function", "location.location.containedby"], ["common.image.appears_in_topic_gallery", "common.image.appears_in_topic_gallery"], ["architecture.building.building_function", "travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where were greek temples built [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> architecture.building.building_function </PATH>"], "scores": [-0.006359300576150417, -0.04709431156516075, -0.07697563618421555], "norm_scores": [0.34579306840896606, 0.33199021220207214, 0.3222166895866394]}}
{"id": "WebQTest-937", "question": "where are people who speak arabic from", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are people who speak arabic from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.016382409259676933, -0.023462845012545586, -0.03547122702002525], "norm_scores": [0.33624333143234253, 0.3338710069656372, 0.32988572120666504]}}
{"id": "WebQTest-938", "question": "who was queen elizabeth ii mom", "prediction": [["people.person.parents"], ["people.person.children"], ["influence.influence_node.influenced"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was queen elizabeth ii mom [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.005262087564915419, -0.029431026428937912, -0.12746591866016388], "norm_scores": [0.349517285823822, 0.3411710858345032, 0.3093115985393524]}}
{"id": "WebQTest-939", "question": "what type of art did pablo picasso paint", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art did pablo picasso paint [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.0017025984125211835, -0.04048625007271767, -0.0715617835521698], "norm_scores": [0.3454847037792206, 0.3323420584201813, 0.32217317819595337]}}
{"id": "WebQTest-941", "question": "what is ella fitzgerald parents name", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.parent"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is ella fitzgerald parents name [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.010594487190246582, -0.017001105472445488, -0.1331665813922882], "norm_scores": [0.34743261337280273, 0.3452138602733612, 0.30735349655151367]}}
{"id": "WebQTest-942", "question": "who is anne hathaway parents", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.parent"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is anne hathaway parents [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.013959497213363647, -0.01424485445022583, -0.15234971046447754], "norm_scores": [0.348374605178833, 0.34827518463134766, 0.3033502399921417]}}
{"id": "WebQTest-943", "question": "who does don draper gets engaged to", "prediction": [["fictional_universe.fictional_character.character_created_by", "fictional_universe.fictional_character.character_created_by"], ["fictional_universe.fictional_character.married_to", "fictional_universe.marriage_of_fictional_characters.spouses"], ["fictional_universe.fictional_character.married_to", "fictional_universe.fictional_character.married_to"]], "ground_paths": [["fictional_universe.fictional_character.married_to", "fictional_universe.fictional_character.married_to"], ["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character_creator.fictional_characters_created", "fictional_universe.fictional_character.character_created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does don draper gets engaged to [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.character_created_by <SEP> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> fictional_universe.fictional_character.married_to <SEP> fictional_universe.marriage_of_fictional_characters.spouses </PATH>", "<PATH> fictional_universe.fictional_character.married_to <SEP> fictional_universe.fictional_character.married_to </PATH>"], "scores": [-0.02245405502617359, -0.025838633999228477, -0.033734895288944244], "norm_scores": [0.3349630534648895, 0.3338312804698944, 0.3312056362628937]}}
{"id": "WebQTest-944", "question": "what was van halen first album", "prediction": [["music.artist.album"], ["music.album.artist"], ["music.recording.artist"]], "ground_paths": [["music.album.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was van halen first album [/INST]", "raw_output": {"paths": ["<PATH> music.artist.album </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.021664287894964218, -0.02510661818087101, -0.03095170110464096], "norm_scores": [0.3347482979297638, 0.33359795808792114, 0.33165374398231506]}}
{"id": "WebQTest-945", "question": "what political system did spain have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political system did spain have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.0043006716296076775, -0.0269753560423851, -0.14473626017570496], "norm_scores": [0.3513012230396271, 0.3434251844882965, 0.3052736222743988]}}
{"id": "WebQTest-948", "question": "what are the supreme court cases", "prediction": [["law.court.jurisdiction", "government.governmental_jurisdiction.government_bodies"], ["law.court.jurisdiction", "book.written_work.subjects"], ["law.court.jurisdiction", "government.governmental_body.jurisdiction"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the supreme court cases [/INST]", "raw_output": {"paths": ["<PATH> law.court.jurisdiction <SEP> government.governmental_jurisdiction.government_bodies </PATH>", "<PATH> law.court.jurisdiction <SEP> book.written_work.subjects </PATH>", "<PATH> law.court.jurisdiction <SEP> government.governmental_body.jurisdiction </PATH>"], "scores": [-0.021633267402648926, -0.03313471004366875, -0.03896539285778999], "norm_scores": [0.3365437686443329, 0.33269521594047546, 0.33076101541519165]}}
{"id": "WebQTest-949", "question": "what is penn state 's main campus", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"]], "ground_paths": [["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is penn state 's main campus [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>"], "scores": [-0.003168313065543771, -0.04838534817099571, -0.051589176058769226], "norm_scores": [0.3438171446323395, 0.32861700654029846, 0.32756584882736206]}}
{"id": "WebQTest-952", "question": "what country does alaska belong to", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.administrative_division.country"]], "ground_paths": [["base.biblioness.bibs_location.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country does alaska belong to [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-0.015997232869267464, -0.03354804217815399, -0.038562171161174774], "norm_scores": [0.33780473470687866, 0.33192771673202515, 0.3302675485610962]}}
{"id": "WebQTest-953", "question": "where is kansas city", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.biblioness.bibs_location.state"]], "ground_paths": [["location.hud_county_place.county"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is kansas city [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>"], "scores": [-0.005985681898891926, -0.05011771619319916, -0.05596241354942322], "norm_scores": [0.3438695967197418, 0.32902392745018005, 0.3271064758300781]}}
{"id": "WebQTest-954", "question": "what was pink floyd 's first album", "prediction": [["music.recording.artist"], ["music.artist.album"], ["music.artist.genre", "music.album.genre"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was pink floyd 's first album [/INST]", "raw_output": {"paths": ["<PATH> music.recording.artist </PATH>", "<PATH> music.artist.album </PATH>", "<PATH> music.artist.genre <SEP> music.album.genre </PATH>"], "scores": [-0.027460845187306404, -0.03504659980535507, -0.042426932603120804], "norm_scores": [0.3358422815799713, 0.3333042860031128, 0.3308534622192383]}}
{"id": "WebQTest-955", "question": "who did granville t. woods married", "prediction": [["people.person.children", "people.person.children"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.spouse_s", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did granville t. woods married [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.03135383874177933, -0.03728923946619034, -0.038534704595804214], "norm_scores": [0.33479225635528564, 0.33281099796295166, 0.3323967456817627]}}
{"id": "WebQTest-957", "question": "what language did the sioux indians speak", "prediction": [["people.ethnicity.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language did the sioux indians speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.0007325468468479812, -0.07987727224826813, -0.07999606430530548], "norm_scores": [0.35116028785705566, 0.3244391679763794, 0.3244006335735321]}}
{"id": "WebQTest-958", "question": "what are some famous people from el salvador", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some famous people from el salvador [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.002743810648098588, -0.04361482337117195, -0.07511885464191437], "norm_scores": [0.3460046350955963, 0.33214813470840454, 0.32184723019599915]}}
{"id": "WebQTest-959", "question": "who did brett gardner play for in 2008", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did brett gardner play for in 2008 [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.009802253916859627, -0.016339661553502083, -0.03164302185177803], "norm_scores": [0.336487352848053, 0.33429476618766785, 0.3292178809642792]}}
{"id": "WebQTest-960", "question": "who plays dwight in the office", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_series_season.episodes", "tv.tv_director.episodes_directed"], ["tv.tv_series_season.regular_cast", "tv.regular_tv_appearance.actor"], ["award.award_winning_work.awards_won", "award.award_honor.award_winner"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays dwight in the office [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.015656448900699615, -0.037244100123643875, -0.04020557180047035], "norm_scores": [0.3384789526462555, 0.33125030994415283, 0.33027076721191406]}}
{"id": "WebQTest-962", "question": "what county is san diego ca", "prediction": [["location.us_county.hud_county_place"], ["location.hud_county_place.county"], ["location.location.containedby"]], "ground_paths": [["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is san diego ca [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01625598594546318, -0.024174222722649574, -0.030368713662028313], "norm_scores": [0.3357846140861511, 0.33313629031181335, 0.33107906579971313]}}
{"id": "WebQTest-963", "question": "who is the miami dolphins quarterback", "prediction": [["american_football.football_player.position_s"], ["american_football.game_rushing_statistics.team", "american_football.game_rushing_statistics.player"], ["american_football.football_team.current_head_coach"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the miami dolphins quarterback [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.position_s </PATH>", "<PATH> american_football.game_rushing_statistics.team <SEP> american_football.game_rushing_statistics.player </PATH>", "<PATH> american_football.football_team.current_head_coach </PATH>"], "scores": [-0.007810473442077637, -0.03703416511416435, -0.05077124014496803], "norm_scores": [0.34139618277549744, 0.33156371116638184, 0.3270401358604431]}}
{"id": "WebQTest-965", "question": "where did anne frank go to school", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["book.written_work.subjects", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"], ["book.book_subject.works", "book.written_work.subjects"], ["book.book_subject.works", "book.book_subject.works"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did anne frank go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>"], "scores": [-0.021142376586794853, -0.02390214428305626, -0.02881278097629547], "norm_scores": [0.334492564201355, 0.3335707187652588, 0.33193668723106384]}}
{"id": "WebQTest-966", "question": "what basketball player is married to monica", "prediction": [["people.person.nationality", "people.person.nationality"], ["sports.sports_team_coach.sports_coached", "people.person.nationality", "people.person.nationality"], ["sports.sports_team_coach.sports_coached", "people.person.gender", "people.person.gender"]], "ground_paths": [["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.marriage.spouse", "people.person.spouse_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what basketball player is married to monica [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.033032502979040146, -0.040520474314689636, -0.04445304721593857], "norm_scores": [0.335437148809433, 0.33293476700782776, 0.33162805438041687]}}
{"id": "WebQTest-967", "question": "what did hitler use to kill himself", "prediction": [["people.deceased_person.cause_of_death"], ["base.activism.activist.area_of_activism", "base.activism.activist.area_of_activism"], ["base.activism.activist.area_of_activism", "base.activism.activism_issue.activists"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did hitler use to kill himself [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> base.activism.activist.area_of_activism </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> base.activism.activism_issue.activists </PATH>"], "scores": [-0.008510435931384563, -0.02973361872136593, -0.04675373062491417], "norm_scores": [0.33996501564979553, 0.3328258693218231, 0.32720911502838135]}}
{"id": "WebQTest-968", "question": "what do people speak in germany", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people speak in germany [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.013314367271959782, -0.019774112850427628, -0.036681029945611954], "norm_scores": [0.33664757013320923, 0.3344799280166626, 0.3288724720478058]}}
{"id": "WebQTest-969", "question": "when is the last time the denver broncos won the superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is the last time the denver broncos won the superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.0155965406447649, -0.015713246539235115, -0.02387189492583275], "norm_scores": [0.3342646062374115, 0.334225594997406, 0.33150985836982727]}}
{"id": "WebQTest-970", "question": "where was caesar when he was stabbed", "prediction": [["people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was caesar when he was stabbed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.0016261592973023653, -0.06864918768405914, -0.06993906944990158], "norm_scores": [0.34853628277778625, 0.32594192028045654, 0.3255217969417572]}}
{"id": "WebQTest-971", "question": "what style of music did bessie smith perform", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.written_work.author"]], "ground_paths": [["music.artist.genre"], ["book.book_subject.works"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what style of music did bessie smith perform [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.0005592282395809889, -0.07234948873519897, -0.10590803623199463], "norm_scores": [0.3532649278640747, 0.32879287004470825, 0.31794217228889465]}}
{"id": "WebQTest-972", "question": "what team did jackie robinson", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did jackie robinson [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.01487604808062315, -0.01731644570827484, -0.028934601694345474], "norm_scores": [0.3351652920246124, 0.3343483805656433, 0.3304862976074219]}}
{"id": "WebQTest-973", "question": "what time zone is chicago in right now", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is chicago in right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-8.81631513038883e-06, -0.1644764393568039, -0.18512165546417236], "norm_scores": [0.37322402000427246, 0.3166228234767914, 0.3101530969142914]}}
{"id": "WebQTest-975", "question": "who was the vice president under ronald reagan", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the vice president under ronald reagan [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.006383985746651888, -0.020068800076842308, -0.08055845648050308], "norm_scores": [0.3430628180503845, 0.3384000360965729, 0.3185371458530426]}}
{"id": "WebQTest-976", "question": "where do the chicago white sox play home games", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the chicago white sox play home games [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.0005609007785096765, -0.0596381314098835, -0.10913273692131042], "norm_scores": [0.35214388370513916, 0.3319427967071533, 0.3159133493900299]}}
{"id": "WebQTest-977", "question": "what part did jeff conaway play in grease", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["theater.theater_character.plays_appears_in"], ["film.performance.actor", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part did jeff conaway play in grease [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.0265129953622818, -0.031896088272333145, -0.0353773795068264], "norm_scores": [0.33491793274879456, 0.3331198990345001, 0.3319621980190277]}}
{"id": "WebQTest-978", "question": "what book did benjamin franklin published", "prediction": [["book.written_work.author"], ["book.book_edition.author_editor"], ["book.author.book_editions_published"]], "ground_paths": [["book.book_edition.author_editor"], ["book.book_edition.author_editor", "book.book.genre"], ["book.written_work.author"], ["book.author.book_editions_published"], ["people.person.profession", "common.topic.subject_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what book did benjamin franklin published [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.book_editions_published </PATH>"], "scores": [-0.011775774881243706, -0.029132867231965065, -0.033336494117975235], "norm_scores": [0.3376709818840027, 0.33186057209968567, 0.33046847581863403]}}
{"id": "WebQTest-979", "question": "what currency to take to barbados", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency to take to barbados [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.010767138563096523, -0.0158139206469059, -0.12530651688575745], "norm_scores": [0.34641116857528687, 0.34466734528541565, 0.3089214861392975]}}
{"id": "WebQTest-980", "question": "where did they get the name stanley cup", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["sports.sports_championship_event.championship"]], "ground_paths": [["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did they get the name stanley cup [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.013242428191006184, -0.022817952558398247, -0.05775802582502365], "norm_scores": [0.339336097240448, 0.33610227704048157, 0.32456162571907043]}}
{"id": "WebQTest-982", "question": "what is the zip code of syracuse ny", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code of syracuse ny [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0002699409087654203, -0.08016487210988998, -0.13613098859786987], "norm_scores": [0.3576310873031616, 0.33016982674598694, 0.31219905614852905]}}
{"id": "WebQTest-983", "question": "where did tennessee williams go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did tennessee williams go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.013675836846232414, -0.020916655659675598, -0.021439209580421448], "norm_scores": [0.3350025415420532, 0.3325856029987335, 0.33241185545921326]}}
{"id": "WebQTest-984", "question": "what is the name of airport in new york", "prediction": [["aviation.airport.serves"], ["location.location.nearby_airports"], ["location.location.containedby"]], "ground_paths": [["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of airport in new york [/INST]", "raw_output": {"paths": ["<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.nearby_airports </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.009646325372159481, -0.026685068383812904, -0.03177882358431816], "norm_scores": [0.3376990854740143, 0.33199384808540344, 0.3303070664405823]}}
{"id": "WebQTest-985", "question": "who was queen victoria 's husband", "prediction": [["people.marriage.spouse", "people.marriage.spouse"], ["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.parents", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.parents"], ["royalty.kingdom.rulers", "people.person.nationality"], ["people.person.children", "people.person.children"], ["base.crime.crime_victim.crime", "base.crime.crime_victim.crime"], ["people.person.spouse_s", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was queen victoria 's husband [/INST]", "raw_output": {"paths": ["<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.03673310577869415, -0.03856692090630531, -0.040613893419504166], "norm_scores": [0.33396846055984497, 0.3333566188812256, 0.33267492055892944]}}
{"id": "WebQTest-986", "question": "where did benjamin franklin work", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.employment_history", "business.employer.employees"], ["people.person.profession"]], "ground_paths": [["people.person.nationality", "government.government_agency.jurisdiction"], ["people.person.employment_history", "business.employer.employees"], ["people.person.children"], ["business.employment_tenure.person", "business.employment_tenure.company"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did benjamin franklin work [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.employment_history <SEP> business.employer.employees </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.016969410702586174, -0.02790234237909317, -0.04335997253656387], "norm_scores": [0.33748650550842285, 0.33381688594818115, 0.3286965489387512]}}
{"id": "WebQTest-987", "question": "what does barbara bush do for work", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.person.children"]], "ground_paths": [["government.politician.government_positions_held", "government.government_office_category.officeholders"], ["business.employment_tenure.person", "business.job_title.people_with_this_title"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does barbara bush do for work [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.0020102455746382475, -0.0668044462800026, -0.0757603645324707], "norm_scores": [0.34889835119247437, 0.32700860500335693, 0.3240930140018463]}}
{"id": "WebQTest-988", "question": "who is jacob latimore", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["common.topic.notable_types", "common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is jacob latimore [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>"], "scores": [-0.0003944679629057646, -0.07738954573869705, -0.14426858723163605], "norm_scores": [0.35818013548851013, 0.3316369950771332, 0.3101828396320343]}}
{"id": "WebQTest-989", "question": "what happened to dunkirk during ww2", "prediction": [["time.event.locations"], ["military.military_conflict.casualties", "military.casualties.event"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what happened to dunkirk during ww2 [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.casualties.event </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.0304443147033453, -0.032195258885622025, -0.03861089050769806], "norm_scores": [0.3344350755214691, 0.3338499963283539, 0.3317149877548218]}}
{"id": "WebQTest-990", "question": "what was scottie pippen good at", "prediction": [["basketball.basketball_player.position_s"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.player"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.player"]], "ground_paths": [["sports.pro_sports_played.athlete", "sports.pro_sports_played.sport"], ["common.topic.notable_types", "type.type.domain"], ["basketball.basketball_player.position_s", "sports.sport.positions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was scottie pippen good at [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.position_s </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.player </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.player </PATH>"], "scores": [-0.016714932397007942, -0.029450025409460068, -0.032213255763053894], "norm_scores": [0.33647751808166504, 0.33221960067749023, 0.3313028812408447]}}
{"id": "WebQTest-991", "question": "who played donna on west wing", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played donna on west wing [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.02210412546992302, -0.035583071410655975, -0.04302699491381645], "norm_scores": [0.33716508746147156, 0.33265095949172974, 0.33018389344215393]}}
{"id": "WebQTest-993", "question": "who played meg in season 1 of family guy", "prediction": [["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played meg in season 1 of family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.022504067048430443, -0.030452910810709, -0.036724694073200226], "norm_scores": [0.33580005168914795, 0.33314138650894165, 0.3310585618019104]}}
{"id": "WebQTest-994", "question": "when was the last time the dallas cowboys won the superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the dallas cowboys won the superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.006095587741583586, -0.02554319240152836, -0.038449726998806], "norm_scores": [0.33910903334617615, 0.3325778841972351, 0.32831305265426636]}}
{"id": "WebQTest-996", "question": "where do ireland play rugby union", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_team_location.teams"], ["sports.sports_facility.teams"]], "ground_paths": [["sports.sports_facility.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do ireland play rugby union [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_facility.teams </PATH>"], "scores": [-0.0051109157502651215, -0.04305668920278549, -0.048010967671871185], "norm_scores": [0.3423751890659332, 0.3296269178390503, 0.3279978930950165]}}
{"id": "WebQTest-997", "question": "who killed lee harvey oswald shot", "prediction": [["people.deceased_person.cause_of_death"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.person.gender", "people.person.gender"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who killed lee harvey oswald shot [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.010553436353802681, -0.02651212178170681, -0.04384530708193779], "norm_scores": [0.3388194739818573, 0.33345526456832886, 0.32772526144981384]}}
{"id": "WebQTest-998", "question": "when was the last time knicks won the championship", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time knicks won the championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.008723948150873184, -0.015261093154549599, -0.1143125519156456], "norm_scores": [0.3456285893917084, 0.3433765470981598, 0.31099480390548706]}}
{"id": "WebQTest-999", "question": "when did celtics win the championship", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did celtics win the championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.011782110668718815, -0.012404791079461575, -0.14908431470394135], "norm_scores": [0.3483004868030548, 0.3480836749076843, 0.30361589789390564]}}
{"id": "WebQTest-1000", "question": "what countries included in oceania", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries included in oceania [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.006375327240675688, -0.03788711875677109, -0.05642305314540863], "norm_scores": [0.34244656562805176, 0.3318237066268921, 0.32572969794273376]}}
{"id": "WebQTest-1001", "question": "what country is nicki minaj from", "prediction": [["people.person.nationality"], ["music.artist.origin"], ["music.recording.artist"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country is nicki minaj from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.0062987226992845535, -0.03985000401735306, -0.058538831770420074], "norm_scores": [0.34292304515838623, 0.331608384847641, 0.3254685699939728]}}
{"id": "WebQTest-1002", "question": "who played harry potter dumbledore actor", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played harry potter dumbledore actor [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.016383379697799683, -0.02386850118637085, -0.024066220968961716], "norm_scores": [0.3350208103656769, 0.3325224816799164, 0.3324567377567291]}}
{"id": "WebQTest-1003", "question": "who was the prime minister of pakistan in 1999", "prediction": [["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the prime minister of pakistan in 1999 [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.009909496642649174, -0.016179271042346954, -0.020752139389514923], "norm_scores": [0.3352368474006653, 0.333141565322876, 0.33162161707878113]}}
{"id": "WebQTest-1004", "question": "what movies did christopher plummer play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.performance.actor"], ["music.artist.album", "film.film.soundtrack"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["music.recording.artist", "music.recording.releases"], ["film.producer.films_executive_produced"], ["film.actor.dubbing_performances", "film.film.dubbing_performances"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["tv.tv_guest_role.actor", "tv.tv_guest_role.episodes_appeared_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did christopher plummer play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.015728658065199852, -0.03394849970936775, -0.03601094335317612], "norm_scores": [0.33762484788894653, 0.33152908086776733, 0.33084601163864136]}}
{"id": "WebQTest-1005", "question": "what is capital city of morocco", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is capital city of morocco [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.006227401085197926, -0.03426748141646385, -0.042417559772729874], "norm_scores": [0.3405059576034546, 0.3310907483100891, 0.3284033238887787]}}
{"id": "WebQTest-1006", "question": "what movies is omar epps in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies is omar epps in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.014521900564432144, -0.030451567843556404, -0.036655060946941376], "norm_scores": [0.3375747799873352, 0.33223995566368103, 0.33018529415130615]}}
{"id": "WebQTest-1007", "question": "what district does nancy pelosi represent", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.district_represented"], ["government.politician.government_positions_held", "government.government_position_held.district_represented"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["government.government_position_held.office_holder", "government.government_position_held.district_represented"], ["government.politician.government_positions_held", "government.political_district.representatives"], ["base.government2.elected_government_positions_held.officeholder", "base.government2.elected_government_positions_held.district_represented"], ["government.politician.government_positions_held", "government.government_position_held.district_represented"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what district does nancy pelosi represent [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.district_represented </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.district_represented </PATH>", "<PATH> people.person.nationality <SEP> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.022156592458486557, -0.025159642100334167, -0.034707095474004745], "norm_scores": [0.3350612223148346, 0.3340565264225006, 0.33088231086730957]}}
{"id": "WebQTest-1008", "question": "where is walton indiana", "prediction": [["location.location.containedby"], ["location.hud_county_place.county"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"], ["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is walton indiana [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0055704498663544655, -0.04602605104446411, -0.05072413757443428], "norm_scores": [0.3429117202758789, 0.3293159008026123, 0.3277723491191864]}}
{"id": "WebQTest-1009", "question": "where did george herbert walker bush go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["base.yalebase.person.residential_college", "education.educational_institution.parent_institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did george herbert walker bush go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.015711650252342224, -0.017577635124325752, -0.019053466618061066], "norm_scores": [0.3339121639728546, 0.33328965306282043, 0.33279815316200256]}}
{"id": "WebQTest-1010", "question": "what did bruce jenner win gold medal for", "prediction": [["olympics.olympic_athlete.medals_won", "olympics.olympic_event_competition.medalists"], ["olympics.olympic_athlete.medals_won", "olympics.olympic_medal_honor.event"], ["common.topic.image", "common.image.appears_in_topic_gallery"]], "ground_paths": [["olympics.olympic_athlete.medals_won", "olympics.olympic_event_competition.medalists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did bruce jenner win gold medal for [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_event_competition.medalists </PATH>", "<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_medal_honor.event </PATH>", "<PATH> common.topic.image <SEP> common.image.appears_in_topic_gallery </PATH>"], "scores": [-0.007299622055143118, -0.019855758175253868, -0.044460199773311615], "norm_scores": [0.3388631343841553, 0.334634929895401, 0.32650190591812134]}}
{"id": "WebQTest-1011", "question": "where do they speak tibetan", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do they speak tibetan [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.013259966857731342, -0.033777616918087006, -0.03578020632266998], "norm_scores": [0.33813226222991943, 0.33126524090766907, 0.3306025266647339]}}
{"id": "WebQTest-1012", "question": "what to do in hollywood ca this weekend", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do in hollywood ca this weekend [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.015126186423003674, -0.015382306650280952, -0.027964964509010315], "norm_scores": [0.334785521030426, 0.3346997797489166, 0.3305147588253021]}}
{"id": "WebQTest-1014", "question": "where did kim kardashian come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kim kardashian come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.003998644649982452, -0.039019010961055756, -0.045311592519283295], "norm_scores": [0.34186694025993347, 0.3301018476486206, 0.32803118228912354]}}
{"id": "WebQTest-1015", "question": "what town did justin bieber grew up in", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["music.artist.origin"]], "ground_paths": [["music.artist.origin"], ["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what town did justin bieber grew up in [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.013284786604344845, -0.025985609740018845, -0.033218640834093094], "norm_scores": [0.3369677662849426, 0.3327150344848633, 0.3303171694278717]}}
{"id": "WebQTest-1017", "question": "who plays charlie in the santa clause movies", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film.country", "people.person.nationality"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays charlie in the santa clause movies [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>"], "scores": [-0.02732892334461212, -0.02773885242640972, -0.028537923470139503], "norm_scores": [0.3335132300853729, 0.33337655663490295, 0.3331102728843689]}}
{"id": "WebQTest-1020", "question": "what highschool did r. kelly attend", "prediction": [["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what highschool did r. kelly attend [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.01690000668168068, -0.018650688230991364, -0.019956152886152267], "norm_scores": [0.3338675796985626, 0.33328360319137573, 0.33284878730773926]}}
{"id": "WebQTest-1021", "question": "what happened at benghazi", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["time.event.locations"], ["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what happened at benghazi [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.005041828379034996, -0.04428178817033768, -0.05108160153031349], "norm_scores": [0.3428739309310913, 0.3296801447868347, 0.32744595408439636]}}
{"id": "WebQTest-1022", "question": "what country is nike based in", "prediction": [["organization.organization.headquarters", "location.mailing_address.country"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.country"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"], ["organization.organization.phone_number", "common.phone_number.service_location"], ["film.film.production_companies", "film.film.country"], ["organization.organization.founders", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country is nike based in [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> base.schemastaging.organization_extra.phone_number <SEP> base.schemastaging.phone_sandbox.service_location </PATH>"], "scores": [-0.004906641319394112, -0.03961717337369919, -0.04432924836874008], "norm_scores": [0.34161996841430664, 0.32996559143066406, 0.3284144103527069]}}
{"id": "WebQTest-1024", "question": "what kind of money do i bring to mexico", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money do i bring to mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.00719752861186862, -0.02639876864850521, -0.06093565747141838], "norm_scores": [0.34145283699035645, 0.3349590599536896, 0.323588103055954]}}
{"id": "WebQTest-1025", "question": "who did steve spurrier play pro football for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["common.topic.notable_types"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did steve spurrier play pro football for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.008251696825027466, -0.022169023752212524, -0.03181617334485054], "norm_scores": [0.33750829100608826, 0.3328436017036438, 0.32964807748794556]}}
{"id": "WebQTest-1027", "question": "where do the blackhawks play", "prediction": [["sports.sports_facility.teams"], ["sports.sports_team.arena_stadium"], ["sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the blackhawks play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.0025165064726024866, -0.03588884696364403, -0.10126299411058426], "norm_scores": [0.34804993867874146, 0.3366263806819916, 0.3153236210346222]}}
{"id": "WebQTest-1028", "question": "who plays john connor", "prediction": [["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays john connor [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.02159174717962742, -0.02186918444931507, -0.02683090791106224], "norm_scores": [0.3339458703994751, 0.33385324478149414, 0.3322008550167084]}}
{"id": "WebQTest-1029", "question": "what is the main language spoken in mexico", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the main language spoken in mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.017515704035758972, -0.025942599400877953, -0.028302613645792007], "norm_scores": [0.3354714512825012, 0.3326563537120819, 0.33187219500541687]}}
{"id": "WebQTest-1030", "question": "what are italians classified as", "prediction": [["people.ethnicity.included_in_group"], ["people.ethnicity.includes_groups"], ["people.person.ethnicity"]], "ground_paths": [["people.ethnicity.includes_groups"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are italians classified as [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.included_in_group </PATH>", "<PATH> people.ethnicity.includes_groups </PATH>", "<PATH> people.person.ethnicity </PATH>"], "scores": [-0.009834224358201027, -0.02082841470837593, -0.0640178918838501], "norm_scores": [0.3405618667602539, 0.3368381857872009, 0.32259997725486755]}}
{"id": "WebQTest-1031", "question": "what has angelina jolie accomplished", "prediction": [["people.person.profession"], ["award.award_nominee.award_nominations", "award.award_nomination.award"], ["award.award_nomination.award_nominee", "award.award_nomination.award"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has angelina jolie accomplished [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.award </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award </PATH>"], "scores": [-0.014677201397716999, -0.028466003015637398, -0.03082871064543724], "norm_scores": [0.33666813373565674, 0.33205774426460266, 0.3312741219997406]}}
{"id": "WebQTest-1032", "question": "who played captain kirk in star trek movie", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.quotations", "media_common.quotation.author"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played captain kirk in star trek movie [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.024187399074435234, -0.02662322297692299, -0.027472587302327156], "norm_scores": [0.33396926522254944, 0.3331567645072937, 0.3328739404678345]}}
{"id": "WebQTest-1033", "question": "who played dana scully in x files", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"]], "ground_paths": [["fictional_universe.fictional_character.quotations", "people.person.quotations"], ["tv.tv_guest_role.character", "tv.tv_actor.guest_roles"], ["film.performance.character", "film.actor.film"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["film.performance.character", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["tv.tv_guest_role.character", "tv.tv_guest_role.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["people.person.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played dana scully in x files [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.02423475868999958, -0.030716633424162865, -0.031946826726198196], "norm_scores": [0.3349122405052185, 0.3327484130859375, 0.3323393166065216]}}
{"id": "WebQTest-1035", "question": "what was f. scott fitzgerald", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was f. scott fitzgerald [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0005446186405606568, -0.06899998337030411, -0.09711010754108429], "norm_scores": [0.3518914580345154, 0.32860860228538513, 0.31949999928474426]}}
{"id": "WebQTest-1036", "question": "what is the dominant language spoken in jamaica", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the dominant language spoken in jamaica [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.015688739717006683, -0.021771488711237907, -0.023055246099829674], "norm_scores": [0.33482933044433594, 0.3327988386154175, 0.33237189054489136]}}
{"id": "WebQTest-1037", "question": "who is the coach for the new york knicks", "prediction": [["basketball.basketball_team.head_coach"], ["basketball.basketball_coach.team"], ["sports.sports_team.coaches", "sports.sports_team_coach_tenure.coach"]], "ground_paths": [["basketball.basketball_team.head_coach"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the coach for the new york knicks [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_team.head_coach </PATH>", "<PATH> basketball.basketball_coach.team </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach_tenure.coach </PATH>"], "scores": [-0.004336223006248474, -0.02928394265472889, -0.06523691862821579], "norm_scores": [0.34290289878845215, 0.33445408940315247, 0.32264307141304016]}}
{"id": "WebQTest-1038", "question": "who was wesley snipes", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was wesley snipes [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.00037813116796314716, -0.08439353853464127, -0.09958194941282272], "norm_scores": [0.35398539900779724, 0.3254602253437042, 0.32055434584617615]}}
{"id": "WebQTest-1039", "question": "where does sherrod brown live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"], ["people.person.nationality", "base.biblioness.bibs_location.country"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does sherrod brown live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.01511408668011427, -0.017972473055124283, -0.03633634373545647], "norm_scores": [0.3360048830509186, 0.33504584431648254, 0.3289492428302765]}}
{"id": "WebQTest-1040", "question": "what kind of language does colombia speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language does colombia speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.006311429664492607, -0.02263510972261429, -0.07286109030246735], "norm_scores": [0.3425331711769104, 0.336987167596817, 0.3204796612262726]}}
{"id": "WebQTest-1041", "question": "what currency does england use now", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"], ["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does england use now [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.0031679633539170027, -0.04373795911669731, -0.060406025499105453], "norm_scores": [0.3442801535129547, 0.33059224486351013, 0.32512760162353516]}}
{"id": "WebQTest-1042", "question": "who was the original voice of meg griffin on family guy", "prediction": [["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the original voice of meg griffin on family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.014393895864486694, -0.028697779402136803, -0.03280949220061302], "norm_scores": [0.3369782269001007, 0.33219242095947266, 0.33082935214042664]}}
{"id": "WebQTest-1043", "question": "what kind of government is mali", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.continent"]], "ground_paths": [["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government is mali [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.008109495975077152, -0.01792818494141102, -0.14037860929965973], "norm_scores": [0.3488776385784149, 0.3454688787460327, 0.30565351247787476]}}
{"id": "WebQTest-1044", "question": "who did cristiano ronaldo play for in 2010", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"]], "ground_paths": [["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.transfers", "soccer.football_player_transfer.purchasing_team"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did cristiano ronaldo play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>"], "scores": [-0.01961112767457962, -0.0222446508705616, -0.025640780106186867], "norm_scores": [0.3342962861061096, 0.33341705799102783, 0.33228665590286255]}}
{"id": "WebQTest-1045", "question": "what is the zip code for morgantown wv", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for morgantown wv [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-6.231096631381661e-05, -0.1060568317770958, -0.15338775515556335], "norm_scores": [0.3626762330532074, 0.32620176672935486, 0.31112200021743774]}}
{"id": "WebQTest-1046", "question": "what is jacksonville fl timezone", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is jacksonville fl timezone [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-9.556404620525427e-06, -0.19028900563716888, -0.19442088901996613], "norm_scores": [0.3773517608642578, 0.31196728348731995, 0.31068092584609985]}}
{"id": "WebQTest-1047", "question": "who does amy stiller play in dodgeball", "prediction": [["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"]], "ground_paths": [["film.actor.film", "film.performance.character"], ["film.film.starring", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does amy stiller play in dodgeball [/INST]", "raw_output": {"paths": ["<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.022948212921619415, -0.024732070043683052, -0.0266512930393219], "norm_scores": [0.3339431583881378, 0.33334797620773315, 0.33270880579948425]}}
{"id": "WebQTest-1048", "question": "who were king henry vii children", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.parent"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were king henry vii children [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.012335250154137611, -0.015394588932394981, -0.1410464495420456], "norm_scores": [0.34768420457839966, 0.34662213921546936, 0.305693656206131]}}
{"id": "WebQTest-1050", "question": "who does mila kunis play on family guy", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["tv.regular_tv_appearance.series", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"]], "ground_paths": [["tv.tv_program.regular_cast", "tv.regular_tv_appearance.character"], ["tv.tv_program_creator.programs_created", "fictional_universe.fictional_character.character_created_by"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does mila kunis play on family guy [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes <SEP> fictional_universe.fictional_character.appears_in_these_fictional_universes </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.character </PATH>"], "scores": [-0.020182831212878227, -0.022966722026467323, -0.023616349324584007], "norm_scores": [0.33402448892593384, 0.333095908164978, 0.33287957310676575]}}
{"id": "WebQTest-1051", "question": "when did the san francisco earthquake occur", "prediction": [["time.event.locations"], ["event.disaster.areas_affected"], ["location.location.events"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the san francisco earthquake occur [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.0028934653382748365, -0.05004457011818886, -0.06194575875997543], "norm_scores": [0.3452322483062744, 0.32933199405670166, 0.3254357874393463]}}
{"id": "WebQTest-1052", "question": "who played damien in the omen 2006", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played damien in the omen 2006 [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.017027754336595535, -0.023687060922384262, -0.03736608847975731], "norm_scores": [0.33633458614349365, 0.33410224318504333, 0.329563170671463]}}
{"id": "WebQTest-1054", "question": "who played denver in four christmases", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"], ["film.performance.film", "film.actor.film"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["film.film.language", "people.person.languages"], ["film.performance.film", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played denver in four christmases [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.film <SEP> film.actor.film </PATH>"], "scores": [-0.01961572654545307, -0.031097223982214928, -0.04724915698170662], "norm_scores": [0.3376862108707428, 0.333831250667572, 0.3284825384616852]}}
{"id": "WebQTest-1055", "question": "what to do with kids in essex", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do with kids in essex [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.012176800519227982, -0.017527339980006218, -0.03554777055978775], "norm_scores": [0.3365231454372406, 0.33472734689712524, 0.32874947786331177]}}
{"id": "WebQTest-1056", "question": "who did the cleveland cavaliers draft", "prediction": [["sports.sports_league_draft_pick.team", "sports.drafted_athlete.drafted"], ["sports.sports_league_draft_pick.team", "sports.sports_league_draft_pick.player"], ["sports.professional_sports_team.draft_picks", "sports.drafted_athlete.drafted"]], "ground_paths": [["sports.sports_team_roster.team", "sports.pro_athlete.teams"], ["sports.professional_sports_team.draft_picks", "sports.sports_league_draft_pick.player"], ["sports.sports_team_roster.team", "sports.sports_team_roster.player"], ["sports.professional_sports_team.draft_picks", "sports.drafted_athlete.drafted"], ["sports.sports_league_draft_pick.team", "sports.sports_league_draft_pick.player"], ["sports.sports_team.roster", "sports.pro_athlete.teams"], ["sports.sports_league_draft_pick.team", "sports.drafted_athlete.drafted"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did the cleveland cavaliers draft [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_league_draft_pick.team <SEP> sports.drafted_athlete.drafted </PATH>", "<PATH> sports.sports_league_draft_pick.team <SEP> sports.sports_league_draft_pick.player </PATH>", "<PATH> sports.professional_sports_team.draft_picks <SEP> sports.drafted_athlete.drafted </PATH>"], "scores": [-0.02129935845732689, -0.023498430848121643, -0.02432936057448387], "norm_scores": [0.333914577960968, 0.33318108320236206, 0.3329043388366699]}}
{"id": "WebQTest-1057", "question": "what concentration camp did anne frank died in", "prediction": [["base.fictionaluniverse.deceased_fictional_character.place_of_death"], ["film.film.story_by", "base.fictionaluniverse.deceased_fictional_character.place_of_death"], ["book.written_work.subjects"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what concentration camp did anne frank died in [/INST]", "raw_output": {"paths": ["<PATH> base.fictionaluniverse.deceased_fictional_character.place_of_death </PATH>", "<PATH> film.film.story_by <SEP> base.fictionaluniverse.deceased_fictional_character.place_of_death </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.006727982312440872, -0.04052358120679855, -0.04541609063744545], "norm_scores": [0.34143444895744324, 0.3300882875919342, 0.32847723364830017]}}
{"id": "WebQTest-1059", "question": "what language does east timor speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does east timor speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.007763287518173456, -0.029141131788492203, -0.04727563634514809], "norm_scores": [0.34012365341186523, 0.33292973041534424, 0.3269466161727905]}}
{"id": "WebQTest-1060", "question": "what is the currency in italy", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency in italy [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.005322195589542389, -0.02723705768585205, -0.1436259001493454], "norm_scores": [0.3509809374809265, 0.3433729112148285, 0.3056461811065674]}}
{"id": "WebQTest-1061", "question": "where does greek language come from", "prediction": [["language.human_language.language_family"], ["base.rosetta.languoid.parent"], ["base.rosetta.languoid.child"]], "ground_paths": [["base.rosetta.languoid.child"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does greek language come from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>", "<PATH> base.rosetta.languoid.child </PATH>"], "scores": [-0.00856938585639, -0.025239141657948494, -0.05542634055018425], "norm_scores": [0.3404032588005066, 0.3347758650779724, 0.3248209059238434]}}
{"id": "WebQTest-1062", "question": "what is the name of the currency used in italy", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the currency used in italy [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.005203659646213055, -0.024493694305419922, -0.1399366706609726], "norm_scores": [0.35028183460235596, 0.3435896337032318, 0.3061285614967346]}}
{"id": "WebQTest-1063", "question": "where is audrey hepburn from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is audrey hepburn from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.0011439210502430797, -0.05320614203810692, -0.0993262305855751], "norm_scores": [0.3501702845096588, 0.33240607380867004, 0.31742361187934875]}}
{"id": "WebQTest-1065", "question": "who does darth vaders voice", "prediction": [["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does darth vaders voice [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.02166890539228916, -0.022343436256051064, -0.03721189498901367], "norm_scores": [0.33513155579566956, 0.33490556478500366, 0.3299628496170044]}}
{"id": "WebQTest-1067", "question": "what school did douglas macarthur go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did douglas macarthur go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.015228070318698883, -0.017209943383932114, -0.030876606702804565], "norm_scores": [0.33528992533683777, 0.33462607860565186, 0.330083966255188]}}
{"id": "WebQTest-1068", "question": "what famous people are from barbados", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.nationality"], ["location.location.contains", "location.location.people_born_here"], ["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what famous people are from barbados [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.013245348818600178, -0.025595417246222496, -0.04111983999609947], "norm_scores": [0.3378108739852905, 0.33366453647613525, 0.3285245895385742]}}
{"id": "WebQTest-1069", "question": "what is the name of the currency used in china", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the currency used in china [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.006708175875246525, -0.020725103095173836, -0.14599482715129852], "norm_scores": [0.3501327633857727, 0.3452591896057129, 0.304608017206192]}}
{"id": "WebQTest-1070", "question": "what are the names of michael jackson 's 3 children", "prediction": [["people.person.parents"], ["people.person.children"], ["music.composition.composer", "people.person.parents"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the names of michael jackson 's 3 children [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> music.composition.composer <SEP> people.person.parents </PATH>"], "scores": [-0.00898187980055809, -0.017233436927199364, -0.09432773292064667], "norm_scores": [0.3436453342437744, 0.34082141518592834, 0.315533310174942]}}
{"id": "WebQTest-1072", "question": "who is the current governor of arizona 2010", "prediction": [["location.location.containedby", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["location.country.first_level_divisions", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the current governor of arizona 2010 [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.019623521715402603, -0.020566992461681366, -0.023613400757312775], "norm_scores": [0.3338814675807953, 0.3335666060447693, 0.3325519561767578]}}
{"id": "WebQTest-1074", "question": "who is lamar odom playing for this year", "prediction": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team.roster"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is lamar odom playing for this year [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.016943685710430145, -0.019642001017928123, -0.032245486974716187], "norm_scores": [0.33533191680908203, 0.3344283103942871, 0.33023977279663086]}}
{"id": "WebQTest-1075", "question": "where were the great pyramids of giza built", "prediction": [["architecture.building.building_complex"], ["location.location.containedby"], ["architecture.structure.architect"]], "ground_paths": [["architecture.building.building_complex"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where were the great pyramids of giza built [/INST]", "raw_output": {"paths": ["<PATH> architecture.building.building_complex </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> architecture.structure.architect </PATH>"], "scores": [-0.007101567927747965, -0.023545870557427406, -0.09392772614955902], "norm_scores": [0.34476497769355774, 0.33914193511009216, 0.3160931169986725]}}
{"id": "WebQTest-1077", "question": "what language do the russian speak", "prediction": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do the russian speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.016846872866153717, -0.020181648433208466, -0.024913256987929344], "norm_scores": [0.33460068702697754, 0.33348676562309265, 0.3319125473499298]}}
{"id": "WebQTest-1078", "question": "what language do australia people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do australia people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.007843396626412868, -0.019880633801221848, -0.07827913761138916], "norm_scores": [0.34246310591697693, 0.3383655250072479, 0.31917139887809753]}}
{"id": "WebQTest-1079", "question": "what country was king tut 's tomb found", "prediction": [["people.person.nationality"], ["people.deceased_person.place_of_death"], ["people.deceased_person.place_of_burial"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country was king tut 's tomb found [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>"], "scores": [-0.016343319788575172, -0.03023521788418293, -0.031027769669890404], "norm_scores": [0.33651599287986755, 0.33187347650527954, 0.3316105604171753]}}
{"id": "WebQTest-1080", "question": "where did abraham and his descendants live", "prediction": [["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality"]], "ground_paths": [["people.deceased_person.place_of_death"], ["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did abraham and his descendants live [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.01945527456700802, -0.03395739570260048, -0.035439975559711456], "norm_scores": [0.3367292582988739, 0.33188119530677795, 0.33138954639434814]}}
{"id": "WebQTest-1081", "question": "where did george washington carver go to collage", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did george washington carver go to collage [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.01708918809890747, -0.01778479665517807, -0.01856153830885887], "norm_scores": [0.3335742652416229, 0.3333422839641571, 0.33308348059654236]}}
{"id": "WebQTest-1082", "question": "what are the sports in spain", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["cricket.cricket_team.country"]], "ground_paths": [["sports.sports_team.location"], ["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the sports in spain [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> cricket.cricket_team.country </PATH>"], "scores": [-0.01058505941182375, -0.015179695561528206, -0.1398317813873291], "norm_scores": [0.34792613983154297, 0.34633123874664307, 0.3057425916194916]}}
{"id": "WebQTest-1083", "question": "where president obama was born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where president obama was born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.0034246344584971666, -0.03973061591386795, -0.08145042508840561], "norm_scores": [0.34610629081726074, 0.3337659239768982, 0.3201277554035187]}}
{"id": "WebQTest-1084", "question": "what sports do people play in poland", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["cricket.cricket_team.country"]], "ground_paths": [["sports.sports_team.location"], ["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sports do people play in poland [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> cricket.cricket_team.country </PATH>"], "scores": [-0.010257803834974766, -0.0150632094591856, -0.1588166058063507], "norm_scores": [0.34999847412109375, 0.3483206331729889, 0.30168095231056213]}}
{"id": "WebQTest-1085", "question": "who was demi lovato dating", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["music.artist.genre", "music.artist.genre"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["music.artist.label", "music.artist.label"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["people.person.languages", "people.person.languages"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["music.recording.featured_artists", "music.album.featured_artists"], ["music.composer.compositions", "music.composer.compositions"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["music.recording.artist", "music.composition.composer"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["base.popstra.friendship.participant", "base.popstra.celebrity.friendship"], ["music.composer.compositions", "music.composition.composer"], ["music.artist.album", "music.composer.compositions"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["music.album.artist", "music.album.artist"], ["music.artist.genre", "music.artist.genre"], ["people.person.nationality", "people.person.nationality"], ["music.recording.artist", "music.composer.compositions"], ["people.person.ethnicity", "people.person.ethnicity"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was demi lovato dating [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.028853248804807663, -0.030513333156704903, -0.04012760519981384], "norm_scores": [0.33476948738098145, 0.3342142105102539, 0.3310163617134094]}}
{"id": "WebQTest-1087", "question": "what films has scarlett johansson been in", "prediction": [["people.person.languages", "film.film.language"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["film.actor.film", "film.film.starring"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["people.person.place_of_birth", "film.film.featured_film_locations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.character"], ["film.actor.film", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["people.person.languages", "film.film.language"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what films has scarlett johansson been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.023814428597688675, -0.02420406974852085, -0.03383077308535576], "norm_scores": [0.3344879746437073, 0.33435767889022827, 0.33115437626838684]}}
{"id": "WebQTest-1088", "question": "who originally voiced meg on family guy", "prediction": [["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_program.regular_cast", "tv.tv_actor.starring_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who originally voiced meg on family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.02431379444897175, -0.02581777237355709, -0.02618330344557762], "norm_scores": [0.3337082862854004, 0.33320677280426025, 0.33308500051498413]}}
{"id": "WebQTest-1089", "question": "who played amy squirrel in bad teacher", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.film.starring", "film.actor.film"], ["film.film.language", "people.person.languages"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played amy squirrel in bad teacher [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.028127701953053474, -0.035767845809459686, -0.03604715317487717], "norm_scores": [0.3350644111633301, 0.332514226436615, 0.33242133259773254]}}
{"id": "WebQTest-1090", "question": "where are boeing headquarters", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.place_founded", "location.location.containedby"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are boeing headquarters [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.place_founded <SEP> location.location.containedby </PATH>"], "scores": [-0.005018908064812422, -0.044027701020240784, -0.05333036184310913], "norm_scores": [0.34310269355773926, 0.329976350069046, 0.3269209563732147]}}
{"id": "WebQTest-1091", "question": "what type of religions are in the united states", "prediction": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"], ["book.newspaper.circulation_areas", "book.periodical.subjects"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of religions are in the united states [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.periodical.subjects </PATH>"], "scores": [-0.007942913100123405, -0.015750307589769363, -0.06535161286592484], "norm_scores": [0.34054943919181824, 0.33790096640586853, 0.32154953479766846]}}
{"id": "WebQTest-1092", "question": "who did viggo mortensen play in lord of the rings", "prediction": [["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"]], "ground_paths": [["media_common.adapted_work.adaptations", "book.book.characters"], ["film.film_series.films_in_series", "book.book.characters"], ["film.performance.actor", "film.performance.character"], ["music.featured_artist.recordings", "book.book_character.appears_in_book"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.film.starring", "film.performance.character"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.film.story_by", "fictional_universe.fictional_character.character_created_by"], ["film.performance.film", "film.performance.character"], ["media_common.adaptation.adapted_from", "book.book_character.appears_in_book"], ["film.actor.film", "film.performance.character"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did viggo mortensen play in lord of the rings [/INST]", "raw_output": {"paths": ["<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.021845603361725807, -0.022746503353118896, -0.027772115543484688], "norm_scores": [0.3340916633605957, 0.3337908387184143, 0.3321175277233124]}}
{"id": "WebQTest-1094", "question": "who was isaac newton and what did he do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was isaac newton and what did he do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0006559921894222498, -0.06503373384475708, -0.08175310492515564], "norm_scores": [0.3496803343296051, 0.32787802815437317, 0.3224416673183441]}}
{"id": "WebQTest-1095", "question": "who did william shakespeare marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.children"]], "ground_paths": [["people.person.parents", "people.person.children"], ["freebase.valuenotation.has_value", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.parents"], ["people.place_of_interment.interred_here", "people.deceased_person.place_of_burial"], ["people.person.place_of_birth", "people.deceased_person.place_of_death"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["book.book_subject.works", "book.book_subject.works"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did william shakespeare marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.018221551552414894, -0.04377893730998039, -0.04521962255239487], "norm_scores": [0.33919811248779297, 0.33063891530036926, 0.3301629424095154]}}
{"id": "WebQTest-1096", "question": "what movies queen latifah is in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"]], "ground_paths": [["film.actor.film", "film.film.starring"], ["film.film.produced_by"], ["film.person_or_entity_appearing_in_film.films", "film.personal_film_appearance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["music.recording.artist"], ["film.personal_film_appearance.person", "film.personal_film_appearance.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["film.producer.film"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.producer.films_executive_produced"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.producer.film", "film.film.sequel"], ["film.performance.actor", "film.performance.film"], ["film.film.executive_produced_by"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["people.person.languages", "film.film.language"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies queen latifah is in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>"], "scores": [-0.024805692955851555, -0.027132559567689896, -0.03963715583086014], "norm_scores": [0.33523818850517273, 0.3344590663909912, 0.33030277490615845]}}
{"id": "WebQTest-1097", "question": "where did michelle obama graduated from", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did michelle obama graduated from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.015978751704096794, -0.01964782364666462, -0.021016456186771393], "norm_scores": [0.3343014121055603, 0.333077073097229, 0.3326215147972107]}}
{"id": "WebQTest-1098", "question": "what kind of music did franz liszt compose", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["music.composition.composer"]], "ground_paths": [["music.artist.genre"], ["music.genre.artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of music did franz liszt compose [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> music.composition.composer </PATH>"], "scores": [-0.005051077343523502, -0.03244893252849579, -0.06288247555494308], "norm_scores": [0.3428434431552887, 0.3335777819156647, 0.32357874512672424]}}
{"id": "WebQTest-1099", "question": "what country does david luiz play for", "prediction": [["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"]], "ground_paths": [["people.person.nationality", "sports.sports_team_location.teams"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country does david luiz play for [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_team.player_statistics </PATH>"], "scores": [-0.030920088291168213, -0.031291842460632324, -0.03388883173465729], "norm_scores": [0.33370441198349, 0.3335804045200348, 0.3327151834964752]}}
{"id": "WebQTest-1100", "question": "what document did thomas jefferson wrote", "prediction": [["book.written_work.author"], ["book.author.book_editions_published"], ["book.author.works_written"]], "ground_paths": [["book.book_edition.author_editor"], ["book.book_subject.works"], ["book.written_work.author"], ["book.author.book_editions_published"], [], ["book.written_work.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what document did thomas jefferson wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.010861825197935104, -0.027330370619893074, -0.03697154298424721], "norm_scores": [0.3380783498287201, 0.33255627751350403, 0.32936546206474304]}}
{"id": "WebQTest-1101", "question": "who is martin luther king jr facts", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["symbols.namesake.named_after"]], "ground_paths": [["base.activism.activist.area_of_activism"], ["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is martin luther king jr facts [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.001728506525978446, -0.06611732393503189, -0.06720618903636932], "norm_scores": [0.3479155898094177, 0.32621970772743225, 0.32586470246315]}}
{"id": "WebQTest-1102", "question": "where is victoria british columbia", "prediction": [["location.location.containedby"], ["location.province.capital"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["location.location.containedby"], ["base.aareas.schema.administrative_area.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is victoria british columbia [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.province.capital </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.009629063308238983, -0.020098038017749786, -0.06852735579013824], "norm_scores": [0.341018944978714, 0.3374674618244171, 0.3215135931968689]}}
{"id": "WebQTest-1103", "question": "how many mary mary sisters", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["music.artist.genre", "music.artist.genre"], ["people.person.parents", "people.person.children"]], "ground_paths": [["music.album.artist", "music.composition.composer"], ["music.artist.origin", "people.person.place_of_birth"], ["music.group_membership.group", "music.group_membership.member"], ["music.album.artist", "music.composer.compositions"], ["music.musical_group.member", "music.group_member.membership"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how many mary mary sisters [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.03876597806811333, -0.04019681364297867, -0.04838593304157257], "norm_scores": [0.33456048369407654, 0.33408209681510925, 0.3313574492931366]}}
{"id": "WebQTest-1104", "question": "what do we call the currency of france", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.currency_used"], ["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do we call the currency of france [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.00865735299885273, -0.01867487095296383, -0.09828497469425201], "norm_scores": [0.3443165719509125, 0.34088459610939026, 0.31479886174201965]}}
{"id": "WebQTest-1106", "question": "what team does david beckham play for in 2012", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player_loan.player", "soccer.football_player_loan.lending_team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["book.written_work.subjects", "book.written_work.subjects"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does david beckham play for in 2012 [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.02060932107269764, -0.02539876662194729, -0.029252635315060616], "norm_scores": [0.33482709527015686, 0.3332273066043854, 0.3319455683231354]}}
{"id": "WebQTest-1107", "question": "what movies did nick cannon star in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["film.actor.film", "film.film.starring"], ["tv.tv_producer.programs_produced", "tv.tv_program.tv_producer"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["music.artist.track", "music.recording.releases"], ["film.actor.film", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did nick cannon star in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.016861533746123314, -0.03537384420633316, -0.03567848727107048], "norm_scores": [0.3374938368797302, 0.33130353689193726, 0.33120259642601013]}}
{"id": "WebQTest-1108", "question": "what radio station is npr on in nyc", "prediction": [["broadcast.radio_station.owner"], ["broadcast.radio_station_owner.radio_stations"], ["broadcast.content.location"]], "ground_paths": [["award.award_winner.awards_won", "award.award_honor.award_winner"], ["broadcast.radio_network.affiliates", "broadcast.radio_affiliation_duration.station"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["broadcast.producer.produces", "broadcast.producer.produces"], ["broadcast.producer.location", "broadcast.broadcast.area_served"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what radio station is npr on in nyc [/INST]", "raw_output": {"paths": ["<PATH> broadcast.radio_station.owner </PATH>", "<PATH> broadcast.radio_station_owner.radio_stations </PATH>", "<PATH> broadcast.content.location </PATH>"], "scores": [-0.019078971818089485, -0.03562545031309128, -0.04415348544716835], "norm_scores": [0.3379717767238617, 0.3324255347251892, 0.3296026587486267]}}
{"id": "WebQTest-1109", "question": "who did tom landry play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did tom landry play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.01138203777372837, -0.012295450083911419, -0.05762520432472229], "norm_scores": [0.338535875082016, 0.3382267653942108, 0.3232373595237732]}}
{"id": "WebQTest-1110", "question": "where was john jay born", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was john jay born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.010450547561049461, -0.016157953068614006, -0.12305564433336258], "norm_scores": [0.3462828993797302, 0.3443121612071991, 0.30940496921539307]}}
{"id": "WebQTest-1111", "question": "where is mount fuji located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.mountain.mountain_range"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mount fuji located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.mountain.mountain_range </PATH>"], "scores": [-0.011890082620084286, -0.023122156038880348, -0.05064310505986214], "norm_scores": [0.3388889729976654, 0.335103839635849, 0.326007217168808]}}
{"id": "WebQTest-1113", "question": "where was toussaint l'ouverture born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was toussaint l'ouverture born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.004149063024669886, -0.030047114938497543, -0.08537912368774414], "norm_scores": [0.3452542722225189, 0.33642762899398804, 0.31831806898117065]}}
{"id": "WebQTest-1114", "question": "what countries makes up the uk", "prediction": [["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"]], "ground_paths": [["base.biblioness.bibs_location.country"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries makes up the uk [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-0.019200723618268967, -0.030791236087679863, -0.031117230653762817], "norm_scores": [0.3359503149986267, 0.3320789635181427, 0.3319707214832306]}}
{"id": "WebQTest-1115", "question": "what is lawton ok zip code", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is lawton ok zip code [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00018235601601190865, -0.09792478382587433, -0.1405043601989746], "norm_scores": [0.36023563146591187, 0.3266913592815399, 0.31307297945022583]}}
{"id": "WebQTest-1116", "question": "what is the capital city of spain wiki answers", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital city of spain wiki answers [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.006988154724240303, -0.03237069770693779, -0.040481340140104294], "norm_scores": [0.3399049937725067, 0.33138594031333923, 0.32870903611183167]}}
{"id": "WebQTest-1117", "question": "where do they speak german", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do they speak german [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.01724132150411606, -0.022718939930200577, -0.0231828223913908], "norm_scores": [0.33460333943367004, 0.3327755033969879, 0.33262118697166443]}}
{"id": "WebQTest-1118", "question": "when did mark mcgwire retired", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.season"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.season"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"]], "ground_paths": [["baseball.lifetime_batting_statistics.player", "baseball.lifetime_batting_statistics.ending_season"], ["baseball.batting_statistics.player", "baseball.batting_statistics.season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did mark mcgwire retired [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.season </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.season </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>"], "scores": [-0.013241749256849289, -0.015026737935841084, -0.026355477049946785], "norm_scores": [0.33498722314834595, 0.3343898057937622, 0.33062297105789185]}}
{"id": "WebQTest-1119", "question": "what is colorado technical university", "prediction": [["location.location.containedby"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"], ["common.topic.notable_types"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is colorado technical university [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.025304436683654785, -0.03334641829133034, -0.03932715579867363], "norm_scores": [0.33578845858573914, 0.3330988883972168, 0.33111265301704407]}}
{"id": "WebQTest-1120", "question": "what was the currency used in france before the euro", "prediction": [["location.country.currency_formerly_used"], ["finance.currency.countries_formerly_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the currency used in france before the euro [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.00601421482861042, -0.025483708828687668, -0.0482020378112793], "norm_scores": [0.34020450711250305, 0.33364495635032654, 0.3261505365371704]}}
{"id": "WebQTest-1121", "question": "what is kentucky state slogan", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.location_symbol_relationship.symbol"]], "ground_paths": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is kentucky state slogan [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.013685076497495174, -0.02006729505956173, -0.02344527095556259], "norm_scores": [0.3351290225982666, 0.3329969644546509, 0.3318740129470825]}}
{"id": "WebQTest-1124", "question": "who played samantha stephens in bewitched", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"]], "ground_paths": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["tv.tv_program.regular_cast", "tv.tv_actor.starring_roles"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_honor.honored_for", "award.award_honor.award_winner"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played samantha stephens in bewitched [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.016075510531663895, -0.032355569303035736, -0.03411830589175224], "norm_scores": [0.3371577262878418, 0.3317132294178009, 0.3311290144920349]}}
{"id": "WebQTest-1125", "question": "what area of science did sir isaac newton study", "prediction": [["people.person.profession"], ["education.education.student", "education.education.major_field_of_study"], ["law.invention.inventor"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what area of science did sir isaac newton study [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> law.invention.inventor </PATH>"], "scores": [-0.005675206426531076, -0.03262922912836075, -0.04495803266763687], "norm_scores": [0.34072887897491455, 0.33166754245758057, 0.3276035785675049]}}
{"id": "WebQTest-1126", "question": "what county is san diego in", "prediction": [["location.us_county.hud_county_place"], ["location.hud_county_place.county"], ["location.location.containedby"]], "ground_paths": [["location.us_county.county_seat"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is san diego in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.019150909036397934, -0.022671034559607506, -0.025508126243948936], "norm_scores": [0.334431529045105, 0.333256334066391, 0.3323121964931488]}}
{"id": "WebQTest-1127", "question": "which country in north america is divided into provinces", "prediction": [["location.location.containedby"], ["base.locations.continents.countries_within"], ["base.locations.countries.continent"]], "ground_paths": [["base.locations.continents.countries_within"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country in north america is divided into provinces [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.01637243665754795, -0.021248964592814445, -0.022740865126252174], "norm_scores": [0.3345838785171509, 0.3329562544822693, 0.33245986700057983]}}
{"id": "WebQTest-1130", "question": "what is real name of santa claus", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["fictional_universe.fictional_character.character_created_by"], ["fictional_universe.fictional_character.occupation", "people.person.profession"]], "ground_paths": [["fictional_universe.fictional_character.based_on"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is real name of santa claus [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> people.person.profession </PATH>"], "scores": [-0.005411460995674133, -0.040192317217588425, -0.05321407690644264], "norm_scores": [0.3425667881965637, 0.3308568596839905, 0.3265764117240906]}}
{"id": "WebQTest-1131", "question": "when did kelly slater go pro", "prediction": [["sports.sports_award_winner.awards", "sports.sports_award.season"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["sports.sports_award.award_winner", "sports.sports_award.season"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did kelly slater go pro [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.season </PATH>"], "scores": [-0.01797562465071678, -0.02987435646355152, -0.03881040960550308], "norm_scores": [0.336978018283844, 0.33299216628074646, 0.33002978563308716]}}
{"id": "WebQTest-1132", "question": "what did coretta scott king died from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did coretta scott king died from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>"], "scores": [-0.001441235188394785, -0.04432033747434616, -0.1135062575340271], "norm_scores": [0.3506295084953308, 0.3359125852584839, 0.3134579062461853]}}
{"id": "WebQTest-1133", "question": "what does wh smith stand for", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["organization.organization.founders"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does wh smith stand for [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> organization.organization.founders </PATH>"], "scores": [-0.02509477362036705, -0.04178796708583832, -0.047282081097364426], "norm_scores": [0.33766645193099976, 0.3320765197277069, 0.33025702834129333]}}
{"id": "WebQTest-1135", "question": "who was vp for nixon", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vp for nixon [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.005762824323028326, -0.02278163656592369, -0.11693421006202698], "norm_scores": [0.3474743068218231, 0.34161075949668884, 0.31091493368148804]}}
{"id": "WebQTest-1136", "question": "what religion did adolf hitler follow", "prediction": [["people.person.religion"], ["religion.religion.notable_figures"], ["religion.religion.founding_figures"]], "ground_paths": [["people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religion did adolf hitler follow [/INST]", "raw_output": {"paths": ["<PATH> people.person.religion </PATH>", "<PATH> religion.religion.notable_figures </PATH>", "<PATH> religion.religion.founding_figures </PATH>"], "scores": [-0.0012432597577571869, -0.05996182933449745, -0.08075784891843796], "norm_scores": [0.3488530218601227, 0.3289587199687958, 0.32218828797340393]}}
{"id": "WebQTest-1137", "question": "where is the illinois river on a map", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the illinois river on a map [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.011196930892765522, -0.026950335130095482, -0.03484615311026573], "norm_scores": [0.3377239406108856, 0.33244532346725464, 0.32983073592185974]}}
{"id": "WebQTest-1138", "question": "where honduras in the world", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.countries.continent"]], "ground_paths": [["location.location.containedby"], ["base.locations.continents.countries_within"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where honduras in the world [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.016693951562047005, -0.019291231408715248, -0.03880609571933746], "norm_scores": [0.3360738158226013, 0.33520206809043884, 0.32872405648231506]}}
{"id": "WebQTest-1142", "question": "who does ronaldinho play for now 2011", "prediction": [["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "ground_paths": [["people.person.nationality", "sports.sports_team_location.teams"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does ronaldinho play for now 2011 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> people.person.nationality <SEP> sports.sports_team_location.teams </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_player_stats.team </PATH>"], "scores": [-0.024636246263980865, -0.025715216994285583, -0.028284022584557533], "norm_scores": [0.33385854959487915, 0.3334985375404358, 0.33264294266700745]}}
{"id": "WebQTest-1143", "question": "what galileo galilei was famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["symbols.namesake.named_after"]], "ground_paths": [["influence.influence_node.influenced", "base.argumentmaps.innovator.original_ideas"], ["book.book_edition.author_editor", "book.written_work.subjects"], ["astronomy.astronomical_discovery.discoverer", "astronomy.star_system_body.star_system"], ["book.written_work.author", "book.book_subject.works"], ["symbols.name_source.namesakes", "astronomy.star_system_body.star_system"], ["astronomy.astronomer.astronomical_objects_discovered", "astronomy.star_system_body.star_system"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what galileo galilei was famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.0012773519847542048, -0.06433379650115967, -0.07650161534547806], "norm_scores": [0.3488664925098419, 0.32754746079444885, 0.32358604669570923]}}
{"id": "WebQTest-1144", "question": "what are the major trading partners of germany", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the major trading partners of germany [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.statistical_region.places_exported_to <SEP> location.statistical_region.places_imported_from </PATH>"], "scores": [-0.028568292036652565, -0.039179105311632156, -0.04110260680317879], "norm_scores": [0.3359098434448242, 0.33236443996429443, 0.33172571659088135]}}
{"id": "WebQTest-1145", "question": "where did romney graduated college", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.religion", "religion.religion.organizations"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did romney graduated college [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.017276709899306297, -0.019232090562582016, -0.02418271079659462], "norm_scores": [0.33431798219680786, 0.3336648941040039, 0.3320171535015106]}}
{"id": "WebQTest-1146", "question": "who did johnny depp play in corpse bride", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.performance.character"], ["film.performance.film", "film.performance.character"]], "ground_paths": [["film.performance.film", "film.performance.character"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did johnny depp play in corpse bride [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>"], "scores": [-0.01784578338265419, -0.024988669902086258, -0.03241917863488197], "norm_scores": [0.3357490599155426, 0.33335939049720764, 0.33089157938957214]}}
{"id": "WebQTest-1147", "question": "what is the zip code for jonesboro ar", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for jonesboro ar [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-7.486243703169748e-05, -0.10565488040447235, -0.15150086581707], "norm_scores": [0.36241286993026733, 0.3260999917984009, 0.3114871382713318]}}
{"id": "WebQTest-1148", "question": "what college did se hinton go", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did se hinton go [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.01785249449312687, -0.019732121378183365, -0.02291410230100155], "norm_scores": [0.3341047167778015, 0.33347731828689575, 0.33241790533065796]}}
{"id": "WebQTest-1149", "question": "when was taylor swift fearless tour", "prediction": [["music.concert_tour.artist"], ["time.event.locations"], ["music.artist.concert_tours", "time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was taylor swift fearless tour [/INST]", "raw_output": {"paths": ["<PATH> music.concert_tour.artist </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> music.artist.concert_tours <SEP> time.event.locations </PATH>"], "scores": [-0.004371113143861294, -0.049309857189655304, -0.06191502884030342], "norm_scores": [0.344811350107193, 0.32965898513793945, 0.32552966475486755]}}
{"id": "WebQTest-1151", "question": "where did charles darwin and his wife live", "prediction": [["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["people.person.nationality", "location.location.containedby"], ["book.written_work.subjects", "location.location.contains"]], "ground_paths": [["people.deceased_person.place_of_burial", "travel.travel_destination.tourist_attractions"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["book.book_subject.works", "book.written_work.subjects"], ["people.person.nationality", "location.country.capital"], ["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["people.person.places_lived", "people.place_lived.location"], ["book.written_work.subjects", "location.country.capital"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["influence.influence_node.influenced", "people.person.place_of_birth"], ["people.person.children", "people.deceased_person.place_of_death"], ["book.written_work.subjects", "location.location.contains"], ["book.written_work.subjects"], ["book.written_work.subjects", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did charles darwin and his wife live [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.contains </PATH>"], "scores": [-0.0303055290132761, -0.034303102642297745, -0.04032325744628906], "norm_scores": [0.3348914086818695, 0.33355531096458435, 0.33155331015586853]}}
{"id": "WebQTest-1152", "question": "who is the voice of kitt", "prediction": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["tv.tv_guest_role.character", "tv.tv_guest_role.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the voice of kitt [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.0323493592441082, -0.03977738693356514, -0.04269101843237877], "norm_scores": [0.3353103995323181, 0.33282893896102905, 0.33186060190200806]}}
{"id": "WebQTest-1153", "question": "what is eminem 's real full name", "prediction": [["people.person.nationality", "people.person.nationality"], ["people.person.nationality"], ["music.artist.album", "music.composer.compositions"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is eminem 's real full name [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> music.artist.album <SEP> music.composer.compositions </PATH>"], "scores": [-0.03364526852965355, -0.039809007197618484, -0.04216871038079262], "norm_scores": [0.33496707677841187, 0.3329087793827057, 0.33212411403656006]}}
{"id": "WebQTest-1154", "question": "who is amy grant 's first husband", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.parents", "people.person.children"], ["people.person.parents", "people.person.parents"], ["people.marriage.spouse", "people.marriage.spouse"], ["music.artist.genre", "music.artist.genre"], ["music.artist.origin", "music.artist.origin"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is amy grant 's first husband [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.02558708004653454, -0.03373601287603378, -0.04305986315011978], "norm_scores": [0.3361838161945343, 0.3334554135799408, 0.3303608000278473]}}
{"id": "WebQTest-1155", "question": "what are the seven nations of central america", "prediction": [["location.location.contains"], ["location.location.containedby"], ["common.topic.notable_types"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the seven nations of central america [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.009074398316442966, -0.025832217186689377, -0.04303422197699547], "norm_scores": [0.338983952999115, 0.33335065841674805, 0.32766538858413696]}}
{"id": "WebQTest-1156", "question": "what is jamie spears baby name", "prediction": [["people.person.children"], ["people.person.parents"], ["base.popstra.celebrity.canoodled", "base.popstra.celebrity.canoodled"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is jamie spears baby name [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> base.popstra.celebrity.canoodled <SEP> base.popstra.celebrity.canoodled </PATH>"], "scores": [-0.008983876556158066, -0.02044765092432499, -0.12939709424972534], "norm_scores": [0.34780725836753845, 0.34384283423423767, 0.30834993720054626]}}
{"id": "WebQTest-1160", "question": "who is the mother of prince michael jackson", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.gender"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the mother of prince michael jackson [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.00756731815636158, -0.03205457329750061, -0.04019102081656456], "norm_scores": [0.33970707654953003, 0.33148959279060364, 0.3288033902645111]}}
{"id": "WebQTest-1161", "question": "what is the clemson mascot name", "prediction": [["education.school_mascot.school"], ["education.educational_institution.mascot"], []], "ground_paths": [["education.school_mascot.school"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the clemson mascot name [/INST]", "raw_output": {"paths": ["<PATH> education.school_mascot.school </PATH>", "<PATH> education.educational_institution.mascot </PATH>", "<PATH> </PATH>"], "scores": [-0.012330176308751106, -0.01433721836656332, -0.06719066947698593], "norm_scores": [0.3396033048629761, 0.338922381401062, 0.3214743435382843]}}
{"id": "WebQTest-1163", "question": "what rainforest is in south america", "prediction": [["location.location.containedby"], ["common.topic.notable_types"], ["location.location.partially_containedby"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what rainforest is in south america [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.0036169930826872587, -0.039820484817028046, -0.06236354634165764], "norm_scores": [0.34395113587379456, 0.33172163367271423, 0.3243272602558136]}}
{"id": "WebQTest-1164", "question": "what type of art does claude monet do", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art does claude monet do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.00586765306070447, -0.026887305080890656, -0.035960808396339417], "norm_scores": [0.33903419971466064, 0.33198222517967224, 0.3289835751056671]}}
{"id": "WebQTest-1166", "question": "when did we start war with iraq", "prediction": [["base.culturalevent.event.entity_involved"], ["time.event.locations"], ["time.event.includes_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did we start war with iraq [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> time.event.includes_event </PATH>"], "scores": [-0.025262754410505295, -0.027895867824554443, -0.0321584977209568], "norm_scores": [0.3343924283981323, 0.3335131108760834, 0.3320944607257843]}}
{"id": "WebQTest-1168", "question": "what guitar did kurt cobain", "prediction": [["music.group_member.instruments_played"], ["music.instrument.family"], ["music.guitarist.guitars_played"]], "ground_paths": [["music.guitarist.guitars_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what guitar did kurt cobain [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.guitarist.guitars_played </PATH>"], "scores": [-0.012262769974768162, -0.03179813176393509, -0.037036120891571045], "norm_scores": [0.3382737934589386, 0.33172962069511414, 0.32999658584594727]}}
{"id": "WebQTest-1169", "question": "who did jennifer lawrence play in x men", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did jennifer lawrence play in x men [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.0214577317237854, -0.027888920158147812, -0.030452216044068336], "norm_scores": [0.3350493013858795, 0.3329014480113983, 0.33204925060272217]}}
{"id": "WebQTest-1170", "question": "who is the governor of virginia 2011", "prediction": [["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the governor of virginia 2011 [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.017319900915026665, -0.018940776586532593, -0.0250310767441988], "norm_scores": [0.3343699872493744, 0.33382847905158997, 0.33180153369903564]}}
{"id": "WebQTest-1171", "question": "who did darth vaders voice", "prediction": [["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did darth vaders voice [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.022691866382956505, -0.023219425231218338, -0.03607814759016037], "norm_scores": [0.33487650752067566, 0.3346998691558838, 0.33042362332344055]}}
{"id": "WebQTest-1173", "question": "where is walton county ga", "prediction": [["location.location.containedby"], ["location.us_county.hud_county_place"], ["location.us_county.county_seat"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is walton county ga [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.us_county.county_seat </PATH>"], "scores": [-0.008988883346319199, -0.031592823565006256, -0.04581877961754799], "norm_scores": [0.33996379375457764, 0.3323654532432556, 0.32767075300216675]}}
{"id": "WebQTest-1174", "question": "what cancer did jackie kennedy die of", "prediction": [["people.deceased_person.cause_of_death"], ["medicine.disease.notable_people_with_this_condition"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what cancer did jackie kennedy die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.01428260374814272, -0.01460742112249136, -0.03564377874135971], "norm_scores": [0.33573487401008606, 0.33562585711479187, 0.3286392390727997]}}
{"id": "WebQTest-1175", "question": "what does lauren conrad major in", "prediction": [["people.person.education", "education.education.major_field_of_study"], ["education.education.student", "education.education.major_field_of_study"], ["education.education.student", "education.education.degree"]], "ground_paths": [["people.person.education", "education.field_of_study.students_majoring"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does lauren conrad major in [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> education.education.student <SEP> education.education.degree </PATH>"], "scores": [-0.008941908366978168, -0.02123931609094143, -0.045152448117733], "norm_scores": [0.3387286961078644, 0.3345887064933777, 0.32668253779411316]}}
{"id": "WebQTest-1176", "question": "what time zone is pennsylvania in right now", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is pennsylvania in right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-1.6281246644211933e-05, -0.12361986190080643, -0.18275776505470276], "norm_scores": [0.3680918216705322, 0.32529380917549133, 0.30661439895629883]}}
{"id": "WebQTest-1177", "question": "where did mario lopez go to school", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did mario lopez go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.012638546526432037, -0.024186640977859497, -0.025665590539574623], "norm_scores": [0.3360694348812103, 0.3322107791900635, 0.3317198157310486]}}
{"id": "WebQTest-1178", "question": "where is tupac from originally", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is tupac from originally [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0046078478917479515, -0.03308418020606041, -0.07014372199773788], "norm_scores": [0.3438209295272827, 0.3341682553291321, 0.3220107853412628]}}
{"id": "WebQTest-1179", "question": "what role did george lucas play in star wars", "prediction": [["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"], ["film.film.produced_by"], ["film.producer.films_executive_produced"]], "ground_paths": [["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what role did george lucas play in star wars [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.film.produced_by </PATH>", "<PATH> film.producer.films_executive_produced </PATH>"], "scores": [-0.029592743143439293, -0.0428895428776741, -0.04307137057185173], "norm_scores": [0.33631500601768494, 0.3318726718425751, 0.3318123519420624]}}
{"id": "WebQTest-1180", "question": "what sarah dessen books are movies", "prediction": [["film.film_story_contributor.film_story_credits"], ["book.book_edition.author_editor"], ["book.author.works_written"]], "ground_paths": [["film.film.story_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sarah dessen books are movies [/INST]", "raw_output": {"paths": ["<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.02515340782701969, -0.03114478662610054, -0.03287782520055771], "norm_scores": [0.3348589539527893, 0.332858681678772, 0.33228230476379395]}}
{"id": "WebQTest-1181", "question": "what is the current time in kauai hawaii", "prediction": [["location.location.time_zones"], ["geography.island.island_group", "location.location.time_zones"], ["location.location.containedby", "location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the current time in kauai hawaii [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> geography.island.island_group <SEP> location.location.time_zones </PATH>", "<PATH> location.location.containedby <SEP> location.location.time_zones </PATH>"], "scores": [-0.00023759432951919734, -0.08201195299625397, -0.09401589632034302], "norm_scores": [0.35311177372932434, 0.3253853917121887, 0.32150283455848694]}}
{"id": "WebQTest-1183", "question": "where was farrah fawcett buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["people.person.place_of_interment"]], "ground_paths": [["people.deceased_person.place_of_burial"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was farrah fawcett buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_interment </PATH>"], "scores": [-0.002533735241740942, -0.038292184472084045, -0.06824260205030441], "norm_scores": [0.34467583894729614, 0.33256852626800537, 0.3227556347846985]}}
{"id": "WebQTest-1185", "question": "what inspired steinbeck", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inspired steinbeck [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>"], "scores": [-0.010258791036903858, -0.014838729053735733, -0.11989288777112961], "norm_scores": [0.34583020210266113, 0.3442499041557312, 0.30991989374160767]}}
{"id": "WebQTest-1186", "question": "what kind of government does cuba have today", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does cuba have today [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.011834414675831795, -0.01191140990704298, -0.1477326601743698], "norm_scores": [0.34808585047721863, 0.3480590581893921, 0.3038550913333893]}}
{"id": "WebQTest-1187", "question": "where are english speakers distributed", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are english speakers distributed [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.014058799482882023, -0.024305762723088264, -0.03138110414147377], "norm_scores": [0.33640217781066895, 0.3329726755619049, 0.33062508702278137]}}
{"id": "WebQTest-1189", "question": "what is the capital of italy today", "prediction": [["location.country.capital"], ["base.biblioness.bibs_location.country"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of italy today [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.013330542482435703, -0.02664513699710369, -0.02713123895227909], "norm_scores": [0.3363529145717621, 0.33190420269966125, 0.33174288272857666]}}
{"id": "WebQTest-1191", "question": "what language do people speak in the bahamas", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people speak in the bahamas [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.004798285197466612, -0.033070944249629974, -0.04255169630050659], "norm_scores": [0.3407069444656372, 0.3312091529369354, 0.328083872795105]}}
{"id": "WebQTest-1192", "question": "where did george michael go to school", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did george michael go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.016069257631897926, -0.019005969166755676, -0.025311464443802834], "norm_scores": [0.334686815738678, 0.3337053656578064, 0.331607848405838]}}
{"id": "WebQTest-1194", "question": "what club does cristiano ronaldo play for in 2010", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"]], "ground_paths": [["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.transfers", "soccer.football_player_transfer.purchasing_team"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what club does cristiano ronaldo play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>"], "scores": [-0.020396392792463303, -0.02173665538430214, -0.02690642885863781], "norm_scores": [0.33420541882514954, 0.33375781774520874, 0.3320367932319641]}}
{"id": "WebQTest-1195", "question": "what time zone is in the bahamas", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is in the bahamas [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-8.129438356263563e-06, -0.15861137211322784, -0.1856643259525299], "norm_scores": [0.372592955827713, 0.31794655323028564, 0.30946049094200134]}}
{"id": "WebQTest-1197", "question": "what language do people speak in brazil", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people speak in brazil [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.0005931984051130712, -0.07401278614997864, -0.08445832878351212], "norm_scores": [0.3510291576385498, 0.326180100440979, 0.3227907121181488]}}
{"id": "WebQTest-1198", "question": "who influenced samuel taylor coleridge", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["people.person.profession"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who influenced samuel taylor coleridge [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.006954825483262539, -0.018703142181038857, -0.08999601751565933], "norm_scores": [0.3438040614128113, 0.339788556098938, 0.3164074122905731]}}
{"id": "WebQTest-1199", "question": "what does kim kardashian work as", "prediction": [["people.person.profession"], ["freebase.valuenotation.is_reviewed", "common.topic.notable_types"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does kim kardashian work as [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> common.topic.notable_types </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.000969265413004905, -0.07494166493415833, -0.08855786919593811], "norm_scores": [0.35151422023773193, 0.3264503479003906, 0.32203543186187744]}}
{"id": "WebQTest-1200", "question": "where did the mississippi river end", "prediction": [["geography.river.mouth"], ["location.location.containedby"], ["location.location.partially_containedby"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the mississippi river end [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.0014128688490018249, -0.06065817549824715, -0.08987310528755188], "norm_scores": [0.3499176800251007, 0.32978886365890503, 0.32029348611831665]}}
{"id": "WebQTest-1201", "question": "who played carlton lassiter on psych", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played carlton lassiter on psych [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.01988854818046093, -0.026677928864955902, -0.03156553581357002], "norm_scores": [0.33538761734962463, 0.3331182599067688, 0.3314940631389618]}}
{"id": "WebQTest-1203", "question": "who is president of israel", "prediction": [["people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is president of israel [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.019112268462777138, -0.019955316558480263, -0.025039594620466232], "norm_scores": [0.3340853154659271, 0.33380377292633057, 0.3321109116077423]}}
{"id": "WebQTest-1204", "question": "where did the anasazi indians live", "prediction": [["people.ethnicity.geographic_distribution"], ["location.location.contains"], ["location.location.containedby"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the anasazi indians live [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0007406082004308701, -0.08259159326553345, -0.09521094709634781], "norm_scores": [0.3531991243362427, 0.3254409432411194, 0.32135990262031555]}}
{"id": "WebQTest-1205", "question": "who plays harley quinn", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["cvg.game_performance.character", "cvg.game_performance.voice_actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays harley quinn [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.016284752637147903, -0.03186153620481491, -0.03922611474990845], "norm_scores": [0.33762526512145996, 0.3324068784713745, 0.3299678564071655]}}
{"id": "WebQTest-1206", "question": "what was francis bacon contributions", "prediction": [["people.person.profession"], ["law.invention.inventor"], ["law.inventor.inventions"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was francis bacon contributions [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.inventions </PATH>"], "scores": [-0.006073321215808392, -0.04308421164751053, -0.04435356706380844], "norm_scores": [0.3417508006095886, 0.32933348417282104, 0.32891571521759033]}}
{"id": "WebQTest-1209", "question": "when does ziva start on ncis", "prediction": [["tv.tv_series_season.series"], ["tv.tv_program.seasons"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.seasons"]], "ground_paths": [["tv.tv_program.seasons"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when does ziva start on ncis [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_series_season.series </PATH>", "<PATH> tv.tv_program.seasons </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.seasons </PATH>"], "scores": [-0.012512021698057652, -0.036127422004938126, -0.04573069140315056], "norm_scores": [0.33967524766921997, 0.3317476511001587, 0.32857707142829895]}}
{"id": "WebQTest-1210", "question": "what is the name of the national anthem of canada ( 2 words )", "prediction": [["music.composition.language"], ["music.composition.language", "music.composition.language"], ["location.location.containedby", "music.composition.language"]], "ground_paths": [["location.country.national_anthem", "government.national_anthem.national_anthem_of"], ["location.country.official_language", "music.composition.language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the national anthem of canada ( 2 words ) [/INST]", "raw_output": {"paths": ["<PATH> music.composition.language </PATH>", "<PATH> music.composition.language <SEP> music.composition.language </PATH>", "<PATH> location.location.containedby <SEP> music.composition.language </PATH>"], "scores": [-0.011074487119913101, -0.020060362294316292, -0.059080567210912704], "norm_scores": [0.3396528661251068, 0.3366144597530365, 0.3237326443195343]}}
{"id": "WebQTest-1212", "question": "what did mark zuckerberg study", "prediction": [["people.person.education", "education.education.major_field_of_study"], ["education.education.student", "education.education.major_field_of_study"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.profession", "base.skills.skill.performed_by"], ["people.person.education", "education.field_of_study.students_majoring"], ["education.education.student", "education.education.major_field_of_study"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did mark zuckerberg study [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.01012670248746872, -0.01944250799715519, -0.03317657485604286], "norm_scores": [0.3369338810443878, 0.33380964398384094, 0.32925644516944885]}}
{"id": "WebQTest-1213", "question": "which country was michael jackson born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["music.artist.origin"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country was michael jackson born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.01342557743191719, -0.017053207382559776, -0.048145975917577744], "norm_scores": [0.33758074045181274, 0.33635833859443665, 0.326060950756073]}}
{"id": "WebQTest-1215", "question": "who was stephen r covey", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"], ["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was stephen r covey [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0010128325084224343, -0.059738874435424805, -0.0995178073644638], "norm_scores": [0.35098108649253845, 0.3309629261493683, 0.31805598735809326]}}
{"id": "WebQTest-1216", "question": "what country was stalin in charge of", "prediction": [["people.person.nationality"], ["government.government_position_held.office_holder", "government.governmental_jurisdiction.governing_officials"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country was stalin in charge of [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>"], "scores": [-0.015031058341264725, -0.022380927577614784, -0.05005471035838127], "norm_scores": [0.33803659677505493, 0.3355611562728882, 0.3264022469520569]}}
{"id": "WebQTest-1217", "question": "what is monta ellis career high points", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"]], "ground_paths": [["sports.sports_award_winner.awards", "sports.sports_award.award"], ["award.award_honor.award_winner", "award.award_honor.award"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is monta ellis career high points [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>"], "scores": [-0.011960937641561031, -0.016871321946382523, -0.04004020243883133], "norm_scores": [0.33699387311935425, 0.3353431522846222, 0.32766297459602356]}}
{"id": "WebQTest-1219", "question": "what nationality is rihanna", "prediction": [["people.person.nationality"], ["people.ethnicity.people"], ["music.artist.origin"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what nationality is rihanna [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.011485306546092033, -0.029665173962712288, -0.04147955775260925], "norm_scores": [0.3387034237384796, 0.3326014578342438, 0.3286950886249542]}}
{"id": "WebQTest-1220", "question": "what has shannen doherty played in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["people.person.languages", "tv.tv_program.languages"], ["film.actor.film", "film.film.starring"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["film.actor.film", "film.performance.film"], ["people.person.languages", "film.film.language"], ["film.performance.actor", "film.film.starring"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has shannen doherty played in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.016121454536914825, -0.03191206604242325, -0.03450309485197067], "norm_scores": [0.3371407985687256, 0.33185896277427673, 0.3310002088546753]}}
{"id": "WebQTest-1221", "question": "what college did joakim noah", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did joakim noah [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.018550481647253036, -0.021506113931536674, -0.025237131863832474], "norm_scores": [0.33440515398979187, 0.33341822028160095, 0.3321765661239624]}}
{"id": "WebQTest-1223", "question": "who plays billy elliot", "prediction": [["film.film.country", "people.person.nationality"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "ground_paths": [["award.award_honor.honored_for", "award.award_honor.award_winner"], ["media_common.netflix_title.netflix_genres", "people.person.nationality"], ["film.film.language", "people.person.languages"], ["film.performance.film", "film.performance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_honor.honored_for", "award.award_winner.awards_won"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays billy elliot [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.02996758185327053, -0.03450050577521324, -0.03668500855565071], "norm_scores": [0.33458441495895386, 0.33307120203971863, 0.3323443830013275]}}
{"id": "WebQTest-1225", "question": "what kourtney kardashian boyfriends name", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["people.person.parents", "people.person.children"], ["people.person.children", "people.person.parents"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kourtney kardashian boyfriends name [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.020060474053025246, -0.023417191579937935, -0.024616165086627007], "norm_scores": [0.3342130184173584, 0.3330930471420288, 0.3326939344406128]}}
{"id": "WebQTest-1226", "question": "what form of government does australia use", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["type.type.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does australia use [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> type.type.form_of_government </PATH>"], "scores": [-0.007412636186927557, -0.018422791734337807, -0.15873613953590393], "norm_scores": [0.35104718804359436, 0.3472033143043518, 0.30174949765205383]}}
{"id": "WebQTest-1227", "question": "what team did howie long", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did howie long [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.professional_sports_team.draft_picks </PATH>"], "scores": [-0.011663872748613358, -0.013140080496668816, -0.05043855682015419], "norm_scores": [0.3377816379070282, 0.3372833728790283, 0.3249349594116211]}}
{"id": "WebQTest-1228", "question": "where did the greek alphabet come from", "prediction": [["language.human_language.language_family"], ["language.language_family.languages"], ["language.human_language.writing_system"]], "ground_paths": [["language.language_writing_system.child_writing_systems"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the greek alphabet come from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> language.language_family.languages </PATH>", "<PATH> language.human_language.writing_system </PATH>"], "scores": [-0.02064201980829239, -0.021854713559150696, -0.045212145894765854], "norm_scores": [0.33618900179862976, 0.3357815444469452, 0.32802942395210266]}}
{"id": "WebQTest-1229", "question": "what timezone is elizabethtown kentucky", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is elizabethtown kentucky [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-9.282492101192474e-06, -0.17697635293006897, -0.1806044727563858], "norm_scores": [0.3741702139377594, 0.3134825527667999, 0.31234726309776306]}}
{"id": "WebQTest-1230", "question": "what to see in christchurch new zealand", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to see in christchurch new zealand [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.012579814530909061, -0.012629753910005093, -0.0458429753780365], "norm_scores": [0.33701419830322266, 0.3369973599910736, 0.32598841190338135]}}
{"id": "WebQTest-1231", "question": "where is chris paul from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is chris paul from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.010658817365765572, -0.01703047752380371, -0.0689687430858612], "norm_scores": [0.34048280119895935, 0.338320255279541, 0.3211970031261444]}}
{"id": "WebQTest-1232", "question": "what year was albert pujols rookie year", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.season"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"]], "ground_paths": [["sports.sports_award.award_winner", "sports.sports_league_season.awards"], ["baseball.lifetime_batting_statistics.player", "baseball.lifetime_batting_statistics.starting_season"], ["baseball.batting_statistics.player", "baseball.batting_statistics.season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was albert pujols rookie year [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.season </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>"], "scores": [-0.013121694326400757, -0.013164649717509747, -0.050649650394916534], "norm_scores": [0.3374815583229065, 0.3374670743942261, 0.32505130767822266]}}
{"id": "WebQTest-1233", "question": "what are the colors of the ny giants", "prediction": [["sports.sports_team.colors"], ["sports.professional_sports_team.owner_s", "sports.sports_team.colors"], ["sports.professional_sports_team.colors"]], "ground_paths": [["sports.sports_team.colors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the colors of the ny giants [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.colors </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.colors </PATH>", "<PATH> sports.professional_sports_team.colors </PATH>"], "scores": [-4.3582993384916335e-05, -0.10725932568311691, -0.1405966579914093], "norm_scores": [0.3613748848438263, 0.3246345520019531, 0.3139905333518982]}}
{"id": "WebQTest-1234", "question": "what business did andrew carnegie dominate", "prediction": [["people.person.profession"], ["book.written_work.subjects"], ["book.written_work.author"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what business did andrew carnegie dominate [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.010515699163079262, -0.043301526457071304, -0.049617886543273926], "norm_scores": [0.34136703610420227, 0.33035650849342346, 0.32827645540237427]}}
{"id": "WebQTest-1235", "question": "what was sir isaac newton 's inventions", "prediction": [["law.invention.inventor"], ["law.inventor.inventions"], ["symbols.namesake.named_after"]], "ground_paths": [["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was sir isaac newton 's inventions [/INST]", "raw_output": {"paths": ["<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.inventions </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.004314119927585125, -0.03151680529117584, -0.061449237167835236], "norm_scores": [0.3427438735961914, 0.3335460126399994, 0.3237101137638092]}}
{"id": "WebQTest-1237", "question": "who did shaq first play for", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"]], "ground_paths": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did shaq first play for [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>"], "scores": [-0.019064411520957947, -0.020505333319306374, -0.0221855491399765], "norm_scores": [0.33384034037590027, 0.33335962891578674, 0.3328000009059906]}}
{"id": "WebQTest-1238", "question": "who was the artist of mickey mouse", "prediction": [["fictional_universe.fictional_character.character_created_by"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["fictional_universe.fictional_character_creator.fictional_characters_created"], ["fictional_universe.fictional_character.character_created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the artist of mickey mouse [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.0009672435116954148, -0.07144904136657715, -0.07549624145030975], "norm_scores": [0.3496350347995758, 0.3258405327796936, 0.3245244324207306]}}
{"id": "WebQTest-1239", "question": "what is the zip code for concord", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for concord [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0001432895369362086, -0.10164634883403778, -0.13789089024066925], "norm_scores": [0.3603867292404175, 0.32560163736343384, 0.31401166319847107]}}
{"id": "WebQTest-1240", "question": "what football team is michael vick on", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what football team is michael vick on [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> american_football.football_player.rushing <SEP> american_football.player_rushing_statistics.team </PATH>"], "scores": [-0.019058218225836754, -0.023220408707857132, -0.037475522607564926], "norm_scores": [0.33584120869636536, 0.3344462811946869, 0.32971253991127014]}}
{"id": "WebQTest-1241", "question": "where does the columbia river start", "prediction": [["geography.lake.outflow"], ["geography.river.origin"], []], "ground_paths": [["geography.lake.outflow"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the columbia river start [/INST]", "raw_output": {"paths": ["<PATH> geography.lake.outflow </PATH>", "<PATH> geography.river.origin </PATH>", "<PATH> </PATH>"], "scores": [-0.007113147526979446, -0.029304256662726402, -0.06959450989961624], "norm_scores": [0.3427610993385315, 0.33523863554000854, 0.32200026512145996]}}
{"id": "WebQTest-1242", "question": "what are all the names of the harry potter books", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes"], ["book.book_character.appears_in_book"], ["fictional_universe.fictional_universe.works_set_here"]], "ground_paths": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes"], ["book.book_character.appears_in_book"], ["fictional_universe.fictional_universe.works_set_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are all the names of the harry potter books [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes </PATH>", "<PATH> book.book_character.appears_in_book </PATH>", "<PATH> fictional_universe.fictional_universe.works_set_here </PATH>"], "scores": [-0.008194705471396446, -0.026178399100899696, -0.031493425369262695], "norm_scores": [0.3379351496696472, 0.3319121301174164, 0.330152690410614]}}
{"id": "WebQTest-1244", "question": "who was sir walter raleigh and what did he do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["fictional_universe.fictional_character.occupation"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was sir walter raleigh and what did he do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>"], "scores": [-0.0008447518339380622, -0.060391414910554886, -0.07653427869081497], "norm_scores": [0.34851759672164917, 0.3283703625202179, 0.3231120705604553]}}
{"id": "WebQTest-1245", "question": "what countries are in the euro dollar", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"]], "ground_paths": [["location.country.currency_used"], ["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are in the euro dollar [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>"], "scores": [-0.011561025865375996, -0.018631462007761, -0.048773933202028275], "norm_scores": [0.33824628591537476, 0.3358631730079651, 0.32589051127433777]}}
{"id": "WebQTest-1246", "question": "what currency can you use in barbados", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency can you use in barbados [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>"], "scores": [-0.011846957728266716, -0.013952485285699368, -0.13814307749271393], "norm_scores": [0.3473125994205475, 0.3465820848941803, 0.30610528588294983]}}
{"id": "WebQTest-1247", "question": "who is angelina jolie", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.person.nationality"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is angelina jolie [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.001951757469214499, -0.051889438182115555, -0.0936761274933815], "norm_scores": [0.34920525550842285, 0.3321950137615204, 0.31859973073005676]}}
{"id": "WebQTest-1248", "question": "where is nineveh city", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is nineveh city [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.0007736706756986678, -0.07405004650354385, -0.10205230116844177], "norm_scores": [0.3529795706272125, 0.32803940773010254, 0.31898099184036255]}}
{"id": "WebQTest-1249", "question": "what is the state flower of new mexico", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"]], "ground_paths": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the state flower of new mexico [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.013482088223099709, -0.01686934381723404, -0.02241203561425209], "norm_scores": [0.3347024619579315, 0.3335706293582916, 0.33172690868377686]}}
{"id": "WebQTest-1250", "question": "where is the national ffa foundation located today", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the national ffa foundation located today [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>"], "scores": [-0.007725215516984463, -0.03532179072499275, -0.041326697915792465], "norm_scores": [0.3401664197444916, 0.33090731501579285, 0.3289262354373932]}}
{"id": "WebQTest-1251", "question": "which countries speak germanic languages", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries speak germanic languages [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.015118088573217392, -0.030376754701137543, -0.03510957211256027], "norm_scores": [0.3372608423233032, 0.3321537375450134, 0.33058544993400574]}}
{"id": "WebQTest-1252", "question": "who plays eowyn in lord of the rings", "prediction": [["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.film.starring", "film.performance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_honor.honored_for", "award.award_winner.awards_won"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_honor.honored_for", "award.award_honor.award_winner"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays eowyn in lord of the rings [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.0333564355969429, -0.033778149634599686, -0.03461796045303345], "norm_scores": [0.3335203528404236, 0.33337974548339844, 0.333099901676178]}}
{"id": "WebQTest-1254", "question": "where is augusta masters 2012", "prediction": [["time.event.locations"], ["time.recurring_event.instances"], ["location.location.events"]], "ground_paths": [["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is augusta masters 2012 [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.0021693555172532797, -0.05201336741447449, -0.06811174005270004], "norm_scores": [0.34631282091140747, 0.3294743001461029, 0.32421281933784485]}}
{"id": "WebQTest-1255", "question": "what type of cancer did eva peron die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["medicine.disease.notable_people_with_this_condition"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of cancer did eva peron die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>"], "scores": [-0.008055605925619602, -0.020414965227246284, -0.037342410534620285], "norm_scores": [0.3379686176776886, 0.3338172435760498, 0.3282141089439392]}}
{"id": "WebQTest-1256", "question": "who did terrell owens play for in 2010", "prediction": [["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"]], "ground_paths": [["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did terrell owens play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>"], "scores": [-0.018549714237451553, -0.02206602320075035, -0.022066878154873848], "norm_scores": [0.33411526679992676, 0.33294248580932617, 0.3329421877861023]}}
{"id": "WebQTest-1257", "question": "where is denmark south carolina", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.location.containedby"], ["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is denmark south carolina [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.013404558412730694, -0.021127652376890182, -0.0514402836561203], "norm_scores": [0.3384111821651459, 0.3358076512813568, 0.3257811963558197]}}
{"id": "WebQTest-1259", "question": "what books did ernest hemingway write", "prediction": [["book.written_work.author"], ["book.book_edition.author_editor"], ["book.author.works_written"]], "ground_paths": [["film.film_story_contributor.film_story_credits"], ["film.film.story_by"], ["book.book_edition.author_editor"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what books did ernest hemingway write [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.016655277460813522, -0.022446805611252785, -0.027381591498851776], "norm_scores": [0.3351704776287079, 0.333234965801239, 0.3315945863723755]}}
{"id": "WebQTest-1260", "question": "what year did baltimore ravens win super bowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did baltimore ravens win super bowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.012757092714309692, -0.01383485272526741, -0.03910069167613983], "norm_scores": [0.33636924624443054, 0.33600693941116333, 0.32762378454208374]}}
{"id": "WebQTest-1261", "question": "where did the battle of passchendaele happen", "prediction": [["location.location.events"], ["time.event.locations"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["time.event.locations"], ["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the battle of passchendaele happen [/INST]", "raw_output": {"paths": ["<PATH> location.location.events </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.013152997009456158, -0.013193868100643158, -0.09113559126853943], "norm_scores": [0.34188738465309143, 0.3418734073638916, 0.31623920798301697]}}
{"id": "WebQTest-1262", "question": "what part of the country is ohio considered", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part of the country is ohio considered [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.012498184107244015, -0.01576531119644642, -0.08120622485876083], "norm_scores": [0.34125763177871704, 0.3401445150375366, 0.31859785318374634]}}
{"id": "WebQTest-1263", "question": "what team is cristiano ronaldo play for", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"]], "ground_paths": [["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.transfers", "soccer.football_player_transfer.purchasing_team"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is cristiano ronaldo play for [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>"], "scores": [-0.02123991772532463, -0.024930719286203384, -0.02516547031700611], "norm_scores": [0.33418014645576477, 0.33294904232025146, 0.33287087082862854]}}
{"id": "WebQTest-1264", "question": "what standard time is michigan", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_Zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what standard time is michigan [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_Zones </PATH>"], "scores": [-1.5565568901365623e-05, -0.1420767903327942, -0.1890474259853363], "norm_scores": [0.3710123300552368, 0.32187846302986145, 0.3071092367172241]}}
{"id": "WebQTest-1265", "question": "what was john tyler", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was john tyler [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.0033447782043367624, -0.04215098172426224, -0.049202773720026016], "norm_scores": [0.342804491519928, 0.32975634932518005, 0.32743915915489197]}}
{"id": "WebQTest-1266", "question": "what did romo do", "prediction": [["american_football.football_player.position_s"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.position"], ["sports.sports_team_roster.player", "sports.sports_team_roster.position"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did romo do [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.position_s </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.position </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.position </PATH>"], "scores": [-0.013962013646960258, -0.03903546929359436, -0.05127107724547386], "norm_scores": [0.3402961492538452, 0.33186984062194824, 0.32783395051956177]}}
{"id": "WebQTest-1267", "question": "what currency can you use in jamaica", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency can you use in jamaica [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.0006729611195623875, -0.06144280731678009, -0.16218824684619904], "norm_scores": [0.35817989706993103, 0.3370615541934967, 0.30475860834121704]}}
{"id": "WebQTest-1268", "question": "where did jay moriarty die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did jay moriarty die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.00026532329502515495, -0.097686268389225, -0.10688213258981705], "norm_scores": [0.3563735783100128, 0.32329288125038147, 0.3203335106372833]}}
{"id": "WebQTest-1269", "question": "where was the boston tea party 1773", "prediction": [["time.event.locations"], ["location.location.events"], ["event.disaster.areas_affected"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the boston tea party 1773 [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> event.disaster.areas_affected </PATH>"], "scores": [-0.006562213413417339, -0.025461209937930107, -0.06414421647787094], "norm_scores": [0.3418425917625427, 0.33544275164604187, 0.322714626789093]}}
{"id": "WebQTest-1270", "question": "what drugs lindsay lohan abuse", "prediction": [["base.popstra.substance_abuse.abuser", "base.popstra.substance_abuse.substance"], ["base.popstra.celebrity.substance_abuse", "base.popstra.substance_abuse.substance"], ["people.person.substance_abuse", "people.substance_abuse.substance"]], "ground_paths": [["celebrities.celebrity.substance_abuse_problems", "celebrities.abused_substance.abused_by"], ["base.popstra.rehab_stay.patient", "base.popstra.rehab_stay.substance_abused"], ["base.popstra.celebrity.substance_abuse", "base.popstra.substance_abuse.substance"], ["base.popstra.substance_abuse.abuser", "base.popstra.substance_abuse.substance"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what drugs lindsay lohan abuse [/INST]", "raw_output": {"paths": ["<PATH> base.popstra.substance_abuse.abuser <SEP> base.popstra.substance_abuse.substance </PATH>", "<PATH> base.popstra.celebrity.substance_abuse <SEP> base.popstra.substance_abuse.substance </PATH>", "<PATH> people.person.substance_abuse <SEP> people.substance_abuse.substance </PATH>"], "scores": [-0.006938083562999964, -0.025249715894460678, -0.05377895385026932], "norm_scores": [0.3405885100364685, 0.33440855145454407, 0.3250029385089874]}}
{"id": "WebQTest-1271", "question": "when did carolina panthers go to superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["tv.tv_actor.guest_roles", "tv.tv_guest_role.episodes_appeared_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did carolina panthers go to superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.009583383798599243, -0.018168842419981956, -0.038441579788923264], "norm_scores": [0.3374951481819153, 0.3346100151538849, 0.32789483666419983]}}
{"id": "WebQTest-1272", "question": "when did the burma cyclone happen", "prediction": [["meteorology.tropical_cyclone.affected_areas"], ["meteorology.cyclone_affected_area.cyclones"], ["time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the burma cyclone happen [/INST]", "raw_output": {"paths": ["<PATH> meteorology.tropical_cyclone.affected_areas </PATH>", "<PATH> meteorology.cyclone_affected_area.cyclones </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.008300567977130413, -0.02090052329003811, -0.049754999577999115], "norm_scores": [0.3393428325653076, 0.33509397506713867, 0.3255631625652313]}}
{"id": "WebQTest-1274", "question": "who won the governor election in texas", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "ground_paths": [["location.us_state.capital", "people.deceased_person.place_of_death"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["film.film_location.featured_in_films", "film.film.subjects"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["location.location.containedby", "symbols.name_source.namesakes"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.political_appointer.appointees"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.appointed_by"], ["location.administrative_division.country", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["people.deceased_person.place_of_death"], ["base.biblioness.bibs_location.state", "symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won the governor election in texas [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.024451155215501785, -0.02449936792254448, -0.024847403168678284], "norm_scores": [0.3333827257156372, 0.33336663246154785, 0.33325064182281494]}}
{"id": "WebQTest-1275", "question": "what does helen flanagan do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does helen flanagan do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0010688103502616286, -0.058094799518585205, -0.09847141057252884], "norm_scores": [0.3506605923175812, 0.33122333884239197, 0.31811603903770447]}}
{"id": "WebQTest-1276", "question": "when are the summer and winter olympics held", "prediction": [["time.recurring_event.instances"], ["time.event.instance_of_recurring_event"], ["time.recurring_event.current_frequency"]], "ground_paths": [["time.recurring_event.current_frequency"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when are the summer and winter olympics held [/INST]", "raw_output": {"paths": ["<PATH> time.recurring_event.instances </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>"], "scores": [-0.008635829202830791, -0.017423691228032112, -0.08646545559167862], "norm_scores": [0.3428916931152344, 0.33989161252975464, 0.3172166645526886]}}
{"id": "WebQTest-1277", "question": "where is tennessee river", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["geography.river.basin_countries"]], "ground_paths": [["location.location.partially_contains"], ["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is tennessee river [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> geography.river.basin_countries </PATH>"], "scores": [-0.004700867459177971, -0.03426438942551613, -0.0752505362033844], "norm_scores": [0.3445008397102356, 0.33446526527404785, 0.32103395462036133]}}
{"id": "WebQTest-1278", "question": "what are some major events in venezuela", "prediction": [["time.event.locations"], ["base.culturalevent.event.entity_involved"], ["location.location.events"]], "ground_paths": [["base.culturalevent.event.entity_involved"], ["time.event.locations"], ["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some major events in venezuela [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.016525961458683014, -0.022248201072216034, -0.024740582332015038], "norm_scores": [0.33488351106643677, 0.3329727053642273, 0.3321438133716583]}}
{"id": "WebQTest-1279", "question": "what guitar does corey taylor play", "prediction": [["music.group_member.instruments_played"], ["music.instrument.family"], ["music.guitarist.guitars_played"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what guitar does corey taylor play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.guitarist.guitars_played </PATH>"], "scores": [-0.007826302200555801, -0.032068703323602676, -0.05181070417165756], "norm_scores": [0.3409457504749298, 0.33277976512908936, 0.32627448439598083]}}
{"id": "WebQTest-1280", "question": "who was president after franklin d. roosevelt", "prediction": [["government.us_president.vice_president"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was president after franklin d. roosevelt [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.019186915829777718, -0.023794589564204216, -0.03603232279419899], "norm_scores": [0.3357170522212982, 0.3341737389564514, 0.3301091492176056]}}
{"id": "WebQTest-1281", "question": "where did kate gosselin grew up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kate gosselin grew up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0022728152107447386, -0.03970174863934517, -0.10003383457660675], "norm_scores": [0.3484164774417877, 0.3356166481971741, 0.3159669041633606]}}
{"id": "WebQTest-1282", "question": "under what political system does the united states government operate", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["government.form_of_government.countries", "location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: under what political system does the united states government operate [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.form_of_government </PATH>"], "scores": [-0.0047507318668067455, -0.023261407390236855, -0.12757177650928497], "norm_scores": [0.3489084243774414, 0.34250929951667786, 0.30858224630355835]}}
{"id": "WebQTest-1284", "question": "what to do in paris in 1 day", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["base.arthist.helynevek.intezmeny"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do in paris in 1 day [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.008488924242556095, -0.020259210839867592, -0.03937368467450142], "norm_scores": [0.3380793035030365, 0.3341233432292938, 0.32779738306999207]}}
{"id": "WebQTest-1285", "question": "what city is mt lassen in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.lake.outflow"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city is mt lassen in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.lake.outflow </PATH>"], "scores": [-0.005496051162481308, -0.030267447233200073, -0.11785956472158432], "norm_scores": [0.3485228717327118, 0.33999550342559814, 0.31148162484169006]}}
{"id": "WebQTest-1287", "question": "what is the state motto of ohio", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.location_symbol_relationship.symbol"]], "ground_paths": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the state motto of ohio [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.017288630828261375, -0.019209444522857666, -0.023792479187250137], "norm_scores": [0.3342694640159607, 0.3336280286312103, 0.332102507352829]}}
{"id": "WebQTest-1289", "question": "what type of music did mozart composed", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["music.composition.composer"]], "ground_paths": [["music.artist.genre"], ["music.genre.artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did mozart composed [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> music.composition.composer </PATH>"], "scores": [-0.005491834133863449, -0.030635854229331017, -0.06739804893732071], "norm_scores": [0.3430366516113281, 0.3345188796520233, 0.32244452834129333]}}
{"id": "WebQTest-1291", "question": "how many kids were there in the kennedy family", "prediction": [["people.person.parents"], ["people.person.children"], ["people.family.members"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how many kids were there in the kennedy family [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.family.members </PATH>"], "scores": [-0.009635347872972488, -0.0170473400503397, -0.08599653840065002], "norm_scores": [0.3425717055797577, 0.3400419354438782, 0.31738632917404175]}}
{"id": "WebQTest-1292", "question": "which barcelona airport is closest to the port", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["travel.transportation.mode_of_transportation", "travel.transport_terminus.travel_destinations_served"]], "ground_paths": [["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which barcelona airport is closest to the port [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transport_terminus.travel_destinations_served </PATH>"], "scores": [-0.006091475952416658, -0.02925012819468975, -0.04981176182627678], "norm_scores": [0.3407933712005615, 0.33299174904823303, 0.32621482014656067]}}
{"id": "WebQTest-1293", "question": "where has the english language evolved from", "prediction": [["language.human_language.language_family"], ["base.rosetta.languoid.parent"], ["base.rosetta.languoid.child"]], "ground_paths": [["language.human_language.language_family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where has the english language evolved from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>", "<PATH> base.rosetta.languoid.child </PATH>"], "scores": [-0.00604636874049902, -0.02585398405790329, -0.06349121779203415], "norm_scores": [0.34193164110183716, 0.3352254033088684, 0.32284295558929443]}}
{"id": "WebQTest-1294", "question": "what country did hitler invade that started ww2", "prediction": [["base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did hitler invade that started ww2 [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>"], "scores": [-0.028845788910984993, -0.03135557100176811, -0.03694740682840347], "norm_scores": [0.3345125615596771, 0.33367404341697693, 0.33181339502334595]}}
{"id": "WebQTest-1295", "question": "what does matt damon play in", "prediction": [["people.person.languages", "film.film.language"], ["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["film.actor.film", "film.film.starring"], ["film.person_or_entity_appearing_in_film.films", "film.film.personal_appearances"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.producer.film"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["music.recording.featured_artists", "music.recording.releases"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["people.person.languages", "film.film.language"], ["film.film.written_by"], ["film.performance.actor", "film.film.starring"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.languages", "tv.tv_program.languages"], ["tv.tv_program.program_creator"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does matt damon play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.022369351238012314, -0.02245582453906536, -0.04483823478221893], "norm_scores": [0.3358302116394043, 0.3358011543750763, 0.328368604183197]}}
{"id": "WebQTest-1296", "question": "what was scottie pippen known for", "prediction": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.player"], ["basketball.basketball_player.position_s"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.player"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was scottie pippen known for [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.player </PATH>", "<PATH> basketball.basketball_player.position_s </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.player </PATH>"], "scores": [-0.015511825680732727, -0.027006246149539948, -0.033095087856054306], "norm_scores": [0.33657094836235046, 0.3327244222164154, 0.3307046592235565]}}
{"id": "WebQTest-1297", "question": "what continent does canada belong to", "prediction": [["base.locations.countries.continent"], ["location.location.containedby"], ["base.locations.continents.countries_within"]], "ground_paths": [["base.locations.continents.countries_within"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what continent does canada belong to [/INST]", "raw_output": {"paths": ["<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.01683715358376503, -0.019320711493492126, -0.02751222811639309], "norm_scores": [0.33479511737823486, 0.33396467566490173, 0.331240177154541]}}
{"id": "WebQTest-1298", "question": "what does george wilson do for a living", "prediction": [["people.person.profession"], ["fictional_universe.fictional_character.occupation"], ["fictional_universe.character_occupation.characters_with_this_occupation"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does george wilson do for a living [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>"], "scores": [-0.004821698646992445, -0.03179093822836876, -0.067035973072052], "norm_scores": [0.3432801365852356, 0.3341458737850189, 0.3225739896297455]}}
{"id": "WebQTest-1300", "question": "what city does fulham play in", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.location"], ["location.location.containedby"]], "ground_paths": [["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city does fulham play in [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0236526932567358, -0.023806557059288025, -0.028246071189641953], "norm_scores": [0.33386045694351196, 0.33380910754203796, 0.3323304355144501]}}
{"id": "WebQTest-1301", "question": "what are fun things to do in sacramento ca", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are fun things to do in sacramento ca [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.014837812632322311, -0.016586991026997566, -0.026130633428692818], "norm_scores": [0.3347814679145813, 0.33419641852378845, 0.33102211356163025]}}
{"id": "WebQTest-1303", "question": "what other movies is josh hutcherson in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["film.actor.film", "film.film.starring"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.performance.film"], ["film.producer.films_executive_produced"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other movies is josh hutcherson in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.017570747062563896, -0.03480282053351402, -0.03770698979496956], "norm_scores": [0.3374979794025421, 0.33173200488090515, 0.33077001571655273]}}
{"id": "WebQTest-1304", "question": "what was hitler the leader of", "prediction": [["organization.organization_founder.organizations_founded"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"], ["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was hitler the leader of [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.017654770985245705, -0.035166170448064804, -0.04109089821577072], "norm_scores": [0.33789747953414917, 0.3320319354534149, 0.3300705552101135]}}
{"id": "WebQTest-1305", "question": "what awards did marilyn monroe won", "prediction": [["award.award_nomination.award_nominee", "award.award_nomination.award"], ["award.award_honor.award_winner", "award.award_honor.award"], ["award.award_winner.awards_won", "award.award_honor.award"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_category.nominees"], ["award.award_honor.award_winner", "award.award_honor.award"], ["award.award_nomination.award_nominee", "award.award_category.nominees"], ["award.award_winner.awards_won", "award.award_honor.award"], ["people.person.profession", "award.award_discipline.awards_in_this_discipline"], ["award.award_nominee.award_nominations", "award.award_nomination.award"], ["award.award_honor.award_winner", "award.award_category.winners"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what awards did marilyn monroe won [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_honor.award </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.award </PATH>"], "scores": [-0.01974320411682129, -0.02622164413332939, -0.02870766818523407], "norm_scores": [0.3350512683391571, 0.3328876495361328, 0.33206111192703247]}}
{"id": "WebQTest-1306", "question": "what kind of government does iran have after 1979", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does iran have after 1979 [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.004230333957821131, -0.023086067289114, -0.1380484700202942], "norm_scores": [0.35013148188591003, 0.3435913324356079, 0.30627721548080444]}}
{"id": "WebQTest-1307", "question": "where did harper lee attend high school", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did harper lee attend high school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.01574663817882538, -0.02157144621014595, -0.021861225366592407], "norm_scores": [0.33466124534606934, 0.33271756768226624, 0.33262115716934204]}}
{"id": "WebQTest-1308", "question": "what college did jrr tolkien go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["business.employment_tenure.person", "business.employment_tenure.company"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.employment_history", "business.employment_tenure.company"], ["people.person.nationality", "location.location.containedby"], ["business.employment_tenure.person", "business.employment_tenure.company"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did jrr tolkien go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.015671003609895706, -0.022316250950098038, -0.03066936507821083], "norm_scores": [0.3357405662536621, 0.3335168659687042, 0.33074256777763367]}}
{"id": "WebQTest-1309", "question": "what art did wassily kandinsky do", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"], ["book.author.school_or_movement"]], "ground_paths": [["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what art did wassily kandinsky do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> book.author.school_or_movement </PATH>"], "scores": [-0.00798586942255497, -0.027708152309060097, -0.046882130205631256], "norm_scores": [0.3398677110671997, 0.3332304060459137, 0.326901912689209]}}
{"id": "WebQTest-1310", "question": "who won the league cup in 2002", "prediction": [["sports.sports_award_type.winners", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship", "sports.sports_team.championships"]], "ground_paths": [["sports.sports_award.award", "sports.sports_award_winner.awards"], ["sports.sports_championship_event.championship", "sports.sports_team.championships"], ["sports.sports_championship.events", "sports.sports_team.championships"], ["time.event.instance_of_recurring_event", "sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won the league cup in 2002 [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_type.winners <SEP> sports.sports_award.award_winner </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_team.championships </PATH>"], "scores": [-0.021476849913597107, -0.023255709558725357, -0.02567031793296337], "norm_scores": [0.33399707078933716, 0.33340349793434143, 0.3325994312763214]}}
{"id": "WebQTest-1311", "question": "what was antoni gaudi inspired by", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was antoni gaudi inspired by [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.010385852307081223, -0.01364236418157816, -0.0752321258187294], "norm_scores": [0.34083622694015503, 0.33972808718681335, 0.31943562626838684]}}
{"id": "WebQTest-1312", "question": "in what country do people speak danish", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: in what country do people speak danish [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.010369103401899338, -0.019317111000418663, -0.04680489003658295], "norm_scores": [0.33837375044822693, 0.33535948395729065, 0.32626673579216003]}}
{"id": "WebQTest-1313", "question": "who were demeter 's siblings", "prediction": [["fictional_universe.fictional_character.siblings", "fictional_universe.fictional_character.siblings"], ["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.parents"]], "ground_paths": [["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.parents"], ["fictional_universe.fictional_character.children", "fictional_universe.fictional_character.parents"], ["fictional_universe.romantic_involvement.partner", "fictional_universe.fictional_character.romantically_involved_with"], ["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"], ["fictional_universe.fictional_character.siblings", "fictional_universe.fictional_character.siblings"], ["common.topic.notable_types", "fictional_universe.fictional_character.species"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were demeter 's siblings [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.siblings <SEP> fictional_universe.fictional_character.siblings </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.parents <SEP> fictional_universe.fictional_character.parents </PATH>"], "scores": [-0.02126675471663475, -0.02454322576522827, -0.027575163170695305], "norm_scores": [0.33439889550209045, 0.3333050608634949, 0.3322960138320923]}}
{"id": "WebQTest-1314", "question": "where should a family stay in paris", "prediction": [["travel.travel_destination.accommodation"], ["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["location.location.containedby"], ["travel.travel_destination.accommodation"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where should a family stay in paris [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.accommodation </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.003628823207691312, -0.03307518735527992, -0.09757158905267715], "norm_scores": [0.34706342220306396, 0.33699265122413635, 0.3159438967704773]}}
{"id": "WebQTest-1316", "question": "what was the name of benjamin franklin wife", "prediction": [["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.children"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.place_of_interment.interred_here", "people.place_of_interment.interred_here"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the name of benjamin franklin wife [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.027146609500050545, -0.03369816765189171, -0.04269924387335777], "norm_scores": [0.33579161763191223, 0.3335988521575928, 0.33060958981513977]}}
{"id": "WebQTest-1317", "question": "what the largest city in spain", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.aareas.schema.administrative_area.capital"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the largest city in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>"], "scores": [-0.02062646858394146, -0.026764988899230957, -0.04766322672367096], "norm_scores": [0.3370174169540405, 0.3349549472332001, 0.328027606010437]}}
{"id": "WebQTest-1319", "question": "who has pudge rodriguez played for", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has pudge rodriguez played for [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.011715540662407875, -0.017235973849892616, -0.029269298538565636], "norm_scores": [0.3358980119228363, 0.3340488076210022, 0.3300531804561615]}}
{"id": "WebQTest-1320", "question": "what is the milwaukee brewers stadium called", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], []], "ground_paths": [["sports.sports_facility.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the milwaukee brewers stadium called [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> </PATH>"], "scores": [-0.0028915659058839083, -0.0308853629976511, -0.11411447077989578], "norm_scores": [0.34878039360046387, 0.33915209770202637, 0.31206753849983215]}}
{"id": "WebQTest-1321", "question": "where did the gallipoli war take place", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the gallipoli war take place [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.007937891408801079, -0.021879341453313828, -0.09091329574584961], "norm_scores": [0.3440530002117157, 0.33928966522216797, 0.3166573643684387]}}
{"id": "WebQTest-1322", "question": "what other movies has tom hardy played in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["film.actor.film", "film.film.starring"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other movies has tom hardy played in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.02268960326910019, -0.035113375633955, -0.03839907795190811], "norm_scores": [0.3364662230014801, 0.3323119282722473, 0.3312218189239502]}}
{"id": "WebQTest-1323", "question": "what to visit in atlanta", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to visit in atlanta [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.015345483087003231, -0.018518129363656044, -0.026611678302288055], "norm_scores": [0.3349377512931824, 0.3338768184185028, 0.3311854600906372]}}
{"id": "WebQTest-1324", "question": "what college did john nash go to", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["business.employment_tenure.person", "business.employment_tenure.company"]], "ground_paths": [["education.education.student", "education.education.institution"], ["book.written_work.author", "education.dissertation.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.employment_history", "business.employment_tenure.company"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did john nash go to [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.01737270876765251, -0.017451005056500435, -0.03548021987080574], "norm_scores": [0.335347980260849, 0.33532172441482544, 0.32933032512664795]}}
{"id": "WebQTest-1325", "question": "what language do maltese speak", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do maltese speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.015405206941068172, -0.029854830354452133, -0.03335409238934517], "norm_scores": [0.33694249391555786, 0.3321087956428528, 0.33094871044158936]}}
{"id": "WebQTest-1326", "question": "what did thomas edison do for the world", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"]], "ground_paths": [["law.invention.inventor"], ["law.inventor.us_patents"], ["law.inventor.inventions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did thomas edison do for the world [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>"], "scores": [-0.013517347164452076, -0.02060667611658573, -0.036472514271736145], "norm_scores": [0.33667290210723877, 0.33429455757141113, 0.3290325403213501]}}
{"id": "WebQTest-1327", "question": "where is shakira from", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is shakira from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.011544395238161087, -0.01855594478547573, -0.059844501316547394], "norm_scores": [0.3394593596458435, 0.3370875418186188, 0.32345306873321533]}}
{"id": "WebQTest-1328", "question": "what did hayes do", "prediction": [["people.person.profession"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["common.topic.notable_types"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did hayes do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.006106555461883545, -0.046717945486307144, -0.04770589992403984], "norm_scores": [0.3425295352935791, 0.3288975954055786, 0.3285728394985199]}}
{"id": "WebQTest-1329", "question": "who played bilbo in the fellowship of the ring", "prediction": [["film.performance.character", "film.actor.film"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "ground_paths": [["film.film.starring", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played bilbo in the fellowship of the ring [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.035071130841970444, -0.04002917930483818, -0.04154300317168236], "norm_scores": [0.3346044421195984, 0.33294957876205444, 0.3324459493160248]}}
{"id": "WebQTest-1330", "question": "what is the most populated state in united states", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["location.administrative_division.first_level_division_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the most populated state in united states [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.008533845655620098, -0.027287013828754425, -0.03511485457420349], "norm_scores": [0.3383876383304596, 0.33210092782974243, 0.329511433839798]}}
{"id": "WebQTest-1331", "question": "who is my state senator arizona", "prediction": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "ground_paths": [["location.location.containedby", "people.deceased_person.place_of_death"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["base.aareas.schema.administrative_area.capital", "people.deceased_person.place_of_death"], ["location.location.containedby", "people.person.place_of_birth"], ["base.aareas.schema.administrative_area.capital", "people.person.place_of_birth"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["location.country.first_level_divisions", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is my state senator arizona [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.023099834099411964, -0.02671746537089348, -0.029465826228260994], "norm_scores": [0.3344433307647705, 0.33323562145233154, 0.33232101798057556]}}
{"id": "WebQTest-1332", "question": "when does the vietnam war end", "prediction": [["time.event.included_in_event"], ["time.event.includes_event"], ["time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when does the vietnam war end [/INST]", "raw_output": {"paths": ["<PATH> time.event.included_in_event </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.015318243764340878, -0.032765038311481476, -0.03314901143312454], "norm_scores": [0.3372645378112793, 0.33143138885498047, 0.331304132938385]}}
{"id": "WebQTest-1333", "question": "where does the tennessee river go", "prediction": [["geography.river.mouth"], ["location.location.partially_containedby"], ["location.location.containedby"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the tennessee river go [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.012491670437157154, -0.03341357782483101, -0.03727952018380165], "norm_scores": [0.3384309709072113, 0.33142393827438354, 0.33014512062072754]}}
{"id": "WebQTest-1334", "question": "who did tom hanks play in apollo 13", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["film.performance.actor", "film.film_character.portrayed_in_films"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["spaceflight.astronaut.missions"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"], ["people.person.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did tom hanks play in apollo 13 [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.023322373628616333, -0.028284216299653053, -0.031857457011938095], "norm_scores": [0.3348343074321747, 0.33317703008651733, 0.3319886326789856]}}
{"id": "WebQTest-1335", "question": "what language do they in ghana", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they in ghana [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.005501923151314259, -0.035058144479990005, -0.043063223361968994], "norm_scores": [0.3408302664756775, 0.3309040367603302, 0.3282656967639923]}}
{"id": "WebQTest-1336", "question": "what is utah 's state capitol", "prediction": [["base.aareas.schema.administrative_area.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.state"]], "ground_paths": [["base.biblioness.bibs_location.city"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is utah 's state capitol [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>"], "scores": [-0.020010514184832573, -0.025103859603405, -0.030022570863366127], "norm_scores": [0.33501315116882324, 0.33331114053726196, 0.3316757082939148]}}
{"id": "WebQTest-1337", "question": "what all did ben franklin invent", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["law.inventor.us_patents"]], "ground_paths": [["law.invention.inventor"], ["law.inventor.inventions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what all did ben franklin invent [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.us_patents </PATH>"], "scores": [-0.006573089398443699, -0.025504162535071373, -0.07861677557229996], "norm_scores": [0.3434375822544098, 0.3369970917701721, 0.3195652961730957]}}
{"id": "WebQTest-1339", "question": "what team does jeremy lin play for 2013", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team.roster"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does jeremy lin play for 2013 [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.014528006315231323, -0.018030039966106415, -0.022315870970487595], "norm_scores": [0.3345884382724762, 0.33341875672340393, 0.3319928050041199]}}
{"id": "WebQTest-1340", "question": "what is steve nash nationality", "prediction": [["people.person.nationality"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.country"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.country"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is steve nash nationality [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.country </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.country </PATH>"], "scores": [-0.0021615452133119106, -0.05677061527967453, -0.05959033966064453], "norm_scores": [0.34589576721191406, 0.3275132179260254, 0.32659101486206055]}}
{"id": "WebQTest-1341", "question": "who are the u s senators of pennsylvania", "prediction": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "ground_paths": [["location.location.containedby", "people.deceased_person.place_of_death"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.administrative_children", "people.deceased_person.place_of_burial"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["base.aareas.schema.administrative_area.administrative_children", "people.deceased_person.place_of_death"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.place_of_birth"], ["location.location.containedby", "people.person.place_of_birth"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["base.aareas.schema.administrative_area.administrative_children", "people.person.place_of_birth"], ["location.location.containedby", "people.person.nationality"], ["location.location.containedby", "people.deceased_person.place_of_burial"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.administrative_parent", "people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the u s senators of pennsylvania [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.023601941764354706, -0.028117643669247627, -0.028251294046640396], "norm_scores": [0.3343524634838104, 0.3328460454940796, 0.33280155062675476]}}
{"id": "WebQTest-1342", "question": "what kind of language does france speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language does france speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.014119233936071396, -0.02178824134171009, -0.027357611805200577], "norm_scores": [0.33565953373908997, 0.3330952227115631, 0.3312452435493469]}}
{"id": "WebQTest-1343", "question": "what degree did martin luther king earn", "prediction": [["education.education.student", "education.education.degree"], ["people.person.education", "education.education.degree"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.degree"], ["people.person.education", "education.education.degree"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what degree did martin luther king earn [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.004890772979706526, -0.023142732679843903, -0.05251698195934296], "norm_scores": [0.34066861867904663, 0.33450716733932495, 0.32482418417930603]}}
{"id": "WebQTest-1344", "question": "what two states is st louis in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.location.time_zones"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what two states is st louis in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.time_zones </PATH>"], "scores": [-0.0035892906598746777, -0.057694483548402786, -0.060749076306819916], "norm_scores": [0.3458082973957062, 0.32759544253349304, 0.3265962600708008]}}
{"id": "WebQTest-1348", "question": "what team did peyton manning 's dad play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did peyton manning 's dad play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.008441083133220673, -0.016336457803845406, -0.038261670619249344], "norm_scores": [0.3375236392021179, 0.33486926555633545, 0.32760709524154663]}}
{"id": "WebQTest-1350", "question": "what did jesse owens won", "prediction": [["olympics.olympic_athlete.medals_won", "olympics.olympic_medal_honor.event"], ["olympics.olympic_medal_honor.medalist", "olympics.olympic_medal_honor.event"], ["olympics.olympic_athlete.medals_won", "olympics.olympic_event_competition.medalists"]], "ground_paths": [["award.award_honor.award_winner", "award.award_category.winners"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did jesse owens won [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_medal_honor.event </PATH>", "<PATH> olympics.olympic_medal_honor.medalist <SEP> olympics.olympic_medal_honor.event </PATH>", "<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_event_competition.medalists </PATH>"], "scores": [-0.018899070098996162, -0.020715631544589996, -0.03864218294620514], "norm_scores": [0.3357241749763489, 0.33511486649513245, 0.3291609585285187]}}
{"id": "WebQTest-1351", "question": "what is the name of the currency used in peru", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the currency used in peru [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.009200927801430225, -0.016304094344377518, -0.11731237918138504], "norm_scores": [0.3459669351577759, 0.3435181975364685, 0.3105148673057556]}}
{"id": "WebQTest-1353", "question": "what are the four nations of the united kingdom", "prediction": [["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"]], "ground_paths": [["base.biblioness.bibs_location.country"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the four nations of the united kingdom [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-0.011062802746891975, -0.02300833724439144, -0.024598926305770874], "norm_scores": [0.3361705243587494, 0.33217868208885193, 0.3316507637500763]}}
{"id": "WebQTest-1354", "question": "who is judy garland father", "prediction": [["people.person.parents"], ["people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is judy garland father [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.007420671638101339, -0.02415495738387108, -0.09835967421531677], "norm_scores": [0.345246821641922, 0.3395174443721771, 0.3152357339859009]}}
{"id": "WebQTest-1355", "question": "what school did robert f kennedy go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did robert f kennedy go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.015140267089009285, -0.02153983898460865, -0.025830240920186043], "norm_scores": [0.3352343440055847, 0.33309584856033325, 0.33166980743408203]}}
{"id": "WebQTest-1356", "question": "what drug killed marilyn monroe", "prediction": [["people.cause_of_death.people"], ["people.deceased_person.cause_of_death"], ["medicine.disease.parent_disease"]], "ground_paths": [["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what drug killed marilyn monroe [/INST]", "raw_output": {"paths": ["<PATH> people.cause_of_death.people </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.disease.parent_disease </PATH>"], "scores": [-0.0070082880556583405, -0.02369762398302555, -0.05725366249680519], "norm_scores": [0.3407799303531647, 0.3351397216320038, 0.32408034801483154]}}
{"id": "WebQTest-1357", "question": "where is the fukushima daiichi nuclear power station", "prediction": [["location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the fukushima daiichi nuclear power station [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.005134824197739363, -0.04173314943909645, -0.04463503137230873], "norm_scores": [0.3418414294719696, 0.32955676317214966, 0.32860180735588074]}}
{"id": "WebQTest-1358", "question": "what is los angeles california time zone", "prediction": [["location.location.time_zones"], ["location.location.containedby"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is los angeles california time zone [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-1.1358567462593783e-05, -0.16325590014457703, -0.1836051046848297], "norm_scores": [0.3729037344455719, 0.3167382478713989, 0.3103580176830292]}}
{"id": "WebQTest-1359", "question": "what did nicolas cage name his son", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.gender"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did nicolas cage name his son [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.011455334722995758, -0.015580946579575539, -0.11926552653312683], "norm_scores": [0.3455805778503418, 0.3441578149795532, 0.3102615475654602]}}
{"id": "WebQTest-1361", "question": "what currency should take to mexico", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should take to mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>"], "scores": [-0.007786763831973076, -0.032584208995103836, -0.0491759218275547], "norm_scores": [0.3407197594642639, 0.3323746919631958, 0.3269055485725403]}}
{"id": "WebQTest-1362", "question": "who is the state governor of florida", "prediction": [["location.administrative_division.first_level_division_of", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"], ["location.administrative_division.first_level_division_of", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the state governor of florida [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.first_level_division_of <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.01453584898263216, -0.022850435227155685, -0.02479695715010166], "norm_scores": [0.33540037274360657, 0.3326232135295868, 0.33197638392448425]}}
{"id": "WebQTest-1363", "question": "who was jessica simpson married too", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.marriage.spouse", "people.marriage.spouse"]], "ground_paths": [["people.person.parents", "people.person.children"], ["broadcast.artist.content", "broadcast.artist.content"], ["base.popstra.celebrity.infidelity_participant", "base.popstra.celebrity.infidelity_victim"], ["music.artist.genre", "music.artist.genre"], ["people.person.languages", "people.person.languages"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["music.artist.album", "music.album.featured_artists"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"], ["common.topic.notable_types", "common.topic.notable_types"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"], ["base.saturdaynightlive.snl_episode.host", "base.saturdaynightlive.snl_host.episodes_hosted"], ["people.person.spouse_s", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was jessica simpson married too [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>"], "scores": [-0.02322905696928501, -0.032423850148916245, -0.043610554188489914], "norm_scores": [0.3366241455078125, 0.3335431218147278, 0.32983270287513733]}}
{"id": "WebQTest-1364", "question": "what do they speak in scotland", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do they speak in scotland [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.014866904355585575, -0.015569846145808697, -0.03814329206943512], "norm_scores": [0.3359887897968292, 0.3357527256011963, 0.3282585144042969]}}
{"id": "WebQTest-1365", "question": "what did john dalton study", "prediction": [["people.person.profession"], ["people.person.education", "education.education.major_field_of_study"], ["education.education.student", "education.education.major_field_of_study"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did john dalton study [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>"], "scores": [-0.012753449380397797, -0.024069422855973244, -0.034558624029159546], "norm_scores": [0.33702048659324646, 0.33322829008102417, 0.32975122332572937]}}
{"id": "WebQTest-1366", "question": "what songs do zac brown band sing", "prediction": [["music.recording.artist"], ["music.artist.album"], ["music.album.artist"]], "ground_paths": [["music.featured_artist.recordings"], ["music.music_video.artist"], ["music.recording.featured_artists"], ["music.album.artist"], ["music.artist.album"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["music.recording.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what songs do zac brown band sing [/INST]", "raw_output": {"paths": ["<PATH> music.recording.artist </PATH>", "<PATH> music.artist.album </PATH>", "<PATH> music.album.artist </PATH>"], "scores": [-0.014251639135181904, -0.029628032818436623, -0.05210436135530472], "norm_scores": [0.3392595052719116, 0.33408284187316895, 0.32665762305259705]}}
{"id": "WebQTest-1367", "question": "where is glastonbury england", "prediction": [["location.location.containedby"], ["location.location.contains"], ["time.event.locations"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is glastonbury england [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.011339711025357246, -0.0171064343303442, -0.09360789507627487], "norm_scores": [0.34302085638046265, 0.3410484492778778, 0.31593072414398193]}}
{"id": "WebQTest-1368", "question": "who formed the american federation of labor", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["organization.organization.geographic_scope", "people.person.nationality"]], "ground_paths": [["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who formed the american federation of labor [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.geographic_scope <SEP> people.person.nationality </PATH>"], "scores": [-0.0015613535651937127, -0.04967993497848511, -0.0777784138917923], "norm_scores": [0.34726613759994507, 0.33095183968544006, 0.32178202271461487]}}
{"id": "WebQTest-1369", "question": "what to visit in denver", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to visit in denver [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.013655207119882107, -0.02385803498327732, -0.02440362609922886], "norm_scores": [0.3356652855873108, 0.3322579562664032, 0.3320767283439636]}}
{"id": "WebQTest-1370", "question": "where did thomas hobbes live", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth"], ["people.deceased_person.place_of_death"]], "ground_paths": [["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["people.person.places_lived", "people.place_lived.location"], ["influence.influence_node.influenced", "people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did thomas hobbes live [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>"], "scores": [-0.02608058601617813, -0.03163589537143707, -0.03206874802708626], "norm_scores": [0.33461716771125793, 0.3327634036540985, 0.33261942863464355]}}
{"id": "WebQTest-1373", "question": "where was john f kennedy when he got shot", "prediction": [["people.deceased_person.place_of_death"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was john f kennedy when he got shot [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.0041593750938773155, -0.051346294581890106, -0.05219659209251404], "norm_scores": [0.3439963757991791, 0.3281412720680237, 0.32786238193511963]}}
{"id": "WebQTest-1374", "question": "what type of government does italy have 2011", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does italy have 2011 [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.009024825878441334, -0.013568402267992496, -0.1422741562128067], "norm_scores": [0.34834545850753784, 0.34676629304885864, 0.3048882484436035]}}
{"id": "WebQTest-1376", "question": "what province is montreal canada located", "prediction": [["location.province.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.state"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what province is montreal canada located [/INST]", "raw_output": {"paths": ["<PATH> location.province.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>"], "scores": [-0.024334479123353958, -0.028383545577526093, -0.03040764108300209], "norm_scores": [0.3344588577747345, 0.3331073522567749, 0.3324337899684906]}}
{"id": "WebQTest-1378", "question": "who does kellan lutz play in prom night", "prediction": [["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["film.actor.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does kellan lutz play in prom night [/INST]", "raw_output": {"paths": ["<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.023661023005843163, -0.024441614747047424, -0.029325414448976517], "norm_scores": [0.33404916524887085, 0.33378851413726807, 0.3321623206138611]}}
{"id": "WebQTest-1379", "question": "what did franz liszt do to earn a living", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["music.artist.genre"]], "ground_paths": [["people.person.profession"], ["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did franz liszt do to earn a living [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> music.artist.genre </PATH>"], "scores": [-0.001011985237710178, -0.0605308897793293, -0.09530550241470337], "norm_scores": [0.3506024479866028, 0.3303438425064087, 0.31905367970466614]}}
{"id": "WebQTest-1380", "question": "what was nikola tesla known for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["law.inventor.inventions"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was nikola tesla known for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> law.inventor.inventions </PATH>"], "scores": [-0.0022854339331388474, -0.04977339133620262, -0.06795281916856766], "norm_scores": [0.34601306915283203, 0.3299656808376312, 0.32402127981185913]}}
{"id": "WebQTest-1381", "question": "what is the zip code for princeton nj", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"], ["location.citytown.postal_codes"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for princeton nj [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0003166146925650537, -0.07845939695835114, -0.11783354729413986], "norm_scores": [0.3553713262081146, 0.32865893840789795, 0.31596970558166504]}}
{"id": "WebQTest-1382", "question": "what other countries border argentina", "prediction": [["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.partially_contains", "location.location.partially_contains"]], "ground_paths": [["location.country.languages_spoken", "location.country.official_language"], ["location.country.form_of_government", "government.form_of_government.countries"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"], ["time.event.locations", "location.location.events"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["base.uncommon.topic.exceptions", "base.uncommon.topic.exceptions"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["location.location.primarily_containedby", "location.location.containedby"], ["biology.animal_breed.place_of_origin", "biology.animal_breed.place_of_origin"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["location.location.events", "time.event.locations"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["organization.organization_founder.organizations_founded", "organization.organization_founder.organizations_founded"], ["location.location.containedby", "location.location.contains_major_portion_of"], ["geography.river.basin_countries", "location.location.partially_contains"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["organization.organization.founders", "organization.organization.founders"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["location.location.contains", "location.location.contains"], ["location.location.partially_contains", "location.location.partially_containedby"], ["sports.multi_event_tournament.participating_countries", "sports.multi_event_tournament.participating_countries"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["film.film.featured_film_locations", "film.film.country"], ["base.locations.continents.counties_within", "base.locations.counties_parishes_and_boroughs.coninent"], ["location.location.events", "location.location.events"], ["common.topic.notable_types", "common.topic.notable_types"], ["film.film.featured_film_locations", "media_common.netflix_title.netflix_genres"], ["language.human_language.main_country", "language.language_family.geographic_distribution"], ["location.location.containedby", "location.location.containedby"], ["location.country.form_of_government", "location.country.form_of_government"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.location.partially_containedby", "location.location.containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.contains", "location.location.containedby"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["time.event.locations", "time.event.locations"], ["book.book_subject.works", "book.book_subject.works"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["government.form_of_government.countries", "location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other countries border argentina [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>"], "scores": [-0.03979894891381264, -0.042342934757471085, -0.045280974358320236], "norm_scores": [0.33422547578811646, 0.33337628841400146, 0.3323982357978821]}}
{"id": "WebQTest-1383", "question": "what language does people in iceland speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does people in iceland speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01611276902258396, -0.017279841005802155, -0.04237046837806702], "norm_scores": [0.33636990189552307, 0.33597758412361145, 0.32765254378318787]}}
{"id": "WebQTest-1384", "question": "what is the predominant religion in israel", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.quotations", "people.person.religion"]], "ground_paths": [["symbols.namesake.named_after", "religion.religion.notable_figures"], ["people.person.nationality", "people.person.religion"], ["film.film_location.featured_in_films", "media_common.netflix_title.netflix_genres"], ["location.country.administrative_divisions", "religion.religion.sacred_sites"], ["people.person.quotations", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the predominant religion in israel [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.quotations <SEP> people.person.religion </PATH>"], "scores": [-0.004023219458758831, -0.028038691729307175, -0.0510401725769043], "norm_scores": [0.34125709533691406, 0.33315926790237427, 0.3255835771560669]}}
{"id": "WebQTest-1385", "question": "who has coached the carolina panthers", "prediction": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["american_football.football_team.historical_coaching_staff", "american_football.football_historical_coach_position.coach"], ["sports.sports_team.sport", "sports.sports_team_coach.sports_coached"]], "ground_paths": [["sports.sports_team.coaches", "sports.sports_team_coach.teams_coached"], ["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["american_football.football_team.historical_coaching_staff", "american_football.football_coach.coaching_history"], ["sports.sports_team.sport", "sports.sports_team_coach.sports_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has coached the carolina panthers [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> american_football.football_team.historical_coaching_staff <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sports_team_coach.sports_coached </PATH>"], "scores": [-0.006649533286690712, -0.032751619815826416, -0.0344415009021759], "norm_scores": [0.3393481373786926, 0.33060503005981445, 0.3300468325614929]}}
{"id": "WebQTest-1387", "question": "what time zone is toronto gmt", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.containedby"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is toronto gmt [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.00021580845350399613, -0.07528513669967651, -0.16743721067905426], "norm_scores": [0.36053037643432617, 0.33445650339126587, 0.30501312017440796]}}
{"id": "WebQTest-1388", "question": "what country did toussaint l'ouverture help free", "prediction": [["people.person.nationality"], ["base.culturalevent.event.entity_involved"], ["base.activism.activist.area_of_activism"]], "ground_paths": [["military.military_service.military_person", "military.armed_force.personnel"], ["military.military_service.military_person", "military.military_service.military_force"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did toussaint l'ouverture help free [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>"], "scores": [-0.018008364364504814, -0.026067478582262993, -0.0493907704949379], "norm_scores": [0.337714821100235, 0.3350040912628174, 0.32728111743927]}}
{"id": "WebQTest-1389", "question": "what jobs did robert burns have", "prediction": [["people.person.profession"], ["influence.influence_node.influenced", "people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what jobs did robert burns have [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.002413180423900485, -0.055447664111852646, -0.06403616070747375], "norm_scores": [0.34619030356407166, 0.328308641910553, 0.32550105452537537]}}
{"id": "WebQTest-1390", "question": "who won utah attorney general", "prediction": [["base.aareas.schema.administrative_area.administrative_children", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["base.biblioness.bibs_location.city", "people.person.place_of_birth"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won utah attorney general [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_children <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.015819696709513664, -0.025784872472286224, -0.028831711038947105], "norm_scores": [0.3358909487724304, 0.33256036043167114, 0.33154866099357605]}}
{"id": "WebQTest-1392", "question": "what the language spoken in indonesia", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the language spoken in indonesia [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.015061404556035995, -0.02353334240615368, -0.026433957740664482], "norm_scores": [0.335541695356369, 0.33271101117134094, 0.3317473530769348]}}
{"id": "WebQTest-1394", "question": "who coaches the carolina panthers", "prediction": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["sports.sports_team.sport", "sports.sport.team_coaches"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"]], "ground_paths": [["american_football.football_coach.current_team_head_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who coaches the carolina panthers [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sport.team_coaches </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach.teams_coached </PATH>"], "scores": [-0.006282547954469919, -0.03690388426184654, -0.03718775138258934], "norm_scores": [0.34020429849624634, 0.3299446702003479, 0.32985103130340576]}}
{"id": "WebQTest-1395", "question": "what other countries does south africa share borders with", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.partially_contains", "location.location.partially_contains"]], "ground_paths": [["location.imports_and_exports.exported_to", "location.imports_and_exports.imported_from"], ["location.country.official_language", "location.country.official_language"], ["time.event.locations", "location.location.events"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.sport_country.multi_event_tournaments_participated_in"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.multi_event_tournament.participating_countries"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["film.film.featured_film_locations", "film.film_location.featured_in_films"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.uncommon.topic.exceptions", "base.uncommon.topic.exceptions"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["language.human_language.countries_spoken_in", "location.country.official_language"], ["location.location.time_zones", "location.location.time_zones"], ["location.country.languages_spoken", "language.human_language.main_country"], ["location.country.official_language", "language.human_language.main_country"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.statistical_region.places_imported_from", "location.statistical_region.places_exported_to"], ["location.location.partially_contains", "location.location.partially_contains"], ["organization.organization.founders", "organization.organization.founders"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["film.film.featured_film_locations", "film.film.subjects"], ["location.location.partially_contains", "location.location.partially_containedby"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.containedby", "location.location.containedby"], ["location.location.containedby", "base.locations.continents.countries_within"], ["location.country.form_of_government", "location.country.form_of_government"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["location.country.official_language", "language.human_language.countries_spoken_in"], ["time.event.locations", "time.event.locations"], ["location.location.containedby", "base.locations.countries.continent"], ["location.country.currency_used", "location.country.currency_used"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["government.form_of_government.countries", "location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other countries does south africa share borders with [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>"], "scores": [-0.03301016986370087, -0.03414589911699295, -0.04408184811472893], "norm_scores": [0.33468833565711975, 0.3343084454536438, 0.33100321888923645]}}
{"id": "WebQTest-1396", "question": "who played darth vader at the end of return of the jedi", "prediction": [["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played darth vader at the end of return of the jedi [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.022479677572846413, -0.026504989713430405, -0.032163359224796295], "norm_scores": [0.33485737442970276, 0.33351218700408936, 0.3316303789615631]}}
{"id": "WebQTest-1398", "question": "what movies did joan crawford play in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["film.actor.film", "film.film.starring"], ["people.deceased_person.place_of_death", "film.film.featured_film_locations"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.producer.film"], ["film.person_or_entity_appearing_in_film.films", "film.personal_film_appearance.film"], ["film.actor.film", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["common.topic.notable_types", "common.topic.notable_types"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did joan crawford play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.02377314679324627, -0.028201023116707802, -0.0348801352083683], "norm_scores": [0.3350604176521301, 0.3335800766944885, 0.33135950565338135]}}
{"id": "WebQTest-1399", "question": "what products does the united states export", "prediction": [["location.statistical_region.major_exports", "location.imports_exports_by_industry.industry"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what products does the united states export [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.major_exports <SEP> location.imports_exports_by_industry.industry </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.023318855091929436, -0.0261447262018919, -0.03652356564998627], "norm_scores": [0.33511388301849365, 0.3341681957244873, 0.33071789145469666]}}
{"id": "WebQTest-1402", "question": "what is the kentucky state bird", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"]], "ground_paths": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the kentucky state bird [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.014199154451489449, -0.022496992722153664, -0.028298769146203995], "norm_scores": [0.3358256220817566, 0.33305051922798157, 0.33112379908561707]}}
{"id": "WebQTest-1403", "question": "who was canadian prime minister in 1993", "prediction": [["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"]], "ground_paths": [["base.firsts.first_achievement.achievement", "base.firsts.first_achievement.first"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was canadian prime minister in 1993 [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.007678322959691286, -0.01669701747596264, -0.028444364666938782], "norm_scores": [0.33664706349372864, 0.33362460136413574, 0.3297283351421356]}}
{"id": "WebQTest-1405", "question": "what there to do in salt lake city utah", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what there to do in salt lake city utah [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01439659669995308, -0.01462792232632637, -0.031222539022564888], "norm_scores": [0.33522361516952515, 0.33514606952667236, 0.3296303451061249]}}
{"id": "WebQTest-1408", "question": "who is the current ohio state senator", "prediction": [["government.political_district.representatives", "government.government_position_held.office_holder"], ["location.country.first_level_divisions", "people.person.nationality"], ["government.government_position_held.governmental_body", "government.government_position_held.office_holder"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the current ohio state senator [/INST]", "raw_output": {"paths": ["<PATH> government.political_district.representatives <SEP> government.government_position_held.office_holder </PATH>", "<PATH> location.country.first_level_divisions <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.023709559813141823, -0.027067825198173523, -0.027274716645479202], "norm_scores": [0.3341030478477478, 0.3329829275608063, 0.3329140245914459]}}
{"id": "WebQTest-1409", "question": "what are the major languages spoken in the united kingdom", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the major languages spoken in the united kingdom [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.01577823981642723, -0.022401150315999985, -0.023176580667495728], "norm_scores": [0.33489304780960083, 0.33268240094184875, 0.3324245512485504]}}
{"id": "WebQTest-1410", "question": "what did st. nicholas do in his life", "prediction": [["people.person.profession"], ["book.written_work.subjects"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did st. nicholas do in his life [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.01741458848118782, -0.03287830203771591, -0.03663749620318413], "norm_scores": [0.3371981084346771, 0.33202388882637024, 0.3307780623435974]}}
{"id": "WebQTest-1414", "question": "what happened to pope john paul ii", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.deceased_people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what happened to pope john paul ii [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.deceased_people </PATH>"], "scores": [-0.0006395146483555436, -0.06558562070131302, -0.11475611478090286], "norm_scores": [0.3534477949142456, 0.3312222957611084, 0.3153298497200012]}}
{"id": "WebQTest-1416", "question": "what was franklin pierce famous for", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was franklin pierce famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>"], "scores": [-0.01140395738184452, -0.03161860257387161, -0.03852454200387001], "norm_scores": [0.33861204981803894, 0.3318358361721039, 0.3295520842075348]}}
{"id": "WebQTest-1417", "question": "when was the last time the boston bruins went to the stanley cup", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.runner_up"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the boston bruins went to the stanley cup [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.011269415728747845, -0.014941838569939137, -0.029287341982126236], "norm_scores": [0.33574196696281433, 0.33451125025749207, 0.3297467827796936]}}
{"id": "WebQTest-1418", "question": "who is meredith gray married to in real life", "prediction": [["fictional_universe.fictional_character.married_to", "fictional_universe.marriage_of_fictional_characters.spouses"], ["fictional_universe.fictional_character.married_to", "fictional_universe.fictional_character.married_to"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is meredith gray married to in real life [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.married_to <SEP> fictional_universe.marriage_of_fictional_characters.spouses </PATH>", "<PATH> fictional_universe.fictional_character.married_to <SEP> fictional_universe.fictional_character.married_to </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.02075755037367344, -0.026646720245480537, -0.03451298177242279], "norm_scores": [0.33551791310310364, 0.33354780077934265, 0.3309343159198761]}}
{"id": "WebQTest-1419", "question": "what inspired michelangelo buonarroti", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inspired michelangelo buonarroti [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>"], "scores": [-0.00557348970323801, -0.020035097375512123, -0.09718316793441772], "norm_scores": [0.34505322575569153, 0.3400990962982178, 0.3148477077484131]}}
{"id": "WebQTest-1420", "question": "what does monsanto own", "prediction": [["business.business_operation.industry", "business.business_operation.industry"], ["organization.organization.parent", "organization.organization.child"], ["business.business_operation.industry"]], "ground_paths": [["business.industry.companies"], ["business.business_operation.industry"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does monsanto own [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> organization.organization.parent <SEP> organization.organization.child </PATH>", "<PATH> business.business_operation.industry </PATH>"], "scores": [-0.03395196050405502, -0.03970574587583542, -0.04110700637102127], "norm_scores": [0.3347691297531128, 0.332848459482193, 0.3323823809623718]}}
{"id": "WebQTest-1421", "question": "who did woody harrelson play on cheers", "prediction": [["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"]], "ground_paths": [["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.character"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["people.person.profession", "fictional_universe.fictional_character.occupation"], ["film.actor.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did woody harrelson play on cheers [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>"], "scores": [-0.02490195259451866, -0.02579338848590851, -0.025871645659208298], "norm_scores": [0.333540141582489, 0.3332429528236389, 0.3332168757915497]}}
{"id": "WebQTest-1422", "question": "what language do they speak in sweden yahoo", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they speak in sweden yahoo [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.012886489741504192, -0.019319983199238777, -0.03357553109526634], "norm_scores": [0.3363480567932129, 0.33419111371040344, 0.32946082949638367]}}
{"id": "WebQTest-1423", "question": "what language did they speak in ghana", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language did they speak in ghana [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.004423689097166061, -0.03738271817564964, -0.04728604108095169], "norm_scores": [0.3418077230453491, 0.33072569966316223, 0.32746657729148865]}}
{"id": "WebQTest-1424", "question": "what form of government do we have in the philippines", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["form.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government do we have in the philippines [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> form.form_of_government.countries </PATH>"], "scores": [-0.008599827997386456, -0.014145574532449245, -0.15594857931137085], "norm_scores": [0.3499608337879181, 0.3480254113674164, 0.30201372504234314]}}
{"id": "WebQTest-1425", "question": "what colleges did mitt romney attend", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.education", "education.education.institution"], ["people.person.religion", "religion.religion.organizations"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what colleges did mitt romney attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.018041307106614113, -0.023365819826722145, -0.024969233199954033], "norm_scores": [0.3346960246562958, 0.332918643951416, 0.3323853015899658]}}
{"id": "WebQTest-1426", "question": "where is ibrahim afellay from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is ibrahim afellay from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0049040233716368675, -0.037838514894247055, -0.04911130666732788], "norm_scores": [0.34195542335510254, 0.33087673783302307, 0.3271677792072296]}}
{"id": "WebQTest-1427", "question": "who was the secretary of state when andrew jackson was president", "prediction": [["book.written_work.subjects", "people.person.nationality"], ["government.government_position_held.appointed_by", "government.government_position_held.office_holder"], ["government.government_position_held.appointed_by", "government.politician.government_positions_held"]], "ground_paths": [["government.political_appointer.appointees", "government.government_position_held.office_holder"], ["people.person.gender", "people.person.gender"], ["government.government_position_held.appointed_by", "government.politician.government_positions_held"], ["people.person.nationality", "people.person.nationality"], ["government.us_vice_president.to_president"], ["people.person.profession", "people.person.profession"], ["people.person.profession", "common.topic.notable_types"], ["government.government_position_held.appointed_by", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the secretary of state when andrew jackson was president [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.appointed_by <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.appointed_by <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.02195487916469574, -0.024661049246788025, -0.02907170169055462], "norm_scores": [0.3344251215457916, 0.3335213363170624, 0.332053542137146]}}
{"id": "WebQTest-1428", "question": "who does ron stoppable 's voice", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does ron stoppable 's voice [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.021585894748568535, -0.03000452183187008, -0.03813149780035019], "norm_scores": [0.33611103892326355, 0.3332933187484741, 0.33059564232826233]}}
{"id": "WebQTest-1429", "question": "what currency is accepted in new zealand", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency is accepted in new zealand [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.008039356209337711, -0.02035808563232422, -0.11974135041236877], "norm_scores": [0.34697312116622925, 0.34272506833076477, 0.3103017807006836]}}
{"id": "WebQTest-1432", "question": "what places are in the west midlands", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_children"], ["location.location.containedby", "common.topic.image"], ["location.location.contains"], ["location.location.containedby"], [], ["base.aareas.schema.administrative_area.administrative_parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what places are in the west midlands [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.005453580990433693, -0.03692231327295303, -0.04818541556596756], "norm_scores": [0.3416246175765991, 0.33104151487350464, 0.32733386754989624]}}
{"id": "WebQTest-1433", "question": "what to do with kids in asheville nc", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do with kids in asheville nc [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.007943219505250454, -0.020322328433394432, -0.039138298481702805], "norm_scores": [0.3381824791431427, 0.3340218663215637, 0.32779571413993835]}}
{"id": "WebQTest-1434", "question": "who is in the american league in baseball", "prediction": [["baseball.baseball_team.league"], ["baseball.baseball_league.teams"], ["sports.sports_position.sport", "sports.sports_team.sport"]], "ground_paths": [["sports.sports_league_participation.league", "sports.sports_league_participation.team"], ["baseball.baseball_league.teams"], ["baseball.baseball_league.teams", "sports.defunct_sports_team.later_known_as"], ["baseball.baseball_team.league", "sports.sports_team.previously_known_as"], ["sports.sports_league.sport", "sports.defunct_sports_team.sport"], ["sports.sports_league.teams", "sports.sports_team.league"], ["sports.sports_league_participation.league", "sports.sports_team.league"], ["baseball.baseball_team.league", "sports.defunct_sports_team.later_known_as"], ["sports.sports_league.sport", "sports.sports_team.sport"], ["baseball.baseball_team.league"], ["sports.sports_league.teams", "sports.sports_league_participation.team"], ["baseball.baseball_league.teams", "sports.sports_team.previously_known_as"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is in the american league in baseball [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_team.league </PATH>", "<PATH> baseball.baseball_league.teams </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>"], "scores": [-0.011140966787934303, -0.02671041153371334, -0.043775781989097595], "norm_scores": [0.33870255947113037, 0.3334699869155884, 0.32782748341560364]}}
{"id": "WebQTest-1435", "question": "who did kurt warner play 4", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"]], "ground_paths": [["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.football_player.passing", "american_football.player_passing_statistics.team"], ["american_football.player_passing_statistics.player", "american_football.player_passing_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did kurt warner play 4 [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>"], "scores": [-0.02454214356839657, -0.029212363064289093, -0.03348085284233093], "norm_scores": [0.3348466455936432, 0.33328649401664734, 0.33186689019203186]}}
{"id": "WebQTest-1436", "question": "who is dirk nowitzki wife", "prediction": [["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.parents"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.person.spouse_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is dirk nowitzki wife [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>"], "scores": [-0.03347551077604294, -0.03947685658931732, -0.04021846130490303], "norm_scores": [0.33475086092948914, 0.3327479064464569, 0.33250123262405396]}}
{"id": "WebQTest-1441", "question": "where all did picasso live", "prediction": [["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["symbols.name_source.namesakes", "location.location.containedby"], ["influence.influence_node.influenced", "people.person.nationality"], ["people.person.place_of_birth"], ["symbols.namesake.named_after", "film.film.country"], ["people.person.parents", "people.person.nationality"], ["film.film_subject.films", "film.film.country"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["influence.influence_node.influenced_by", "people.person.nationality"], ["book.written_work.subjects", "book.written_work.subjects"], ["people.person.places_lived", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where all did picasso live [/INST]", "raw_output": {"paths": ["<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.02465570718050003, -0.027232544496655464, -0.0310466680675745], "norm_scores": [0.33433008193969727, 0.3334696590900421, 0.33220019936561584]}}
{"id": "WebQTest-1442", "question": "what currency shall i take to croatia", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency shall i take to croatia [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.008517753332853317, -0.01973068341612816, -0.08021300286054611], "norm_scores": [0.3425051271915436, 0.33868610858917236, 0.31880873441696167]}}
{"id": "WebQTest-1443", "question": "what team is raul ibanez on", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is raul ibanez on [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.01561712846159935, -0.019955791532993317, -0.024560648947954178], "norm_scores": [0.33481016755104065, 0.3333607017993927, 0.33182913064956665]}}
{"id": "WebQTest-1445", "question": "what tv series did mark harmon star in", "prediction": [["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.languages", "tv.tv_program.languages"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"]], "ground_paths": [["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["people.person.languages", "tv.tv_program.languages"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["people.person.languages", "film.film.language"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what tv series did mark harmon star in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> people.person.languages <SEP> tv.tv_program.languages </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_program.regular_cast </PATH>"], "scores": [-0.020587433129549026, -0.024913309141993523, -0.032117072492837906], "norm_scores": [0.33509594202041626, 0.3336494565010071, 0.33125460147857666]}}
{"id": "WebQTest-1446", "question": "what year did jimmy the rev sullivan die", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.cause_of_death"], ["people.person.deceased_place_of_death"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did jimmy the rev sullivan die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.deceased_place_of_death </PATH>"], "scores": [-0.0007618542877025902, -0.06473511457443237, -0.07633031904697418], "norm_scores": [0.3490101397037506, 0.32738199830055237, 0.3236078917980194]}}
{"id": "WebQTest-1447", "question": "who plays anakin skywalker in star wars 1", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["film.film.language", "people.person.languages"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["film.film.starring", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays anakin skywalker in star wars 1 [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.01597064733505249, -0.022623052820563316, -0.02426428161561489], "norm_scores": [0.33499598503112793, 0.3327748477458954, 0.3322291672229767]}}
{"id": "WebQTest-1448", "question": "where does michelle pfeiffer live now", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["people.person.place_of_birth", "location.us_county.county_seat"], ["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does michelle pfeiffer live now [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.015882056206464767, -0.031493477523326874, -0.03228025138378143], "norm_scores": [0.33689936995506287, 0.3316807448863983, 0.3314198851585388]}}
{"id": "WebQTest-1449", "question": "what was laura ingalls wilder famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["book.written_work.author"]], "ground_paths": [["people.person.profession"], ["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was laura ingalls wilder famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.0034974825102835894, -0.04659649357199669, -0.05296821519732475], "norm_scores": [0.3436957895755768, 0.3291975259780884, 0.32710662484169006]}}
{"id": "WebQTest-1452", "question": "where is dwight howard now", "prediction": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is dwight howard now [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.019855480641126633, -0.0217767134308815, -0.02349795401096344], "norm_scores": [0.3339517116546631, 0.3333107531070709, 0.332737535238266]}}
{"id": "WebQTest-1453", "question": "who is nick lachey married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["people.person.parents", "people.person.children"], ["people.marriage.spouse", "people.marriage.spouse"], ["people.person.languages", "people.person.languages"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is nick lachey married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.020693201571702957, -0.035679250955581665, -0.04312937334179878], "norm_scores": [0.337502658367157, 0.33248257637023926, 0.33001473546028137]}}
{"id": "WebQTest-1454", "question": "where did madonna grew up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["music.artist.origin"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did madonna grew up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.007809459697455168, -0.03453487902879715, -0.037685543298721313], "norm_scores": [0.339651495218277, 0.33069437742233276, 0.32965409755706787]}}
{"id": "WebQTest-1455", "question": "what capital city of brazil", "prediction": [["location.country.capital"], ["location.location.containedby"], ["location.administrative_division.country"]], "ground_paths": [["base.biblioness.bibs_location.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what capital city of brazil [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-0.004029382951557636, -0.042737893760204315, -0.07959231734275818], "norm_scores": [0.3461102843284607, 0.3329688608646393, 0.3209208846092224]}}
{"id": "WebQTest-1456", "question": "what college did john stockton go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.place_of_birth", "location.location.contains"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did john stockton go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.012777794152498245, -0.021418018266558647, -0.02549625001847744], "norm_scores": [0.3357102572917938, 0.33282214403152466, 0.33146756887435913]}}
{"id": "WebQTest-1457", "question": "what kind of currency does mexico use", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of currency does mexico use [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.0025031964760273695, -0.0450684018433094, -0.07157973945140839], "norm_scores": [0.3458313047885895, 0.33141982555389404, 0.3227488696575165]}}
{"id": "WebQTest-1458", "question": "who owns chrysler corporation 2011", "prediction": [["business.business_operation.industry", "business.business_operation.industry"], ["organization.organization_relationship.child", "organization.organization_relationship.parent"], ["organization.organization.parent", "organization.organization.child"]], "ground_paths": [["organization.organization_relationship.child", "organization.organization_relationship.parent"], ["organization.organization.parent", "organization.organization.child"], ["business.business_operation.industry", "business.business_operation.industry"], ["organization.organization.parent", "organization.organization_relationship.parent"], ["common.topic.notable_types", "common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who owns chrysler corporation 2011 [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> organization.organization_relationship.child <SEP> organization.organization_relationship.parent </PATH>", "<PATH> organization.organization.parent <SEP> organization.organization.child </PATH>"], "scores": [-0.016615835949778557, -0.022731004282832146, -0.028039980679750443], "norm_scores": [0.33528420329093933, 0.3332401514053345, 0.3314756453037262]}}
{"id": "WebQTest-1459", "question": "what are some books that mark twain wrote", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.book_edition.author_editor"]], "ground_paths": [["book.book_edition.author_editor"], ["film.film.story_by"], ["film.film_story_contributor.film_story_credits"], ["book.book_subject.works"], ["book.written_work.author"], ["book.written_work.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some books that mark twain wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.01151334773749113, -0.024767877534031868, -0.03349550813436508], "norm_scores": [0.3372578024864197, 0.3328171372413635, 0.3299250304698944]}}
{"id": "WebQTest-1460", "question": "what was gregor mendel contribution", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["symbols.namesake.named_after"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was gregor mendel contribution [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.008961164392530918, -0.044907357543706894, -0.05604037642478943], "norm_scores": [0.34261780977249146, 0.33052074909210205, 0.3268614709377289]}}
{"id": "WebQTest-1461", "question": "who has ray allen dated", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"]], "ground_paths": [["people.person.parents", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has ray allen dated [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>"], "scores": [-0.018824446946382523, -0.03107590787112713, -0.04387854039669037], "norm_scores": [0.3374866247177124, 0.3333771228790283, 0.3291362524032593]}}
{"id": "WebQTest-1462", "question": "who is running for vice president with barack obama 2012", "prediction": [["government.election_campaign.election", "government.election_campaign.candidate"], ["government.election_campaign.election", "government.politician.election_campaigns"], ["government.election_campaign.candidate", "government.election_campaign.candidate"]], "ground_paths": [["government.us_vice_president.to_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is running for vice president with barack obama 2012 [/INST]", "raw_output": {"paths": ["<PATH> government.election_campaign.election <SEP> government.election_campaign.candidate </PATH>", "<PATH> government.election_campaign.election <SEP> government.politician.election_campaigns </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.candidate </PATH>"], "scores": [-0.01605505496263504, -0.03020855225622654, -0.031056176871061325], "norm_scores": [0.3365805447101593, 0.3318503201007843, 0.3315691351890564]}}
{"id": "WebQTest-1463", "question": "what language do people from the uk speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people from the uk speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01565868966281414, -0.017150355502963066, -0.04017471522092819], "norm_scores": [0.336214542388916, 0.33571338653564453, 0.32807210087776184]}}
{"id": "WebQTest-1464", "question": "what to do with my kids in toronto", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["film.film_festival.location"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do with my kids in toronto [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.013872724957764149, -0.014149313792586327, -0.03378836810588837], "norm_scores": [0.33556991815567017, 0.3354771137237549, 0.32895296812057495]}}
{"id": "WebQTest-1465", "question": "what kind of government is south korea", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government is south korea [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.009752766229212284, -0.014378542080521584, -0.1347552388906479], "norm_scores": [0.34747806191444397, 0.3458744287490845, 0.30664753913879395]}}
{"id": "WebQTest-1467", "question": "who did ben franklin marry", "prediction": [["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.parents"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.place_of_interment.interred_here", "people.place_of_interment.interred_here"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did ben franklin marry [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>"], "scores": [-0.03205643221735954, -0.032923098653554916, -0.04442640766501427], "norm_scores": [0.3348020315170288, 0.33451199531555176, 0.3306860625743866]}}
{"id": "WebQTest-1468", "question": "what sri lanka capital", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sri lanka capital [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.009165595285594463, -0.027251053601503372, -0.04243180900812149], "norm_scores": [0.3390568494796753, 0.3329799771308899, 0.3279632329940796]}}
{"id": "WebQTest-1470", "question": "what kind of government does poland have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does poland have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.010808253660798073, -0.013435467146337032, -0.14649473130702972], "norm_scores": [0.3483722507953644, 0.34745821356773376, 0.30416950583457947]}}
{"id": "WebQTest-1471", "question": "what famous people came from delaware", "prediction": [["people.person.place_of_birth"], ["location.location.containedby", "people.person.nationality"], ["location.location.containedby", "people.person.place_of_birth"]], "ground_paths": [["location.location.people_born_here"], ["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what famous people came from delaware [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> location.location.containedby <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.013112729415297508, -0.04028976708650589, -0.04690958559513092], "norm_scores": [0.34014105796813965, 0.3310215175151825, 0.32883745431900024]}}
{"id": "WebQTest-1472", "question": "when did romney become governor", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did romney become governor [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.02077697031199932, -0.03129581734538078, -0.0319480374455452], "norm_scores": [0.3357476592063904, 0.3322345018386841, 0.33201783895492554]}}
{"id": "WebQTest-1473", "question": "what currency does cyprus use", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does cyprus use [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.002824770752340555, -0.0390520878136158, -0.1112121194601059], "norm_scores": [0.3494425117969513, 0.3370096981525421, 0.3135477602481842]}}
{"id": "WebQTest-1475", "question": "what type of guitar does johnny depp play", "prediction": [["music.group_member.instruments_played"], ["music.instrument.family"], ["music.guitar.guitarists"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of guitar does johnny depp play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.guitar.guitarists </PATH>"], "scores": [-0.013556540943682194, -0.03151187300682068, -0.03587033599615097], "norm_scores": [0.3378220498561859, 0.33181047439575195, 0.33036744594573975]}}
{"id": "WebQTest-1477", "question": "what episode of how i met your mother is carrie underwood on", "prediction": [["people.person.nationality", "tv.tv_program.country_of_origin"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"]], "ground_paths": [["tv.tv_guest_role.actor", "tv.tv_guest_role.episodes_appeared_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what episode of how i met your mother is carrie underwood on [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.regular_tv_appearance.series </PATH>"], "scores": [-0.018001629039645195, -0.027228692546486855, -0.02979194186627865], "norm_scores": [0.3356724977493286, 0.3325894773006439, 0.33173805475234985]}}
{"id": "WebQTest-1478", "question": "who are the 9 justices of the supreme court 2011", "prediction": [["law.court.jurisdiction", "people.person.nationality"], ["book.written_work.subjects", "book.written_work.author"], ["people.appointment.appointed_role", "people.appointee.position"]], "ground_paths": [["law.court.jurisdiction", "people.person.nationality"], ["book.written_work.subjects", "book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the 9 justices of the supreme court 2011 [/INST]", "raw_output": {"paths": ["<PATH> law.court.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> book.written_work.subjects <SEP> book.written_work.author </PATH>", "<PATH> people.appointment.appointed_role <SEP> people.appointee.position </PATH>"], "scores": [-0.007944513112306595, -0.03305286169052124, -0.03379364684224129], "norm_scores": [0.3390190899372101, 0.3306128680706024, 0.3303680419921875]}}
{"id": "WebQTest-1479", "question": "who is dustin johnson the golfer", "prediction": [["people.person.profession"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.player"], ["sports.sports_league_draft_pick.player", "sports.drafted_athlete.drafted"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is dustin johnson the golfer [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.player </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.drafted_athlete.drafted </PATH>"], "scores": [-0.00237138825468719, -0.06022508442401886, -0.06591762602329254], "norm_scores": [0.34695491194725037, 0.3274519145488739, 0.32559314370155334]}}
{"id": "WebQTest-1480", "question": "who did cher have a son with", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["music.artist.genre", "music.artist.genre"]], "ground_paths": [["people.person.parents", "people.person.children"], ["award.award_winner.awards_won", "award.award_honor.award_winner"], ["music.artist.concert_tours", "music.concert_tour.artist"], ["base.americancomedy.double_act.comic", "base.americancomedy.double_act.straight_man"], ["music.artist.genre", "music.artist.genre"], ["people.marriage.spouse", "people.marriage.spouse"], ["music.artist.label", "music.artist.label"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.spouse_s", "people.person.spouse_s"], ["award.award_nominee.award_nominations", "award.award_nominee.award_nominations"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.celebrity.breakup", "base.popstra.breakup.participant"], ["base.popstra.celebrity.breakup", "base.popstra.celebrity.breakup"], ["music.composer.compositions", "music.composer.compositions"], ["people.person.profession", "people.person.profession"], ["award.award_nomination.award_nominee", "award.award_nominee.award_nominations"], ["music.album.artist", "music.album.artist"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did cher have a son with [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>"], "scores": [-0.021610280498862267, -0.03288755193352699, -0.03981988877058029], "norm_scores": [0.33661630749702454, 0.33284154534339905, 0.3305421471595764]}}
{"id": "WebQTest-1481", "question": "what city is the university of oregon state in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.parent_institution", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city is the university of oregon state in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>"], "scores": [-0.0026479987427592278, -0.0541258342564106, -0.056522589176893234], "norm_scores": [0.34513989090919495, 0.32782241702079773, 0.32703763246536255]}}
{"id": "WebQTest-1482", "question": "who played carmen cortez in spy kids", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played carmen cortez in spy kids [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.02546059340238571, -0.033192045986652374, -0.034189142286777496], "norm_scores": [0.3351646959781647, 0.33258339762687683, 0.3322519063949585]}}
{"id": "WebQTest-1483", "question": "what is the political system in germany", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the political system in germany [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.010025249794125557, -0.014469158835709095, -0.12850427627563477], "norm_scores": [0.3467603325843811, 0.3452227711677551, 0.30801695585250854]}}
{"id": "WebQTest-1484", "question": "what prompted shakespeare to write poetry", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what prompted shakespeare to write poetry [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.01654430851340294, -0.02604457549750805, -0.028319811448454857], "norm_scores": [0.3357013463973999, 0.33252719044685364, 0.33177146315574646]}}
{"id": "WebQTest-1485", "question": "what is the currency in turkey called", "prediction": [["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"], ["location.country.currency_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency in turkey called [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.016457853838801384, -0.019708920270204544, -0.03241651505231857], "norm_scores": [0.3354666531085968, 0.33437779545783997, 0.33015555143356323]}}
{"id": "WebQTest-1486", "question": "what school did mark zuckerberg attend", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did mark zuckerberg attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.015397931449115276, -0.01833423227071762, -0.022316524758934975], "norm_scores": [0.3344287872314453, 0.33344823122024536, 0.3321229815483093]}}
{"id": "WebQTest-1487", "question": "where does kate middleton live 2012", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does kate middleton live 2012 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.01124873198568821, -0.021506374701857567, -0.03396429866552353], "norm_scores": [0.337002694606781, 0.33356353640556335, 0.32943376898765564]}}
{"id": "WebQTest-1488", "question": "what type of art does marc chagall do", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_art_form.associated_artists"]], "ground_paths": [["book.school_or_movement.associated_authors"], ["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art does marc chagall do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_art_form.associated_artists </PATH>"], "scores": [-0.0015901232836768031, -0.03819518908858299, -0.11137308180332184], "norm_scores": [0.349639892578125, 0.3370727300643921, 0.3132873475551605]}}
{"id": "WebQTest-1489", "question": "what character did billy d williams play in star wars", "prediction": [["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"], ["film.actor.film", "film.performance.character"]], "ground_paths": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"], ["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what character did billy d williams play in star wars [/INST]", "raw_output": {"paths": ["<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>"], "scores": [-0.022324830293655396, -0.029182767495512962, -0.035098545253276825], "norm_scores": [0.33551719784736633, 0.3332241475582123, 0.3312586843967438]}}
{"id": "WebQTest-1490", "question": "what language is spoken in argentina", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is spoken in argentina [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.007469221018254757, -0.028399402275681496, -0.04852541163563728], "norm_scores": [0.3402445316314697, 0.3331971764564514, 0.32655826210975647]}}
{"id": "WebQTest-1493", "question": "what school did wilfred owen go to", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did wilfred owen go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.01500898040831089, -0.02449812740087509, -0.024812551215291023], "norm_scores": [0.33548039197921753, 0.3323120176792145, 0.3322075605392456]}}
{"id": "WebQTest-1494", "question": "who is kevin jonas married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["people.person.nationality", "people.person.nationality"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["people.marriage.spouse", "people.person.spouse_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is kevin jonas married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.0284917950630188, -0.0345611609518528, -0.039682477712631226], "norm_scores": [0.33525311946868896, 0.33322450518608093, 0.3315223455429077]}}
{"id": "WebQTest-1495", "question": "where does airtran airways fly", "prediction": [["aviation.airline.airports_served", "aviation.airline_airport_presence.airport"], ["aviation.airline_airport_presence.airline", "aviation.airline_airport_presence.airport"], ["aviation.airport.hub_for", "aviation.airport.serves"]], "ground_paths": [["aviation.airline.airports_served", "aviation.airline_airport_presence.airport"], ["aviation.airline_airport_presence.airline", "aviation.airport.airlines"], ["aviation.airport.focus_city_for"], ["aviation.airline.focus_cities"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does airtran airways fly [/INST]", "raw_output": {"paths": ["<PATH> aviation.airline.airports_served <SEP> aviation.airline_airport_presence.airport </PATH>", "<PATH> aviation.airline_airport_presence.airline <SEP> aviation.airline_airport_presence.airport </PATH>", "<PATH> aviation.airport.hub_for <SEP> aviation.airport.serves </PATH>"], "scores": [-0.019093342125415802, -0.024364346638321877, -0.03861173614859581], "norm_scores": [0.336087703704834, 0.3343208432197571, 0.32959139347076416]}}
{"id": "WebQTest-1496", "question": "where does the city of paris get its name", "prediction": [["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["location.location.containedby"]], "ground_paths": [["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the city of paris get its name [/INST]", "raw_output": {"paths": ["<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.011057442054152489, -0.01497038546949625, -0.07768358290195465], "norm_scores": [0.34110602736473083, 0.339773952960968, 0.31912001967430115]}}
{"id": "WebQTest-1499", "question": "who did sandra bullock marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["people.marriage.spouse", "people.marriage.spouse"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.infidelity.victim", "base.popstra.celebrity.infidelity_perpetrator"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["base.popstra.infidelity.participant", "base.popstra.infidelity.perpetrator"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did sandra bullock marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.01799919828772545, -0.03735103830695152, -0.03957509994506836], "norm_scores": [0.3378961384296417, 0.33142006397247314, 0.33068379759788513]}}
{"id": "WebQTest-1500", "question": "where is olympic national park located", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is olympic national park located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.011326837353408337, -0.024352438747882843, -0.029154112562537193], "norm_scores": [0.33676958084106445, 0.33241137862205505, 0.3308190703392029]}}
{"id": "WebQTest-1501", "question": "where is the un based", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["book.book_subject.works", "base.reviews.review.mentions"], ["organization.organization_founder.organizations_founded", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the un based [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.01325315237045288, -0.041500240564346313, -0.042032960802316666], "norm_scores": [0.33969947695732117, 0.3302381932735443, 0.3300623297691345]}}
{"id": "WebQTest-1502", "question": "what city and state is yale located in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.parent_institution", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city and state is yale located in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>"], "scores": [-0.0017386524705216289, -0.058839280158281326, -0.06696204841136932], "norm_scores": [0.3470586836338043, 0.3277965784072876, 0.3251447379589081]}}
{"id": "WebQTest-1503", "question": "where did thomas paine die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here"], ["people.place_of_death.people_deceased_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did thomas paine die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>"], "scores": [-0.00038024975219741464, -0.09478087723255157, -0.10291267931461334], "norm_scores": [0.35555973649024963, 0.32353025674819946, 0.3209100365638733]}}
{"id": "WebQTest-1504", "question": "where was barack hussein obama sr born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.parents"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was barack hussein obama sr born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.0007554953917860985, -0.06362996250391006, -0.10349325090646744], "norm_scores": [0.35193610191345215, 0.3304895758628845, 0.31757432222366333]}}
{"id": "WebQTest-1506", "question": "where is the home stadium of the new england patriots", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["location.location.containedby"]], "ground_paths": [["sports.sports_facility.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the home stadium of the new england patriots [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0034829401411116123, -0.027460582554340363, -0.10526315122842789], "norm_scores": [0.3472782075405121, 0.3390503227710724, 0.31367141008377075]}}
{"id": "WebQTest-1507", "question": "who was kate chopin", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was kate chopin [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0004957993514835835, -0.07927916944026947, -0.09749310463666916], "norm_scores": [0.3531324565410614, 0.3263791799545288, 0.3204883635044098]}}
{"id": "WebQTest-1508", "question": "when did bill clinton go to college", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did bill clinton go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.014923566952347755, -0.020386336371302605, -0.021979359909892082], "norm_scores": [0.33472564816474915, 0.33290210366249084, 0.3323722183704376]}}
{"id": "WebQTest-1510", "question": "what are staffordshire terriers", "prediction": [["biology.animal_breed.breed_of"], ["biology.breed_of_breed.breed_of"], ["biology.breed_breed.breed_of"]], "ground_paths": [["biology.animal_breed.breed_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are staffordshire terriers [/INST]", "raw_output": {"paths": ["<PATH> biology.animal_breed.breed_of </PATH>", "<PATH> biology.breed_of_breed.breed_of </PATH>", "<PATH> biology.breed_breed.breed_of </PATH>"], "scores": [-0.007227474823594093, -0.03917167708277702, -0.05013275891542435], "norm_scores": [0.3416977822780609, 0.33095502853393555, 0.32734718918800354]}}
{"id": "WebQTest-1513", "question": "what to do downtown sacramento", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do downtown sacramento [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.017067616805434227, -0.019389139488339424, -0.02214609645307064], "norm_scores": [0.3341558575630188, 0.3333809971809387, 0.3324631452560425]}}
{"id": "WebQTest-1515", "question": "who is michael j fox parents", "prediction": [["people.person.parents"], ["people.person.children"], ["influence.influence_node.influenced_by"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is michael j fox parents [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.011741451919078827, -0.016039615496993065, -0.1464599221944809], "norm_scores": [0.3484717607498169, 0.34697720408439636, 0.30455103516578674]}}
{"id": "WebQTest-1516", "question": "where did sanjay gupta go to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.honorary_degree_recipient.honorary_degrees", "education.honorary_degree.institution"], ["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did sanjay gupta go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.01674565114080906, -0.0195782408118248, -0.02931872569024563], "norm_scores": [0.33504462242126465, 0.33409690856933594, 0.3308584690093994]}}
{"id": "WebQTest-1518", "question": "what countries are involved in the nafta", "prediction": [["organization.membership_organization.members", "organization.organization_member.member_of"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["common.topic.notable_types"]], "ground_paths": [["organization.membership_organization.members", "organization.organization_membership.member"], ["organization.organization_founder.organizations_founded", "people.person.nationality"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["organization.membership_organization.members", "organization.organization_member.member_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are involved in the nafta [/INST]", "raw_output": {"paths": ["<PATH> organization.membership_organization.members <SEP> organization.organization_member.member_of </PATH>", "<PATH> organization.organization_membership.organization <SEP> organization.organization_member.member_of </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.011495514772832394, -0.024957090616226196, -0.041559185832738876], "norm_scores": [0.33817920088768005, 0.33365729451179504, 0.32816359400749207]}}
{"id": "WebQTest-1519", "question": "what continent is cayman islands located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what continent is cayman islands located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.009034940972924232, -0.021580154076218605, -0.07821924984455109], "norm_scores": [0.34238511323928833, 0.3381166458129883, 0.3194982707500458]}}
{"id": "WebQTest-1520", "question": "what language speak in brazil", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language speak in brazil [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.00108174211345613, -0.06727982312440872, -0.07576502859592438], "norm_scores": [0.34916409850120544, 0.32679855823516846, 0.3240373134613037]}}
{"id": "WebQTest-1522", "question": "who does kroy biermann play football for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"]], "ground_paths": [["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does kroy biermann play football for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>"], "scores": [-0.020031839609146118, -0.0233105830848217, -0.028753889724612236], "norm_scores": [0.33466726541519165, 0.33357179164886475, 0.3317610025405884]}}
{"id": "WebQTest-1523", "question": "when is the last time the pittsburgh steelers won a superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is the last time the pittsburgh steelers won a superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.010587559081614017, -0.01583941839635372, -0.034591756761074066], "norm_scores": [0.3365820646286011, 0.3348190486431122, 0.32859888672828674]}}
{"id": "WebQTest-1524", "question": "who invented islamic religion", "prediction": [["religion.religion.notable_figures"], ["religion.religion.founding_figures"], ["religion.belief.belief_of"]], "ground_paths": [["religion.religion.notable_figures"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who invented islamic religion [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.notable_figures </PATH>", "<PATH> religion.religion.founding_figures </PATH>", "<PATH> religion.belief.belief_of </PATH>"], "scores": [-0.0024795529898256063, -0.04386720433831215, -0.06787896156311035], "norm_scores": [0.34528595209121704, 0.33128705620765686, 0.3234270215034485]}}
{"id": "WebQTest-1525", "question": "who were we fighting in the gulf war", "prediction": [["military.military_combatant_group.conflict", "military.military_combatant.military_conflicts"], ["time.event.locations"], ["military.military_combatant_group.conflict", "military.military_combatant_group.combatants"]], "ground_paths": [["military.military_conflict.combatants", "military.military_combatant_group.combatants"], ["military.military_person.participated_in_conflicts", "people.person.nationality"], ["base.culturalevent.event.entity_involved", "people.person.nationality"], ["book.book_subject.works", "media_common.netflix_title.netflix_genres"], ["film.film.subjects", "film.film.country"], ["film.film_subject.films", "tv.tv_program.country_of_origin"], ["film.film_genre.films_in_this_genre", "film.film.country"], ["time.event.locations"], ["film.film_subject.films", "film.film.country"], ["film.film.genre", "film.film.country"], ["military.casualties.military_conflict", "military.military_combatant.casualties"], ["base.culturalevent.event.entity_involved", "people.person.place_of_birth"], ["military.military_conflict.casualties", "military.military_combatant.casualties"], ["time.event.included_in_event", "base.culturalevent.event.entity_involved"], ["time.event.includes_event", "base.culturalevent.event.entity_involved"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were we fighting in the gulf war [/INST]", "raw_output": {"paths": ["<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant.military_conflicts </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant_group.combatants </PATH>"], "scores": [-0.024717947468161583, -0.027084507048130035, -0.029338901862502098], "norm_scores": [0.3341100215911865, 0.333320289850235, 0.3325696885585785]}}
{"id": "WebQTest-1526", "question": "how many languages do they speak in switzerland", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how many languages do they speak in switzerland [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.015119405463337898, -0.022315410897135735, -0.02359665185213089], "norm_scores": [0.33507704734802246, 0.33267447352409363, 0.3322485089302063]}}
{"id": "WebQTest-1527", "question": "what political party does barack obama represent", "prediction": [["government.politician.party", "government.political_party_tenure.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "ground_paths": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["government.politician.election_campaigns", "government.election_campaign.party"], ["government.election.winner", "government.primary_election.party"], ["government.politician.party", "government.political_party_tenure.party"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party does barack obama represent [/INST]", "raw_output": {"paths": ["<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.party </PATH>", "<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>"], "scores": [-0.019100073724985123, -0.02098579704761505, -0.023357491940259933], "norm_scores": [0.3340161144733429, 0.3333868086338043, 0.33259710669517517]}}
{"id": "WebQTest-1528", "question": "what movies does logan lerman play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["film.actor.film", "film.film.starring"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.performance.film"], ["tv.tv_guest_role.actor", "tv.tv_guest_role.episodes_appeared_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies does logan lerman play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.0166492722928524, -0.036370228976011276, -0.036631952971220016], "norm_scores": [0.3377593457698822, 0.3311636745929718, 0.331076979637146]}}
{"id": "WebQTest-1529", "question": "where did emperor constantine die", "prediction": [["people.deceased_person.place_of_death"], ["location.deceased_person.place_of_death"], ["location.location.people_born_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did emperor constantine die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0006788374739699066, -0.07447933405637741, -0.0822572335600853], "norm_scores": [0.35081350803375244, 0.3258555829524994, 0.32333093881607056]}}
{"id": "WebQTest-1531", "question": "where is mali located", "prediction": [["location.location.containedby"], ["base.locations.countries.continent"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mali located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.016920296475291252, -0.022288115695118904, -0.040924299508333206], "norm_scores": [0.3365951478481293, 0.33479321002960205, 0.32861170172691345]}}
{"id": "WebQTest-1533", "question": "what planet did luke skywalker came from", "prediction": [["fictional_universe.fictional_setting.characters_that_have_lived_here"], ["fictional_universe.fictional_character.place_of_birth"], ["fictional_universe.fictional_character.places_lived"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what planet did luke skywalker came from [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_setting.characters_that_have_lived_here </PATH>", "<PATH> fictional_universe.fictional_character.place_of_birth </PATH>", "<PATH> fictional_universe.fictional_character.places_lived </PATH>"], "scores": [-0.01226892601698637, -0.01880187727510929, -0.020028242841362953], "norm_scores": [0.3349232077598572, 0.332742303609848, 0.3323344886302948]}}
{"id": "WebQTest-1534", "question": "who are the coaches for the oakland raiders", "prediction": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["american_football.football_team.current_head_coach"], ["american_football.football_coach.current_team_head_coached"]], "ground_paths": [["american_football.football_coach.current_team_head_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the coaches for the oakland raiders [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> american_football.football_team.current_head_coach </PATH>", "<PATH> american_football.football_coach.current_team_head_coached </PATH>"], "scores": [-0.015950916334986687, -0.021579168736934662, -0.02545621059834957], "norm_scores": [0.33501654863357544, 0.33313626050949097, 0.3318471908569336]}}
{"id": "WebQTest-1535", "question": "where was the first microsoft headquarters located", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["base.engineering1.rfid_company.location", "base.biblioness.bibs_location.country"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the first microsoft headquarters located [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0179604459553957, -0.033964306116104126, -0.04265210032463074], "norm_scores": [0.33786818385124207, 0.3325040340423584, 0.3296278119087219]}}
{"id": "WebQTest-1536", "question": "where did john steinbeck go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.nationality", "base.biblioness.bibs_location.country"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did john steinbeck go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.01649419032037258, -0.018151547759771347, -0.018414949998259544], "norm_scores": [0.3337310254573822, 0.3331783413887024, 0.333090603351593]}}
{"id": "WebQTest-1537", "question": "what did niels bohr discover about the atomic structure", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["base.nobelprizes.nobel_honor.nobel_prize_winner"]], "ground_paths": [["base.nobelprizes.nobel_honor.nobel_prize_winner", "base.nobelprizes.nobel_honor.subject_area"], ["education.academic.advisees", "book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did niels bohr discover about the atomic structure [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.nobelprizes.nobel_honor.nobel_prize_winner </PATH>"], "scores": [-0.012277967296540737, -0.02488747611641884, -0.03444022685289383], "norm_scores": [0.3372054398059845, 0.3329801559448242, 0.3298144042491913]}}
{"id": "WebQTest-1538", "question": "what books did louis sachar wrote", "prediction": [["book.book_edition.author_editor"], ["book.author.works_written"], ["book.written_work.author"]], "ground_paths": [["book.author.works_written"], ["book.book_edition.author_editor"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what books did louis sachar wrote [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.012111613526940346, -0.025598015636205673, -0.02956383302807808], "norm_scores": [0.33677932620048523, 0.3322678804397583, 0.3309527635574341]}}
{"id": "WebQTest-1539", "question": "who is ryan reynolds married to 2012", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.spouse_s", "people.person.spouse_s"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["people.person.spouse_s", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is ryan reynolds married to 2012 [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.017514411360025406, -0.03010355867445469, -0.03176546469330788], "norm_scores": [0.3363221287727356, 0.33211466670036316, 0.33156320452690125]}}
{"id": "WebQTest-1540", "question": "what is newt gingrich 's wife 's name", "prediction": [["people.person.nationality", "people.person.nationality"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.children"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"], ["people.person.profession", "common.topic.notable_types"], ["people.person.religion", "people.person.religion"], ["film.film.executive_produced_by", "film.film.executive_produced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is newt gingrich 's wife 's name [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.023950213566422462, -0.036403588950634, -0.039314448833465576], "norm_scores": [0.336431086063385, 0.3322673738002777, 0.33130159974098206]}}
{"id": "WebQTest-1541", "question": "what type of music did antonio vivaldi play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did antonio vivaldi play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.002486956072971225, -0.0376717671751976, -0.11045435816049576], "norm_scores": [0.3492737412452698, 0.33719828724861145, 0.3135279715061188]}}
{"id": "WebQTest-1542", "question": "what school does james franco go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school does james franco go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.014820476993918419, -0.0162724070250988, -0.023996280506253242], "norm_scores": [0.3345136046409607, 0.3340282440185547, 0.331458181142807]}}
{"id": "WebQTest-1543", "question": "what is samuel langhorne clemens well known for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["fictional_universe.fictional_character.occupation"]], "ground_paths": [["people.person.profession"], ["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is samuel langhorne clemens well known for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>"], "scores": [-0.001455298624932766, -0.04666807875037193, -0.09022578597068787], "norm_scores": [0.34832894802093506, 0.3329307734966278, 0.3187403380870819]}}
{"id": "WebQTest-1544", "question": "who was president after william henry harrison", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["people.person.gender", "people.person.gender"]], "ground_paths": [["government.government_office_or_title.jurisdiction", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["freebase.type_profile.equivalent_topic", "common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was president after william henry harrison [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.015163013711571693, -0.02256866544485092, -0.04377229884266853], "norm_scores": [0.33733436465263367, 0.3348454236984253, 0.32782021164894104]}}
{"id": "WebQTest-1546", "question": "where did rick santorum attend high school", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did rick santorum attend high school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.016124000772833824, -0.02223682589828968, -0.02764960750937462], "norm_scores": [0.3352952003479004, 0.33325183391571045, 0.3314529359340668]}}
{"id": "WebQTest-1547", "question": "what is the title of george bush 's new book", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.book_edition.author_editor"]], "ground_paths": [["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the title of george bush 's new book [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.009705104865133762, -0.023760968819260597, -0.04581119492650032], "norm_scores": [0.3389163613319397, 0.334185928106308, 0.3268977105617523]}}
{"id": "WebQTest-1548", "question": "where did antoine lavoisier die", "prediction": [["people.deceased_person.place_of_death"], ["deceased_person.place_of_death"], ["people.place_of_death.people_deceased_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did antoine lavoisier die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>"], "scores": [-0.00023397378390654922, -0.1036367192864418, -0.10496720671653748], "norm_scores": [0.3568461239337921, 0.3217909038066864, 0.32136303186416626]}}
{"id": "WebQTest-1550", "question": "what is the name of king george vi wife", "prediction": [["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.children", "people.person.parents"], ["people.family.members", "people.family.members"], ["film.film.subjects", "film.film_subject.films"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["people.person.religion", "people.person.religion"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of king george vi wife [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.03651369363069534, -0.03685806319117546, -0.03907213360071182], "norm_scores": [0.33365580439567566, 0.3335409164428711, 0.33280327916145325]}}
{"id": "WebQTest-1551", "question": "what year did seattle seahawks go to the super bowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did seattle seahawks go to the super bowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.01807529293000698, -0.021721597760915756, -0.025225475430488586], "norm_scores": [0.33453369140625, 0.3333160877227783, 0.33215025067329407]}}
{"id": "WebQTest-1553", "question": "where does sami yusuf live", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does sami yusuf live [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.02448582835495472, -0.028670310974121094, -0.03914427384734154], "norm_scores": [0.3354272246360779, 0.3340265452861786, 0.33054623007774353]}}
{"id": "WebQTest-1554", "question": "what is the official language of brazil now", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the official language of brazil now [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.0005195270641706884, -0.07373546808958054, -0.09046182781457901], "norm_scores": [0.35169360041618347, 0.326864093542099, 0.32144230604171753]}}
{"id": "WebQTest-1555", "question": "what is the parliament of nepal called", "prediction": [["government.governmental_jurisdiction.government_bodies"], ["government.governmental_jurisdiction.governing_officials", "government.governmental_body.members"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.governmental_body"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the parliament of nepal called [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.government_bodies </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.governmental_body.members </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.governmental_body </PATH>"], "scores": [-0.016778791323304176, -0.018475813791155815, -0.026961149647831917], "norm_scores": [0.3346525728702545, 0.3340851366519928, 0.3312623202800751]}}
{"id": "WebQTest-1556", "question": "when 's the last time the mets won the world series", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when 's the last time the mets won the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.0109620476141572, -0.011657515540719032, -0.0777086466550827], "norm_scores": [0.34074604511260986, 0.3405091464519501, 0.31874480843544006]}}
{"id": "WebQTest-1557", "question": "where do florida panthers play", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_team.location"]], "ground_paths": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do florida panthers play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.location </PATH>"], "scores": [-0.007012615911662579, -0.024143196642398834, -0.063966765999794], "norm_scores": [0.34157055616378784, 0.33576908707618713, 0.32266032695770264]}}
{"id": "WebQTest-1558", "question": "who is john cena married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "ground_paths": [["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.person.spouse_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is john cena married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>"], "scores": [-0.01994526945054531, -0.033688656985759735, -0.03513642027974129], "norm_scores": [0.33655595779418945, 0.3319621682167053, 0.3314819037914276]}}
{"id": "WebQTest-1559", "question": "what to do san jose ca", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["film.film_festival.location"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["location.neighborhood.neighborhood_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do san jose ca [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01636018045246601, -0.016371021047234535, -0.028266876935958862], "norm_scores": [0.33465486764907837, 0.33465126156806946, 0.3306938409805298]}}
{"id": "WebQTest-1560", "question": "who founded new york university", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["education.educational_institution.parent_institution", "people.person.parents"]], "ground_paths": [["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who founded new york university [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> people.person.parents </PATH>"], "scores": [-0.026224734261631966, -0.03158469870686531, -0.03594931587576866], "norm_scores": [0.33501094579696655, 0.33322009444236755, 0.3317689001560211]}}
{"id": "WebQTest-1561", "question": "who created arthur", "prediction": [["tv.tv_program_creator.programs_created"], ["tv.tv_program.program_creator"], ["fictional_universe.fictional_character_creator.fictional_characters_created"]], "ground_paths": [["tv.tv_program_creator.programs_created"], ["tv.tv_program.program_creator"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who created arthur [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program_creator.programs_created </PATH>", "<PATH> tv.tv_program.program_creator </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>"], "scores": [-0.007002726662904024, -0.03761659190058708, -0.046981241554021835], "norm_scores": [0.34122005105018616, 0.33093225955963135, 0.3278476595878601]}}
{"id": "WebQTest-1562", "question": "what countries constitute the eu", "prediction": [["organization.organization_membership.organization", "organization.organization_member.member_of"], ["organization.organization.geographic_scope", "base.locations.continents.countries_within"], ["organization.organization.geographic_scope", "location.location.containedby"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.district_represented"], ["organization.organization.geographic_scope", "base.locations.countries.continent"], ["organization.membership_organization.members", "organization.organization_membership.member"], ["government.government_position_held.jurisdiction_of_office", "government.political_district.representatives"], ["organization.organization.geographic_scope", "base.locations.continents.countries_within"], ["organization.organization.geographic_scope", "location.location.primarily_containedby"], ["base.industrystandards.standard_organization.standards", "base.aareas.coding_scheme.geo_scope"], ["organization.membership_organization.members", "organization.organization_member.member_of"], ["government.governmental_jurisdiction.governing_officials", "government.political_district.representatives"], ["organization.organization.geographic_scope", "location.location.containedby"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["organization.organization_membership.organization", "organization.organization_membership.member"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.district_represented"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries constitute the eu [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_membership.organization <SEP> organization.organization_member.member_of </PATH>", "<PATH> organization.organization.geographic_scope <SEP> base.locations.continents.countries_within </PATH>", "<PATH> organization.organization.geographic_scope <SEP> location.location.containedby </PATH>"], "scores": [-0.029476415365934372, -0.032076627016067505, -0.0334637314081192], "norm_scores": [0.33406564593315125, 0.3331981301307678, 0.3327362537384033]}}
{"id": "WebQTest-1563", "question": "what are all the countries in north africa", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.countries.continent"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are all the countries in north africa [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.005114622879773378, -0.03322840854525566, -0.0711483359336853], "norm_scores": [0.34383440017700195, 0.33430251479148865, 0.3218631148338318]}}
{"id": "WebQTest-1564", "question": "what did bobby darin die of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did bobby darin die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>"], "scores": [-0.0014592802617698908, -0.044332947582006454, -0.11038335412740707], "norm_scores": [0.3502834439277649, 0.3355828821659088, 0.3141336441040039]}}
{"id": "WebQTest-1565", "question": "where does houston dynamo play", "prediction": [["sports.sports_facility.teams"], ["sports.sports_team.arena_stadium"], ["sports.sports_team.location"]], "ground_paths": [["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does houston dynamo play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_team.location </PATH>"], "scores": [-0.011957616545259953, -0.013487502001225948, -0.08318564295768738], "norm_scores": [0.3413294553756714, 0.34080764651298523, 0.3178628981113434]}}
{"id": "WebQTest-1566", "question": "what kind of guitar john mayer", "prediction": [["music.guitar.guitarists"], ["music.guitarist.guitars_played"], ["music.instrument.family"]], "ground_paths": [["music.guitar.guitarists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of guitar john mayer [/INST]", "raw_output": {"paths": ["<PATH> music.guitar.guitarists </PATH>", "<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.instrument.family </PATH>"], "scores": [-0.014714176766574383, -0.015805983915925026, -0.05214783549308777], "norm_scores": [0.33759069442749023, 0.33722230792045593, 0.3251870274543762]}}
{"id": "WebQTest-1567", "question": "what cancer did patrick swayze", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["people.deceased_person.cause_of_death"]], "ground_paths": [["medicine.disease.notable_people_with_this_condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what cancer did patrick swayze [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.0016559746582061052, -0.04579117149114609, -0.05877654254436493], "norm_scores": [0.3446725308895111, 0.3297911584377289, 0.3255363702774048]}}
{"id": "WebQTest-1568", "question": "what university did obama graduated from", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what university did obama graduated from [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.014167127199470997, -0.016146156936883926, -0.026224419474601746], "norm_scores": [0.3348918855190277, 0.33422979712486267, 0.33087825775146484]}}
{"id": "WebQTest-1569", "question": "what jamaican money called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what jamaican money called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.0005219523445703089, -0.07259884476661682, -0.13523973524570465], "norm_scores": [0.3565797209739685, 0.3317829668521881, 0.31163734197616577]}}
{"id": "WebQTest-1570", "question": "where is the arizona cardinals stadium located", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_team_location.teams"]], "ground_paths": [["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the arizona cardinals stadium located [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team_location.teams </PATH>"], "scores": [-0.0060102250427007675, -0.025997493416070938, -0.053167980164289474], "norm_scores": [0.340814471244812, 0.3340701162815094, 0.32511547207832336]}}
{"id": "WebQTest-1571", "question": "what is sir isaac newton theory", "prediction": [["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["law.invention.inventor"]], "ground_paths": [["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is sir isaac newton theory [/INST]", "raw_output": {"paths": ["<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> law.invention.inventor </PATH>"], "scores": [-0.007514773402363062, -0.023149680346250534, -0.06979331374168396], "norm_scores": [0.3419846296310425, 0.3366793096065521, 0.3213360011577606]}}
{"id": "WebQTest-1572", "question": "what should i do today in san francisco", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["location.neighborhood.neighborhood_of"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what should i do today in san francisco [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.015543066896498203, -0.01919132098555565, -0.02186291292309761], "norm_scores": [0.3344416320323944, 0.33322373032569885, 0.3323346674442291]}}
{"id": "WebQTest-1573", "question": "who influenced picasso art", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who influenced picasso art [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.011972537264227867, -0.013070912100374699, -0.11048231273889542], "norm_scores": [0.3442235291004181, 0.3438456356525421, 0.3119308054447174]}}
{"id": "WebQTest-1574", "question": "what is my timezone in pennsylvania", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is my timezone in pennsylvania [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-1.473951488151215e-05, -0.131673201918602, -0.18913619220256805], "norm_scores": [0.3697778582572937, 0.32416218519210815, 0.30605995655059814]}}
{"id": "WebQTest-1575", "question": "what products and\\/or services does google offer customers", "prediction": [["organization.organization.sectors"], ["organization.organization_founder.organizations_founded"], ["organization.organization.sectors", "organization.organization.sectors"]], "ground_paths": [["internet.website.owner"], ["computer.computer_manufacturer_brand.computer_models"], ["computer.software.developer"], ["business.consumer_company.products"], ["business.company_product_relationship.company", "business.consumer_product.company"], ["computer.software_developer.software"], ["projects.project_participation.participant", "projects.project_participation.project"], ["business.consumer_company.products", "business.consumer_product.company"], ["business.company_product_relationship.company", "business.company_product_relationship.consumer_product"], ["business.consumer_company.products", "business.company_product_relationship.consumer_product"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what products and\\/or services does google offer customers [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.sectors </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.sectors <SEP> organization.organization.sectors </PATH>"], "scores": [-0.04367188364267349, -0.050137367099523544, -0.055408526211977005], "norm_scores": [0.3353580832481384, 0.33319681882858276, 0.3314451277256012]}}
{"id": "WebQTest-1577", "question": "who did cliff lee play for last year", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did cliff lee play for last year [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.014274493791162968, -0.014670921489596367, -0.029972311109304428], "norm_scores": [0.3351174294948578, 0.3349846303462982, 0.3298979103565216]}}
{"id": "WebQTest-1580", "question": "what type of democracy is the united states", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.planet"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of democracy is the united states [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.planet </PATH>"], "scores": [-0.00871801096946001, -0.015363997779786587, -0.1620507389307022], "norm_scores": [0.35072702169418335, 0.34840381145477295, 0.3008691370487213]}}
{"id": "WebQTest-1582", "question": "what time in china hong kong", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["base.locations.countries.continent"]], "ground_paths": [["time.time_zone.locations_in_this_time_zone"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time in china hong kong [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.005470023490488529, -0.024396007880568504, -0.08510806411504745], "norm_scores": [0.344269335269928, 0.33781498670578003, 0.3179157078266144]}}
{"id": "WebQTest-1583", "question": "what all movies has taylor lautner been in", "prediction": [["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"]], "ground_paths": [["film.actor.film", "film.film.starring"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.actor.film", "film.performance.film"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what all movies has taylor lautner been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>"], "scores": [-0.024233022704720497, -0.02930842712521553, -0.02989118918776512], "norm_scores": [0.33452701568603516, 0.3328334391117096, 0.33263951539993286]}}
{"id": "WebQTest-1584", "question": "what disease did bernie mac die of", "prediction": [["people.deceased_person.cause_of_death"], ["medicine.disease.notable_people_with_this_condition"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what disease did bernie mac die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.004598779603838921, -0.0346582755446434, -0.039748623967170715], "norm_scores": [0.34061700105667114, 0.33053058385849, 0.3288523256778717]}}
{"id": "WebQTest-1585", "question": "who plays spike spiegel", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["base.schemastaging.tv_character_extra.regular_dubbing_performances", "base.schemastaging.tv_star_dubbing_performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays spike spiegel [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.01664438471198082, -0.035338424146175385, -0.035402942448854446], "norm_scores": [0.3375076353549957, 0.3312568664550781, 0.33123549818992615]}}
{"id": "WebQTest-1586", "question": "who was queen isabella 's mother", "prediction": [["people.person.parents"], ["people.person.children"], ["influence.influence_node.influenced"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was queen isabella 's mother [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.008893323130905628, -0.01979154720902443, -0.12569157779216766], "norm_scores": [0.3473517894744873, 0.343586802482605, 0.30906134843826294]}}
{"id": "WebQTest-1587", "question": "who does chris hemsworth have a baby with", "prediction": [["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"]], "ground_paths": [["celebrities.friendship.friend", "celebrities.friendship.friend"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.parents"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.marriage.spouse", "people.person.spouse_s"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does chris hemsworth have a baby with [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.02174852043390274, -0.02472606860101223, -0.0269983671605587], "norm_scores": [0.33424797654151917, 0.33325421810150146, 0.33249783515930176]}}
{"id": "WebQTest-1589", "question": "where is the mts iceplex in winnipeg", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the mts iceplex in winnipeg [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.01401096023619175, -0.026009894907474518, -0.030140221118927002], "norm_scores": [0.3364655077457428, 0.3324523866176605, 0.3310820758342743]}}
{"id": "WebQTest-1590", "question": "what are the most popular religions in israel", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.quotations", "people.person.religion"]], "ground_paths": [["symbols.namesake.named_after", "religion.religion.notable_figures"], ["people.person.nationality", "people.person.religion"], ["film.film_location.featured_in_films", "media_common.netflix_title.netflix_genres"], ["location.country.administrative_divisions", "religion.religion.sacred_sites"], ["people.person.quotations", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the most popular religions in israel [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.quotations <SEP> people.person.religion </PATH>"], "scores": [-0.0030920703429728746, -0.030313992872834206, -0.05901551619172096], "norm_scores": [0.3426116406917572, 0.3334108889102936, 0.3239775002002716]}}
{"id": "WebQTest-1591", "question": "what was obama 's former occupation", "prediction": [["people.person.profession"], ["people.person.profession", "base.onephylogeny.type_of_thing.things_of_this_type"], ["people.person.profession", "people.profession.specializations"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was obama 's former occupation [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.person.profession <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> people.person.profession <SEP> people.profession.specializations </PATH>"], "scores": [-0.00777056347578764, -0.03507648780941963, -0.040034644305706024], "norm_scores": [0.33998405933380127, 0.330826073884964, 0.32918983697891235]}}
{"id": "WebQTest-1592", "question": "what are all the songs justin bieber wrote", "prediction": [["music.composer.compositions"], ["music.recording.artist"], ["music.composition.composer"]], "ground_paths": [["music.featured_artist.recordings"], ["music.composer.compositions"], ["music.lyricist.lyrics_written"], ["music.featured_artist.albums"], ["music.recording.featured_artists"], ["music.album.artist"], ["music.artist.album"], ["music.recording.artist"], ["music.composition.composer"], ["music.composition.lyricist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are all the songs justin bieber wrote [/INST]", "raw_output": {"paths": ["<PATH> music.composer.compositions </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.composition.composer </PATH>"], "scores": [-0.017548779025673866, -0.02203477919101715, -0.033401068300008774], "norm_scores": [0.3355933427810669, 0.3340912461280823, 0.33031535148620605]}}
{"id": "WebQTest-1593", "question": "where is the thames river source", "prediction": [["geography.river.origin"], ["geography.river.basin_countries"], ["geography.river.mouth"]], "ground_paths": [["geography.river.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the thames river source [/INST]", "raw_output": {"paths": ["<PATH> geography.river.origin </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> geography.river.mouth </PATH>"], "scores": [-0.0050214678049087524, -0.051926374435424805, -0.05346293747425079], "norm_scores": [0.3440101444721222, 0.32824695110321045, 0.32774296402931213]}}
{"id": "WebQTest-1595", "question": "what new movies is robert pattinson in", "prediction": [["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["film.actor.film", "film.film.starring"], ["music.artist.track", "music.recording.releases"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.producer.films_executive_produced"], ["music.recording.artist", "music.recording.releases"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what new movies is robert pattinson in [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.02839536964893341, -0.030623074620962143, -0.032393328845500946], "norm_scores": [0.3340253233909607, 0.33328208327293396, 0.33269259333610535]}}
{"id": "WebQTest-1596", "question": "what language do they use in pakistan", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they use in pakistan [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01597849279642105, -0.01774417608976364, -0.03429388627409935], "norm_scores": [0.3355606496334076, 0.3349686861038208, 0.3294706344604492]}}
{"id": "WebQTest-1597", "question": "what kind of products does nike sell", "prediction": [["business.business_operation.industry"], ["business.industry.companies"], ["business.industry.parent_industry"]], "ground_paths": [["business.industry.companies"], ["business.business_operation.industry"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of products does nike sell [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry </PATH>", "<PATH> business.industry.companies </PATH>", "<PATH> business.industry.parent_industry </PATH>"], "scores": [-0.01161663606762886, -0.029215488582849503, -0.05306161567568779], "norm_scores": [0.33990976214408875, 0.33398008346557617, 0.3261101543903351]}}
{"id": "WebQTest-1598", "question": "who did lbj marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.parents", "people.person.parents"]], "ground_paths": [["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_owner.pets_owned"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_owner.pets_owned"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.parents"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_ownership.owner"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.marriage.spouse", "people.person.spouse_s"], ["book.written_work.subjects", "people.person.nationality"], ["people.person.profession", "common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did lbj marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>"], "scores": [-0.026541264727711678, -0.04053473472595215, -0.046215470880270004], "norm_scores": [0.33708372712135315, 0.3323996067047119, 0.3305166959762573]}}
{"id": "WebQTest-1599", "question": "where did kim jong il die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here"], ["people.place_of_death.people"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kim jong il die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.place_of_death.people </PATH>"], "scores": [-0.00036778912181034684, -0.09673500806093216, -0.10995063185691833], "norm_scores": [0.3565903306007385, 0.3238305449485779, 0.3195790946483612]}}
{"id": "WebQTest-1603", "question": "what is the sacred text of daoism", "prediction": [["religion.religion.texts"], ["religion.religious_text.religious_text_of"], ["religion.religion.sacred_sites"]], "ground_paths": [["religion.religion.notable_figures"], ["book.written_work.subjects"], ["religion.religion.texts"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the sacred text of daoism [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.texts </PATH>", "<PATH> religion.religious_text.religious_text_of </PATH>", "<PATH> religion.religion.sacred_sites </PATH>"], "scores": [-0.009411506354808807, -0.014503901824355125, -0.11502181738615036], "norm_scores": [0.3454594612121582, 0.3437047302722931, 0.3108358383178711]}}
{"id": "WebQTest-1604", "question": "what countries speak english natively", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries speak english natively [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01652316376566887, -0.021316489204764366, -0.025332514196634293], "norm_scores": [0.33484601974487305, 0.33324483036994934, 0.3319091796875]}}
{"id": "WebQTest-1607", "question": "who won fedex cup 2012", "prediction": [["award.competition.winner", "people.person.profession"], ["award.competition.winner", "people.person.gender", "people.person.gender"], ["award.competitor.competitions", "people.person.profession"]], "ground_paths": [["sports.sports_award.award", "sports.sports_award.award_winner"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won fedex cup 2012 [/INST]", "raw_output": {"paths": ["<PATH> award.competition.winner <SEP> people.person.profession </PATH>", "<PATH> award.competition.winner <SEP> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> award.competitor.competitions <SEP> people.person.profession </PATH>"], "scores": [-0.017934096977114677, -0.02782776765525341, -0.029799843207001686], "norm_scores": [0.335755318403244, 0.33244985342025757, 0.3317948579788208]}}
{"id": "WebQTest-1608", "question": "what language is mainly spoken in egypt", "prediction": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is mainly spoken in egypt [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.02268943376839161, -0.023555759340524673, -0.026595357805490494], "norm_scores": [0.3338635563850403, 0.33357444405555725, 0.33256202936172485]}}
{"id": "WebQTest-1609", "question": "who played alf on tv show", "prediction": [["tv.tv_program_creator.programs_created"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played alf on tv show [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program_creator.programs_created </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.017664914950728416, -0.020789366215467453, -0.048433925956487656], "norm_scores": [0.3370884954929352, 0.33603692054748535, 0.32687458395957947]}}
{"id": "WebQTest-1610", "question": "who was gerald ford vp", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was gerald ford vp [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.005346649792045355, -0.025029590353369713, -0.0904531329870224], "norm_scores": [0.34495559334754944, 0.33823224902153015, 0.3168122172355652]}}
{"id": "WebQTest-1611", "question": "what is armenian currency", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is armenian currency [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.0066264658235013485, -0.02594565413892269, -0.07909329980611801], "norm_scores": [0.34352895617485046, 0.3369559347629547, 0.3195151090621948]}}
{"id": "WebQTest-1612", "question": "what type of art did raphael sanzio create", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art did raphael sanzio create [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.004621601663529873, -0.02252976782619953, -0.05880831927061081], "norm_scores": [0.34135445952415466, 0.33529579639434814, 0.3233497738838196]}}
{"id": "WebQTest-1613", "question": "what language do they speak in malta", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they speak in malta [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.01835261844098568, -0.021567225456237793, -0.026982750743627548], "norm_scores": [0.3346498906612396, 0.33357587456703186, 0.3317742347717285]}}
{"id": "WebQTest-1614", "question": "what religion was king louis xiv", "prediction": [["people.person.religion"], ["royalty.monarch.kingdom", "people.person.religion"], ["religion.religion.notable_figures"]], "ground_paths": [["people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religion was king louis xiv [/INST]", "raw_output": {"paths": ["<PATH> people.person.religion </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.religion </PATH>", "<PATH> religion.religion.notable_figures </PATH>"], "scores": [-0.0017274923156946898, -0.052931152284145355, -0.0659017562866211], "norm_scores": [0.3462691605091095, 0.32898518443107605, 0.32474562525749207]}}
{"id": "WebQTest-1615", "question": "what football team does mike wright play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what football team does mike wright play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.0157182477414608, -0.016479751095175743, -0.030291588976979256], "norm_scores": [0.3350340723991394, 0.3347790539264679, 0.3301869332790375]}}
{"id": "WebQTest-1616", "question": "what did shannon hoon die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did shannon hoon die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0024610923137515783, -0.03612871840596199, -0.12511825561523438], "norm_scores": [0.3506975769996643, 0.33908694982528687, 0.31021547317504883]}}
{"id": "WebQTest-1617", "question": "what did nicholas copernicus discover", "prediction": [["symbols.namesake.named_after"], ["base.argumentmaps.original_idea.innovator"], ["base.argumentmaps.innovator.original_ideas"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], ["symbols.namesake.named_after"], ["base.argumentmaps.original_idea.innovator"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did nicholas copernicus discover [/INST]", "raw_output": {"paths": ["<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.argumentmaps.original_idea.innovator </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>"], "scores": [-0.015534351579844952, -0.02317063882946968, -0.03539636358618736], "norm_scores": [0.33639147877693176, 0.3338324725627899, 0.32977601885795593]}}
{"id": "WebQTest-1618", "question": "what teams has manny ramirez played for", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_coach.teams_coached", "sports.sports_team.coaches"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what teams has manny ramirez played for [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>"], "scores": [-0.018608786165714264, -0.021765269339084625, -0.021900320425629616], "norm_scores": [0.33405017852783203, 0.3329974114894867, 0.33295243978500366]}}
{"id": "WebQTest-1619", "question": "where do the appalachian mountains run", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["geography.mountain_range.mountains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the appalachian mountains run [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> geography.mountain_range.mountains </PATH>"], "scores": [-0.008319295011460781, -0.020490571856498718, -0.08169903606176376], "norm_scores": [0.3428003191947937, 0.3386532664299011, 0.31854644417762756]}}
{"id": "WebQTest-1620", "question": "when was president wilson in office", "prediction": [["book.written_work.subjects", "base.culturalevent.event.entity_involved"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["government.government_position_held.basic_title", "government.government_position_held.jurisdiction_of_office", "location.location.events"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was president wilson in office [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.jurisdiction_of_office <SEP> location.location.events </PATH>"], "scores": [-0.03131481260061264, -0.03333646431565285, -0.03341872617602348], "norm_scores": [0.3337918817996979, 0.3331177532672882, 0.3330903649330139]}}
{"id": "WebQTest-1621", "question": "what political party is jerry brown", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"], ["government.election_campaign.candidate", "government.election_campaign.party"]], "ground_paths": [["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party is jerry brown [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.party </PATH>"], "scores": [-0.013715393841266632, -0.0185085516422987, -0.02752949669957161], "norm_scores": [0.3354017436504364, 0.33379796147346497, 0.330800324678421]}}
{"id": "WebQTest-1622", "question": "what ocean is around hawaii", "prediction": [["location.location.contains", "location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["location.location.containedby", "location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what ocean is around hawaii [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.contains </PATH>"], "scores": [-0.027448702603578568, -0.029034141451120377, -0.03938252851366997], "norm_scores": [0.3348342180252075, 0.33430373668670654, 0.3308620750904083]}}
{"id": "WebQTest-1623", "question": "what is the most common language spoken in argentina", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the most common language spoken in argentina [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.00730936648324132, -0.027960024774074554, -0.04558093473315239], "norm_scores": [0.3399033844470978, 0.33295613527297974, 0.3271405100822449]}}
{"id": "WebQTest-1624", "question": "what language tunisia", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language tunisia [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.016787629574537277, -0.017049895599484444, -0.045959919691085815], "norm_scores": [0.3365885019302368, 0.33650025725364685, 0.3269112706184387]}}
{"id": "WebQTest-1625", "question": "what are some inventions that leonardo da vinci invented", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], ["law.invention.inventor"], ["law.inventor.inventions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some inventions that leonardo da vinci invented [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>"], "scores": [-0.00710702920332551, -0.01886747032403946, -0.07578825205564499], "norm_scores": [0.34223923087120056, 0.3382379114627838, 0.3195228576660156]}}
{"id": "WebQTest-1626", "question": "who wrote st trinians", "prediction": [["film.film.written_by"], ["fictional_universe.fictional_universe.locations"], ["film.film_story_contributor.film_story_credits"]], "ground_paths": [["fictional_universe.fictional_universe.created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who wrote st trinians [/INST]", "raw_output": {"paths": ["<PATH> film.film.written_by </PATH>", "<PATH> fictional_universe.fictional_universe.locations </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>"], "scores": [-0.02233153022825718, -0.03485235199332237, -0.04299541190266609], "norm_scores": [0.3370288014411926, 0.33283522725105286, 0.33013594150543213]}}
{"id": "WebQTest-1627", "question": "what are malaysian coins called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are malaysian coins called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.0065985070541501045, -0.027207227423787117, -0.06355869024991989], "norm_scores": [0.34196990728378296, 0.33499446511268616, 0.3230356276035309]}}
{"id": "WebQTest-1628", "question": "what type government does mexico have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type government does mexico have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.00588077399879694, -0.022680768743157387, -0.14287488162517548], "norm_scores": [0.35022395849227905, 0.3443893492221832, 0.30538666248321533]}}
{"id": "WebQTest-1629", "question": "what currency does dominican republic take", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does dominican republic take [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.00044258745037950575, -0.07351747900247574, -0.1339748501777649], "norm_scores": [0.3565658926963806, 0.33143913745880127, 0.31199488043785095]}}
{"id": "WebQTest-1630", "question": "what inspired michael jackson to become a singer", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["people.person.profession"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inspired michael jackson to become a singer [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.006209319457411766, -0.019875533878803253, -0.09115462750196457], "norm_scores": [0.344235360622406, 0.33956295251846313, 0.31620168685913086]}}
{"id": "WebQTest-1631", "question": "who is the texas state senator", "prediction": [["location.administrative_division.country", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "ground_paths": [["location.us_state.capital", "people.deceased_person.place_of_death"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.political_district.elections", "government.election.winner"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["location.administrative_division.country", "people.person.nationality"], ["location.location.events", "military.military_person.participated_in_conflicts"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["base.biblioness.bibs_location.state", "symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the texas state senator [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.country <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.016881447285413742, -0.02573401853442192, -0.02862725406885147], "norm_scores": [0.3356257379055023, 0.33266767859458923, 0.33170661330223083]}}
{"id": "WebQTest-1632", "question": "what to visit in london city", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["transportation.bridge.locale"], ["organization.organization.geographic_scope"], ["travel.tourist_attraction.near_travel_destination"], ["metropolitan_transit.transit_system.area_served"], ["location.location.containedby"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to visit in london city [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.012310837395489216, -0.02032763697206974, -0.031229741871356964], "norm_scores": [0.33632954955101013, 0.3336440324783325, 0.3300263583660126]}}
{"id": "WebQTest-1633", "question": "what genre of art is the mona lisa", "prediction": [["visual_art.artwork.art_form"], ["visual_art.visual_artist.art_forms"], ["visual_art.art_subject.artwork_on_the_subject"]], "ground_paths": [["visual_art.artwork.art_genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what genre of art is the mona lisa [/INST]", "raw_output": {"paths": ["<PATH> visual_art.artwork.art_form </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_subject.artwork_on_the_subject </PATH>"], "scores": [-0.018784357234835625, -0.028386646881699562, -0.039418771862983704], "norm_scores": [0.3366979956626892, 0.3334803879261017, 0.3298215866088867]}}
{"id": "WebQTest-1634", "question": "who does peter griffin voice", "prediction": [["fictional_universe.fictional_character.character_created_by"], ["film.performance.character", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "ground_paths": [["fictional_universe.fictional_character.character_created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does peter griffin voice [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.017688408493995667, -0.033603206276893616, -0.03921334445476532], "norm_scores": [0.3375053107738495, 0.3321765065193176, 0.3303181827068329]}}
{"id": "WebQTest-1635", "question": "when did tony romo got drafted", "prediction": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.draft"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did tony romo got drafted [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.draft </PATH>"], "scores": [-0.01287892833352089, -0.014807208441197872, -0.026001790538430214], "norm_scores": [0.3350042998790741, 0.33435893058776855, 0.33063679933547974]}}
{"id": "WebQTest-1637", "question": "what are the catholic holy days", "prediction": [["religion.religion.holidays"], ["time.holiday.featured_in_religions"], ["time.recurring_event.holidays"]], "ground_paths": [["religion.religion.holidays"], ["time.holiday.featured_in_religions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the catholic holy days [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.holidays </PATH>", "<PATH> time.holiday.featured_in_religions </PATH>", "<PATH> time.recurring_event.holidays </PATH>"], "scores": [-0.011512363329529762, -0.014491673558950424, -0.09239312261343002], "norm_scores": [0.34254446625709534, 0.34152546525001526, 0.31593000888824463]}}
{"id": "WebQTest-1638", "question": "what language is most commonly spoken in belgium", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is most commonly spoken in belgium [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.011047669686377048, -0.019661802798509598, -0.03539656102657318], "norm_scores": [0.33699896931648254, 0.33410850167274475, 0.3288925290107727]}}
{"id": "WebQTest-1639", "question": "where did frank baum die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people_deceased_here"], ["people.place_of_death.people", "people.place_of_death.location"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did frank baum die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>", "<PATH> people.place_of_death.people <SEP> people.place_of_death.location </PATH>"], "scores": [-0.0003420518187340349, -0.10130805522203445, -0.1075214222073555], "norm_scores": [0.3568461239337921, 0.32257598638534546, 0.3205778896808624]}}
{"id": "WebQTest-1640", "question": "where is tunis on a map", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"]], "ground_paths": [["location.administrative_division.country"], ["base.locations.countries.continent"], ["location.location.containedby"], [], ["base.aareas.schema.administrative_area.administrative_parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is tunis on a map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.01386149600148201, -0.03554224222898483, -0.03872564062476158], "norm_scores": [0.33852460980415344, 0.3312641382217407, 0.33021125197410583]}}
{"id": "WebQTest-1641", "question": "who is nicolas cage married too", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.parents"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is nicolas cage married too [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.017415883019566536, -0.03153981268405914, -0.04298040270805359], "norm_scores": [0.3377539813518524, 0.33301711082458496, 0.3292289078235626]}}
{"id": "WebQTest-1643", "question": "who does the voice for chucky in child 's play", "prediction": [["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.film.country", "people.person.nationality"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["film.film.starring", "film.actor.film"], ["film.performance.film", "film.performance.actor"], ["film.performance.film", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does the voice for chucky in child 's play [/INST]", "raw_output": {"paths": ["<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>"], "scores": [-0.029554996639490128, -0.029977893456816673, -0.030826831236481667], "norm_scores": [0.33352163434028625, 0.33338063955307007, 0.3330977261066437]}}
{"id": "WebQTest-1644", "question": "what did mary wollstonecraft fight for", "prediction": [["people.person.profession"], ["base.activism.activist.area_of_activism"], ["base.activism.activism_issue.activists"]], "ground_paths": [["base.activism.activist.area_of_activism"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did mary wollstonecraft fight for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> base.activism.activism_issue.activists </PATH>"], "scores": [-0.018411140888929367, -0.022043894976377487, -0.04269528388977051], "norm_scores": [0.33643049001693726, 0.33521053194999695, 0.3283589482307434]}}
{"id": "WebQTest-1646", "question": "what year did the phillies begin", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_team.previously_known"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the phillies begin [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.previously_known </PATH>"], "scores": [-0.003958207555115223, -0.035507649183273315, -0.05901109799742699], "norm_scores": [0.34300869703292847, 0.33235588669776917, 0.32463544607162476]}}
{"id": "WebQTest-1648", "question": "what language does iceland speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does iceland speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.014559859409928322, -0.018270179629325867, -0.04670841246843338], "norm_scores": [0.3373068869113922, 0.33605772256851196, 0.3266354203224182]}}
{"id": "WebQTest-1649", "question": "what was dr seuss real name", "prediction": [["people.person.profession"], ["book.book.interior_illustrations_by", "people.person.nationality"], ["book.book_edition.author_editor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was dr seuss real name [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> book.book.interior_illustrations_by <SEP> people.person.nationality </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.02535882033407688, -0.027705388143658638, -0.03870456665754318], "norm_scores": [0.33507582545280457, 0.334290474653244, 0.3306336998939514]}}
{"id": "WebQTest-1650", "question": "what did gloria steinem founded", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["organization.founder.organizations_founded"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did gloria steinem founded [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.founder.organizations_founded </PATH>"], "scores": [-0.005602165125310421, -0.02409203164279461, -0.10645114630460739], "norm_scores": [0.34653037786483765, 0.34018197655677795, 0.3132875859737396]}}
{"id": "WebQTest-1652", "question": "what language they speak in scotland", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language they speak in scotland [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.016375083476305008, -0.016844699159264565, -0.031019795686006546], "norm_scores": [0.33500921726226807, 0.3348519206047058, 0.33013883233070374]}}
{"id": "WebQTest-1653", "question": "what boarding school did mark zuckerberg go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what boarding school did mark zuckerberg go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.01435212790966034, -0.019858775660395622, -0.025521619245409966], "norm_scores": [0.3351879119873047, 0.33334723114967346, 0.33146485686302185]}}
{"id": "WebQTest-1654", "question": "what are bigos", "prediction": [["food.dish.cuisine"], ["food.diet_based_dish.diet"], ["food.diet_and_soup.diet"]], "ground_paths": [["food.type_of_dish.dishes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are bigos [/INST]", "raw_output": {"paths": ["<PATH> food.dish.cuisine </PATH>", "<PATH> food.diet_based_dish.diet </PATH>", "<PATH> food.diet_and_soup.diet </PATH>"], "scores": [-0.033095914870500565, -0.08138469606637955, -0.08810525387525558], "norm_scores": [0.34490668773651123, 0.32864728569984436, 0.3264460265636444]}}
{"id": "WebQTest-1655", "question": "where are ike and tina turner 's children", "prediction": [["people.person.children"], ["people.person.parents"], ["music.artist.genre", "music.artist.genre"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are ike and tina turner 's children [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>"], "scores": [-0.00824752077460289, -0.020032966509461403, -0.08153806626796722], "norm_scores": [0.34274575114250183, 0.3387300670146942, 0.31852418184280396]}}
{"id": "WebQTest-1656", "question": "what are some of leonardo da vinci most famous inventions", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], ["law.invention.inventor"], ["law.inventor.inventions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some of leonardo da vinci most famous inventions [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>"], "scores": [-0.006886700168251991, -0.01943700946867466, -0.0757799744606018], "norm_scores": [0.342353880405426, 0.33808407187461853, 0.3195621073246002]}}
{"id": "WebQTest-1657", "question": "when did shaq come into the nba", "prediction": [["sports.sports_award_winner.awards", "sports.sports_award.season"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["sports.sports_award_winner.awards", "sports.sports_league_season.awards"]], "ground_paths": [["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.draft"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did shaq come into the nba [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_league_season.awards </PATH>"], "scores": [-0.015720410272479057, -0.021321861073374748, -0.021529845893383026], "norm_scores": [0.334602415561676, 0.3327333927154541, 0.3326641917228699]}}
{"id": "WebQTest-1658", "question": "where do they speak turkish", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do they speak turkish [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.013907610438764095, -0.022168660536408424, -0.02800513431429863], "norm_scores": [0.335821270942688, 0.3330584466457367, 0.33112025260925293]}}
{"id": "WebQTest-1659", "question": "what is the zip code for walnut creek", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for walnut creek [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-7.517697667935863e-05, -0.10707005113363266, -0.16075335443019867], "norm_scores": [0.36362361907958984, 0.32672685384750366, 0.3096495568752289]}}
{"id": "WebQTest-1661", "question": "who was the voice of darth vader in the original star wars movie", "prediction": [["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the voice of darth vader in the original star wars movie [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.017465488985180855, -0.02652156725525856, -0.03096047043800354], "norm_scores": [0.33584314584732056, 0.3328154385089874, 0.33134138584136963]}}
{"id": "WebQTest-1662", "question": "where is english spoken around the world", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is english spoken around the world [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.012599749490618706, -0.025364099070429802, -0.02849598601460457], "norm_scores": [0.3365251421928406, 0.33225691318511963, 0.3312179446220398]}}
{"id": "WebQTest-1663", "question": "what music did louis armstrong play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what music did louis armstrong play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.0017305725486949086, -0.04934539273381233, -0.0925421193242073], "norm_scores": [0.3488342761993408, 0.33261382579803467, 0.3185518980026245]}}
{"id": "WebQTest-1664", "question": "what form of currency does germany use", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of currency does germany use [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.005766838788986206, -0.025508027523756027, -0.11977343261241913], "norm_scores": [0.3481040298938751, 0.341299444437027, 0.3105965554714203]}}
{"id": "WebQTest-1665", "question": "where is the latin language from", "prediction": [["language.human_language.language_family"], ["base.rosetta.languoid.parent"], ["base.rosetta.languoid.child"]], "ground_paths": [["language.human_language.language_family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the latin language from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>", "<PATH> base.rosetta.languoid.child </PATH>"], "scores": [-0.001959107583388686, -0.05240018665790558, -0.06861203163862228], "norm_scores": [0.3464607298374176, 0.3294183313846588, 0.3241209089756012]}}
{"id": "WebQTest-1666", "question": "what are the top tourist attractions in rome", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["location.location.containedby"], ["base.arthist.int_zm_nyek.helynev"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the top tourist attractions in rome [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.017415283247828484, -0.01864933967590332, -0.01909090019762516], "norm_scores": [0.3336566984653473, 0.3332452178001404, 0.3330981135368347]}}
{"id": "WebQTest-1667", "question": "who were the children of king solomon", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.parent"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were the children of king solomon [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.013553033582866192, -0.01413705199956894, -0.1351437270641327], "norm_scores": [0.34662923216819763, 0.3464268743991852, 0.3069438934326172]}}
{"id": "WebQTest-1668", "question": "what did andy irons die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did andy irons die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0020000627264380455, -0.03982004150748253, -0.12464841455221176], "norm_scores": [0.35119006037712097, 0.3381561040878296, 0.31065383553504944]}}
{"id": "WebQTest-1669", "question": "where is auburn university located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is auburn university located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.002344653010368347, -0.04549425095319748, -0.12746337056159973], "norm_scores": [0.35209280252456665, 0.33722326159477234, 0.310683935880661]}}
{"id": "WebQTest-1670", "question": "who is tatyana ali parents", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.parent"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is tatyana ali parents [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.012090349569916725, -0.016222527250647545, -0.15564383566379547], "norm_scores": [0.34938761591911316, 0.3479468524456024, 0.3026655614376068]}}
{"id": "WebQTest-1671", "question": "where did leif ericson grow up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did leif ericson grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0016877863090485334, -0.04881489649415016, -0.08574419468641281], "norm_scores": [0.3480263650417328, 0.3320053517818451, 0.31996825337409973]}}
{"id": "WebQTest-1672", "question": "what language do the maasai speak", "prediction": [["people.ethnicity.languages_spoken"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do the maasai speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.0028828629292547703, -0.049217913299798965, -0.06403521448373795], "norm_scores": [0.3453751802444458, 0.3297373056411743, 0.3248875141143799]}}
{"id": "WebQTest-1673", "question": "what political party was hitler in", "prediction": [["base.activism.activist.area_of_activism", "government.political_party.ideology"], ["organization.organization_founder.organizations_founded"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["base.activism.activist.area_of_activism", "government.political_party.ideology"], ["government.political_party_tenure.politician", "government.political_party.politicians_in_this_party"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party was hitler in [/INST]", "raw_output": {"paths": ["<PATH> base.activism.activist.area_of_activism <SEP> government.political_party.ideology </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>"], "scores": [-0.01792968437075615, -0.020501473918557167, -0.030528254806995392], "norm_scores": [0.33501824736595154, 0.3341577649116516, 0.33082398772239685]}}
{"id": "WebQTest-1674", "question": "who is the leader of north korea today", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the leader of north korea today [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.004337935708463192, -0.029346179217100143, -0.06751219928264618], "norm_scores": [0.34316128492355347, 0.3346858620643616, 0.32215291261672974]}}
{"id": "WebQTest-1675", "question": "where do the appalachian mountains run through", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.containedby"]], "ground_paths": [["location.location.partially_contains"], ["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the appalachian mountains run through [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.007588124368339777, -0.020363494753837585, -0.09771309792995453], "norm_scores": [0.3446941375732422, 0.34031856060028076, 0.3149873614311218]}}
{"id": "WebQTest-1676", "question": "what money does spain use", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money does spain use [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.006132989190518856, -0.026066219434142113, -0.13830552995204926], "norm_scores": [0.35008442401885986, 0.3431752026081085, 0.306740403175354]}}
{"id": "WebQTest-1677", "question": "where was terry waite born", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was terry waite born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.009699216112494469, -0.017129648476839066, -0.1418737918138504], "norm_scores": [0.34857961535453796, 0.3459991216659546, 0.30542129278182983]}}
{"id": "WebQTest-1678", "question": "what country did the battle of san juan hill take place", "prediction": [["location.location.events"], ["base.culturalevent.event.entity_involved"], ["military.military_conflict.casualties", "military.casualties.combatant"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did the battle of san juan hill take place [/INST]", "raw_output": {"paths": ["<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.casualties.combatant </PATH>"], "scores": [-0.006399061996489763, -0.027097979560494423, -0.05513620376586914], "norm_scores": [0.3410704731941223, 0.3340832591056824, 0.3248462378978729]}}
{"id": "WebQTest-1679", "question": "what kind of government does the united states have currently", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.planet"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does the united states have currently [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.planet </PATH>"], "scores": [-0.007578795775771141, -0.01670513115823269, -0.1571100801229477], "norm_scores": [0.3506278395652771, 0.3474424481391907, 0.30192968249320984]}}
{"id": "WebQTest-1680", "question": "when was the last time the orioles won the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the orioles won the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.00917611364275217, -0.013419625349342823, -0.098701611161232], "norm_scores": [0.3436272144317627, 0.34217211604118347, 0.31420066952705383]}}
{"id": "WebQTest-1682", "question": "who did ben stiller date", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"]], "ground_paths": [["people.person.parents", "people.person.children"], ["base.popstra.friendship.participant", "base.popstra.celebrity.friendship"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["people.person.parents", "people.person.parents"], ["people.marriage.spouse", "people.marriage.spouse"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["people.person.languages", "people.person.languages"], ["people.person.nationality", "people.person.nationality"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["people.person.ethnicity", "people.person.ethnicity"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did ben stiller date [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>"], "scores": [-0.015393559820950031, -0.029998531565070152, -0.048156555742025375], "norm_scores": [0.3386078178882599, 0.3336983919143677, 0.32769376039505005]}}
{"id": "WebQTest-1683", "question": "what is the most common language in norway", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the most common language in norway [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.014992250129580498, -0.022547198459506035, -0.03233262524008751], "norm_scores": [0.33610251545906067, 0.3335728347301483, 0.33032459020614624]}}
{"id": "WebQTest-1684", "question": "what influenced william shakespeare to start writing", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["people.person.profession"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what influenced william shakespeare to start writing [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.009739790111780167, -0.013443578965961933, -0.10580465942621231], "norm_scores": [0.34426867961883545, 0.34299594163894653, 0.3127354085445404]}}
{"id": "WebQTest-1685", "question": "where did benjamin franklin went to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did benjamin franklin went to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.013179505243897438, -0.02313724346458912, -0.024069294333457947], "norm_scores": [0.33565372228622437, 0.3323279619216919, 0.33201834559440613]}}
{"id": "WebQTest-1686", "question": "who is the governor of arizona 2009", "prediction": [["location.location.containedby", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["location.country.first_level_divisions", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the governor of arizona 2009 [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.020377255976200104, -0.021174335852265358, -0.02609100565314293], "norm_scores": [0.3340564966201782, 0.33379030227661133, 0.33215323090553284]}}
{"id": "WebQTest-1687", "question": "what movies has spike lee made", "prediction": [["film.film.directed_by"], ["film.film.produced_by"], ["film.producer.film"]], "ground_paths": [["film.film.executive_produced_by"], ["film.film.produced_by"], ["film.director.film"], ["film.producer.film"], ["film.writer.film"], ["film.film.directed_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies has spike lee made [/INST]", "raw_output": {"paths": ["<PATH> film.film.directed_by </PATH>", "<PATH> film.film.produced_by </PATH>", "<PATH> film.producer.film </PATH>"], "scores": [-0.027303067967295647, -0.03224150836467743, -0.0379716195166111], "norm_scores": [0.3350687623023987, 0.3334181308746338, 0.33151307702064514]}}
{"id": "WebQTest-1688", "question": "what did bernie mac died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did bernie mac died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>"], "scores": [-0.001536648371256888, -0.04434708505868912, -0.12665586173534393], "norm_scores": [0.35205259919166565, 0.3372991383075714, 0.3106482923030853]}}
{"id": "WebQTest-1690", "question": "where is electrolux based", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"]], "ground_paths": [["organization.organization.founders", "people.deceased_person.place_of_death"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is electrolux based [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>"], "scores": [-0.004700147546827793, -0.03471507504582405, -0.05503314733505249], "norm_scores": [0.3423082232475281, 0.3321865200996399, 0.32550522685050964]}}
{"id": "WebQTest-1691", "question": "what is houston timezone", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is houston timezone [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-9.538098311168142e-06, -0.19501566886901855, -0.19708415865898132], "norm_scores": [0.37822067737579346, 0.31121116876602173, 0.3105681240558624]}}
{"id": "WebQTest-1694", "question": "what was shakespeare 's wife called", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.parents", "people.person.children"], ["freebase.valuenotation.has_value", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.parents"], ["people.place_of_interment.interred_here", "people.deceased_person.place_of_burial"], ["people.person.place_of_birth", "people.deceased_person.place_of_death"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["book.book_subject.works", "book.book_subject.works"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was shakespeare 's wife called [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.025605181232094765, -0.036837972700595856, -0.04059576988220215], "norm_scores": [0.33625298738479614, 0.3324970304965973, 0.3312499523162842]}}
{"id": "WebQTest-1695", "question": "when did truman become president", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["government.government_position_held.office_holder", "government.government_position_held.office_holder"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did truman become president [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.007202774286270142, -0.027998434379696846, -0.06155269965529442], "norm_scores": [0.34170281887054443, 0.3346702456474304, 0.32362696528434753]}}
{"id": "WebQTest-1696", "question": "what did ferdinand magellan do", "prediction": [["people.person.profession"], ["people.profession.people_with_this_profession"], ["common.topic.notable_types"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did ferdinand magellan do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0016625917050987482, -0.061025891453027725, -0.06570304185152054], "norm_scores": [0.3471822440624237, 0.327172189950943, 0.3256455361843109]}}
{"id": "WebQTest-1698", "question": "where do burmese cats originate from", "prediction": [["biology.animal_breed.place_of_origin"], ["biology.breed_origin.breeds_originating_here"], ["biology.animal_breed.breed_of"]], "ground_paths": [["biology.animal_breed.place_of_origin"], ["biology.breed_origin.breeds_originating_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do burmese cats originate from [/INST]", "raw_output": {"paths": ["<PATH> biology.animal_breed.place_of_origin </PATH>", "<PATH> biology.breed_origin.breeds_originating_here </PATH>", "<PATH> biology.animal_breed.breed_of </PATH>"], "scores": [-0.00395706482231617, -0.026858031749725342, -0.06262261420488358], "norm_scores": [0.3424210250377655, 0.33466836810112, 0.3229105770587921]}}
{"id": "WebQTest-1699", "question": "who played kirk", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.quotations", "media_common.quotation.author"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played kirk [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.023587631061673164, -0.031651560217142105, -0.034551139920949936], "norm_scores": [0.3354506194591522, 0.3327564299106598, 0.3317929804325104]}}
{"id": "WebQTest-1700", "question": "where is mt st helens located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.mountain.mountain_type"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mt st helens located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.mountain.mountain_type </PATH>"], "scores": [-0.008047067560255527, -0.022117994725704193, -0.09155698865652084], "norm_scores": [0.34412631392478943, 0.33931806683540344, 0.31655558943748474]}}
{"id": "WebQTest-1701", "question": "what county is st paul minnesota in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"]], "ground_paths": [["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st paul minnesota in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.014707825146615505, -0.018266385421156883, -0.036122843623161316], "norm_scores": [0.33610501885414124, 0.3349110782146454, 0.328983873128891]}}
{"id": "WebQTest-1702", "question": "who was king henry viii son", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.gender"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was king henry viii son [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.013699401170015335, -0.01588326133787632, -0.06370668113231659], "norm_scores": [0.33909329771995544, 0.33835357427597046, 0.3225531578063965]}}
{"id": "WebQTest-1705", "question": "where was franz ferdinand from", "prediction": [["people.person.place_of_birth"], ["people.deceased_person.place_of_death"], ["people.person.nationality"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was franz ferdinand from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0009507820941507816, -0.07217766344547272, -0.07735641300678253], "norm_scores": [0.34993287920951843, 0.3258752226829529, 0.3241919279098511]}}
{"id": "WebQTest-1706", "question": "what was benedict arnold most famous for", "prediction": [["base.honouriam.dishonoured_person.dishonor_bestowed"], ["people.person.profession"], ["base.honouriam.dishonoured_person.dishonor_bestowed", "base.honouri.dishonor.dishonor_bestowed"]], "ground_paths": [["base.honouriam.dishonoured_person.dishonor_bestowed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was benedict arnold most famous for [/INST]", "raw_output": {"paths": ["<PATH> base.honouriam.dishonoured_person.dishonor_bestowed </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> base.honouriam.dishonoured_person.dishonor_bestowed <SEP> base.honouri.dishonor.dishonor_bestowed </PATH>"], "scores": [-0.02144317701458931, -0.025341616943478584, -0.06012363359332085], "norm_scores": [0.33804717659950256, 0.336731880903244, 0.3252210021018982]}}
{"id": "WebQTest-1707", "question": "who plays lex luthor on smallville", "prediction": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "ground_paths": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["tv.tv_program.languages", "people.person.languages"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays lex luthor on smallville [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.022169308736920357, -0.02383437380194664, -0.02747940644621849], "norm_scores": [0.3341084420681, 0.33355259895324707, 0.33233898878097534]}}
{"id": "WebQTest-1708", "question": "what are dallas cowboys colors", "prediction": [["sports.sports_team.colors"], ["sports.sports_team.colors", "sports.sports_team.colors"], ["sports.sports_team.color"]], "ground_paths": [["sports.sports_team.colors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are dallas cowboys colors [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.colors </PATH>", "<PATH> sports.sports_team.colors <SEP> sports.sports_team.colors </PATH>", "<PATH> sports.sports_team.color </PATH>"], "scores": [-8.35105383885093e-05, -0.11519332230091095, -0.12201526761054993], "norm_scores": [0.3601686358451843, 0.3210068941116333, 0.31882444024086]}}
{"id": "WebQTest-1710", "question": "what is the currency of the czech republic", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency of the czech republic [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.0022749777417629957, -0.039955880492925644, -0.12045817822217941], "norm_scores": [0.35068604350090027, 0.337717741727829, 0.31159621477127075]}}
{"id": "WebQTest-1711", "question": "when was the last time the la kings won a playoff series", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the la kings won a playoff series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.006765332072973251, -0.017413612455129623, -0.06317222863435745], "norm_scores": [0.3407662808895111, 0.3371569514274597, 0.3220767676830292]}}
{"id": "WebQTest-1712", "question": "where did spencer pratt go to school", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did spencer pratt go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.016093209385871887, -0.02573937550187111, -0.030476197600364685], "norm_scores": [0.3360079526901245, 0.3327823579311371, 0.3312097489833832]}}
{"id": "WebQTest-1713", "question": "what disease did abraham lincoln had", "prediction": [["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"]], "ground_paths": [["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.notable_people_with_this_condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what disease did abraham lincoln had [/INST]", "raw_output": {"paths": ["<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.0030808818992227316, -0.036281768232584, -0.047299500554800034], "norm_scores": [0.34198686480522156, 0.3308190107345581, 0.3271941542625427]}}
{"id": "WebQTest-1714", "question": "who is vancouver canucks coach", "prediction": [["sports.sports_team.coaches", "sports.sports_team_coach_tenure.coach"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"], ["sports.sports_team.coaches", "sports.sports_team_coach.teams_coached"]], "ground_paths": [["ice_hockey.hockey_coach.current_team"], ["sports.sports_team.coaches", "sports.sports_team_coach.teams_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is vancouver canucks coach [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach.teams_coached </PATH>"], "scores": [-0.01215018518269062, -0.02121506631374359, -0.030380018055438995], "norm_scores": [0.3363706171512604, 0.3333352208137512, 0.3302941918373108]}}
{"id": "WebQTest-1715", "question": "where did cs lewis wrote", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.book_edition.author_editor"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did cs lewis wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.021089304238557816, -0.028548985719680786, -0.036394014954566956], "norm_scores": [0.3358657956123352, 0.333369642496109, 0.3307645916938782]}}
{"id": "WebQTest-1716", "question": "what is carlos pena jr nationality", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["people.person.ethnicity"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is carlos pena jr nationality [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.ethnicity </PATH>"], "scores": [-0.0011304470244795084, -0.06683483719825745, -0.0833352729678154], "norm_scores": [0.3499573767185211, 0.32770276069641113, 0.32233989238739014]}}
{"id": "WebQTest-1719", "question": "who was married to jane wyman", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.children"]], "ground_paths": [["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.spouse_s", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was married to jane wyman [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.020329399034380913, -0.039207953959703445, -0.04366885870695114], "norm_scores": [0.3380400240421295, 0.33171820640563965, 0.33024170994758606]}}
{"id": "WebQTest-1721", "question": "what language does peru speak", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does peru speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.01067462656646967, -0.02987942285835743, -0.039711855351924896], "norm_scores": [0.338712215423584, 0.3322693705558777, 0.32901838421821594]}}
{"id": "WebQTest-1723", "question": "where is english spoken in the world map", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is english spoken in the world map [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.012907412834465504, -0.023959578946232796, -0.028454329818487167], "norm_scores": [0.3362947404384613, 0.33259841799736023, 0.3311068117618561]}}
{"id": "WebQTest-1724", "question": "when did arsenal won the league", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["soccer.football_team.matches_played", "soccer.football_player_match_participation.season"]], "ground_paths": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did arsenal won the league [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> soccer.football_team.matches_played <SEP> soccer.football_player_match_participation.season </PATH>"], "scores": [-0.011064698919653893, -0.013764588162302971, -0.08782830089330673], "norm_scores": [0.3420659899711609, 0.3411436676979065, 0.31679031252861023]}}
{"id": "WebQTest-1725", "question": "what timezone is phoenix az in right now", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is phoenix az in right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-9.32974126044428e-06, -0.17728205025196075, -0.18545015156269073], "norm_scores": [0.37477195262908936, 0.3138907551765442, 0.31133729219436646]}}
{"id": "WebQTest-1726", "question": "where is south portland", "prediction": [["location.location.containedby"], ["location.us_county.hud_county_place"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"], ["location.hud_county_place.county"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is south portland [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0074865990318357944, -0.03988543525338173, -0.05032079666852951], "norm_scores": [0.3417412340641022, 0.3308466672897339, 0.32741212844848633]}}
{"id": "WebQTest-1727", "question": "what countries speak french as a first language", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["base.rosetta.local_name.locale"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries speak french as a first language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.011804086156189442, -0.024373142048716545, -0.027091456577181816], "norm_scores": [0.3364354372024536, 0.3322332203388214, 0.331331342458725]}}
{"id": "WebQTest-1728", "question": "who was ptolemy and what did he do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was ptolemy and what did he do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0017570682102814317, -0.05114450678229332, -0.07125384360551834], "norm_scores": [0.34665945172309875, 0.32995471358299255, 0.3233858346939087]}}
{"id": "WebQTest-1729", "question": "what was the name of henry viii first wife", "prediction": [["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.parents"]], "ground_paths": [["people.person.children", "people.person.parents"], ["book.written_work.subjects", "theater.play.characters"], ["people.person.parents", "people.person.parents"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.family_member.family", "people.family.members"], ["people.person.religion", "people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the name of henry viii first wife [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>"], "scores": [-0.03434760123491287, -0.03507646545767784, -0.038669999688863754], "norm_scores": [0.3338944613933563, 0.33365118503570557, 0.3324543237686157]}}
{"id": "WebQTest-1730", "question": "where does the parana river flow", "prediction": [["location.location.containedby"], ["geography.river.basin_countries"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the parana river flow [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.019754063338041306, -0.022760383784770966, -0.026995811611413956], "norm_scores": [0.3344724476337433, 0.3334684669971466, 0.3320590555667877]}}
{"id": "WebQTest-1731", "question": "what is president nixon 's first name", "prediction": [["people.person.nationality", "people.person.nationality"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is president nixon 's first name [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.023453988134860992, -0.035043831914663315, -0.05327405035495758], "norm_scores": [0.33794090151786804, 0.3340468406677246, 0.32801225781440735]}}
{"id": "WebQTest-1732", "question": "where did theodor schwann get his education", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did theodor schwann get his education [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.016416260972619057, -0.020215075463056564, -0.021187976002693176], "norm_scores": [0.33428627252578735, 0.33301880955696106, 0.332694947719574]}}
{"id": "WebQTest-1733", "question": "who or what influenced frank lloyd wright", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["people.person.profession"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who or what influenced frank lloyd wright [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.009947185404598713, -0.013515230268239975, -0.09392368793487549], "norm_scores": [0.342948317527771, 0.3417268693447113, 0.3153248131275177]}}
{"id": "WebQTest-1734", "question": "what year did houston rockets win their first championship", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did houston rockets win their first championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.006984202191233635, -0.017101909965276718, -0.12375801056623459], "norm_scores": [0.34725600481033325, 0.34376031160354614, 0.3089836835861206]}}
{"id": "WebQTest-1735", "question": "where do most of the people live in russia", "prediction": [["location.location.containedby"], ["location.administrative_division.country"], ["location.location.partially_containedby"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do most of the people live in russia [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.02200743928551674, -0.02632804773747921, -0.04160390421748161], "norm_scores": [0.3359895348548889, 0.334540992975235, 0.3294694125652313]}}
{"id": "WebQTest-1736", "question": "where is holy roman empire located", "prediction": [["location.location.contains"], ["location.location.containedby"], ["fictional_universe.fictional_setting.languages", "language.human_language.main_country"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is holy roman empire located [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> fictional_universe.fictional_setting.languages <SEP> language.human_language.main_country </PATH>"], "scores": [-0.018332812935113907, -0.02079981006681919, -0.04294917359948158], "norm_scores": [0.3363356590270996, 0.33550694584846497, 0.32815736532211304]}}
{"id": "WebQTest-1737", "question": "what movies has liam hemsworth played in", "prediction": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["film.actor.film", "film.film.starring"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["people.person.place_of_birth", "film.film.featured_film_locations"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies has liam hemsworth played in [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.02663039043545723, -0.030174387618899345, -0.03675316274166107], "norm_scores": [0.3348523676395416, 0.3336677551269531, 0.33147984743118286]}}
{"id": "WebQTest-1738", "question": "who plays nicholas newman on the young and the restless", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays nicholas newman on the young and the restless [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.021587081253528595, -0.026727383956313133, -0.02724762260913849], "norm_scores": [0.33453449606895447, 0.3328193128108978, 0.3326462209224701]}}
{"id": "WebQTest-1739", "question": "who is the voice of eric cartman on south park", "prediction": [["film.performance.special_performance_type", "film.performance.actor"], ["film.performance.special_performance_type", "film.actor.film"], ["tv.regular_tv_appearance.special_performance_type", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the voice of eric cartman on south park [/INST]", "raw_output": {"paths": ["<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.special_performance_type <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.01804277114570141, -0.02802862599492073, -0.0305258110165596], "norm_scores": [0.3358343839645386, 0.3324974775314331, 0.3316681683063507]}}
{"id": "WebQTest-1740", "question": "who is angelina jolies brother", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.nationality", "people.person.place_of_birth"], ["people.person.children", "people.person.children"]], "ground_paths": [["film.producer.films_executive_produced", "film.producer.films_executive_produced"], ["people.person.parents", "people.person.parents"], ["people.person.sibling_s", "people.sibling_relationship.sibling"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"], ["people.person.place_of_birth", "people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is angelina jolies brother [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.nationality <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.021388940513134003, -0.03384409844875336, -0.039888083934783936], "norm_scores": [0.33678051829338074, 0.33261188864707947, 0.3306076228618622]}}
{"id": "WebQTest-1741", "question": "where is the headquarters of bank of america", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"]], "ground_paths": [["common.image.appears_in_topic_gallery", "location.location.containedby"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the headquarters of bank of america [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>"], "scores": [-0.0050928364507853985, -0.03651008754968643, -0.051085855811834335], "norm_scores": [0.34198325872421265, 0.3314061164855957, 0.32661065459251404]}}
{"id": "WebQTest-1742", "question": "what type of government japan have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government japan have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.010510987602174282, -0.014276945032179356, -0.12933465838432312], "norm_scores": [0.34671592712402344, 0.34541264176368713, 0.30787140130996704]}}
{"id": "WebQTest-1744", "question": "what team does josh hamilton play for", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does josh hamilton play for [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.013948577456176281, -0.01676597073674202, -0.029941236600279808], "norm_scores": [0.33542177081108093, 0.33447808027267456, 0.3301001489162445]}}
{"id": "WebQTest-1746", "question": "where is burundi country", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.river.basin_countries"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is burundi country [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.river.basin_countries </PATH>"], "scores": [-0.0032486130949109793, -0.05188898369669914, -0.06726419925689697], "norm_scores": [0.34595924615859985, 0.3295343518257141, 0.3245064318180084]}}
{"id": "WebQTest-1748", "question": "through which countries of the sahel does the niger river flow", "prediction": [["location.location.partially_containedby"], ["geography.river.basin_countries"], ["location.location.partially_contains"]], "ground_paths": [["geography.river.basin_countries"], ["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: through which countries of the sahel does the niger river flow [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.017113808542490005, -0.01843174919486046, -0.031443122774362564], "norm_scores": [0.3350694477558136, 0.334628164768219, 0.3303023874759674]}}
{"id": "WebQTest-1750", "question": "what city does paul ryan live in", "prediction": [["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city does paul ryan live in [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.020374925807118416, -0.02155916392803192, -0.038077205419540405], "norm_scores": [0.33542752265930176, 0.33503055572509766, 0.329541951417923]}}
{"id": "WebQTest-1751", "question": "what city is acadia university in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.parent_institution", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city is acadia university in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>"], "scores": [-0.00682388199493289, -0.04379173368215561, -0.05031010881066322], "norm_scores": [0.34233057498931885, 0.3299064040184021, 0.3277629613876343]}}
{"id": "WebQTest-1752", "question": "what did f. scott fitzgerald do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did f. scott fitzgerald do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0005872846813872457, -0.07957250624895096, -0.08045151084661484], "norm_scores": [0.35121050477027893, 0.32453733682632446, 0.324252188205719]}}
{"id": "WebQTest-1753", "question": "what countries does tomtom western europe include", "prediction": [["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"], ["common.topic.notable_types"], ["location.location.containedby"]], "ground_paths": [["organization.organization.phone_number", "common.phone_number.service_location"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does tomtom western europe include [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.organization_extra.phone_number <SEP> base.schemastaging.phone_sandbox.service_location </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.009140455164015293, -0.021642377600073814, -0.04466964676976204], "norm_scores": [0.33867645263671875, 0.33446869254112244, 0.32685479521751404]}}
{"id": "WebQTest-1754", "question": "what is the national language of iran", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the national language of iran [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.01764455810189247, -0.02192843146622181, -0.027887213975191116], "norm_scores": [0.3349483609199524, 0.33351653814315796, 0.33153510093688965]}}
{"id": "WebQTest-1755", "question": "where did the second battle of ypres occur", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the second battle of ypres occur [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.009445015341043472, -0.020033663138747215, -0.06787073612213135], "norm_scores": [0.3409809172153473, 0.3373894691467285, 0.32162967324256897]}}
{"id": "WebQTest-1756", "question": "what languages do people speak in spain", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what languages do people speak in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.013987304642796516, -0.02377926930785179, -0.026633109897375107], "norm_scores": [0.33583083748817444, 0.33255842328071594, 0.33161070942878723]}}
{"id": "WebQTest-1757", "question": "what did michael crabtree do", "prediction": [["american_football.football_player.games", "american_football.football_player_game_statistics.position_s"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.position"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.role"]], "ground_paths": [["american_football.football_player.position_s"], ["people.person.profession"], ["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did michael crabtree do [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.games <SEP> american_football.football_player_game_statistics.position_s </PATH>", "<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.position </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.role </PATH>"], "scores": [-0.029744699597358704, -0.03227349370718002, -0.03404618054628372], "norm_scores": [0.3340925872325897, 0.3332488238811493, 0.332658588886261]}}
{"id": "WebQTest-1758", "question": "where do they speak afrikaans", "prediction": [["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do they speak afrikaans [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.004619448445737362, -0.044314194470644, -0.04892902821302414], "norm_scores": [0.34273093938827515, 0.32939282059669495, 0.3278762102127075]}}
{"id": "WebQTest-1759", "question": "what 3 states border florida", "prediction": [["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.partially_contains", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"]], "ground_paths": [["location.administrative_division.first_level_division_of", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.contains", "location.location.contains"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_contains", "location.location.partially_containedby"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.administrative_division.first_level_division_of", "location.administrative_division.first_level_division_of"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["location.location.time_zones", "location.location.time_zones"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.tropical_cyclone.affected_areas"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["common.topic.notable_types", "common.topic.notable_types"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["location.location.events", "time.event.locations"], ["location.location.containedby", "location.location.contains"], ["location.location.partially_containedby", "location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what 3 states border florida [/INST]", "raw_output": {"paths": ["<PATH> meteorology.tropical_cyclone.affected_areas <SEP> meteorology.cyclone_affected_area.cyclones </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization.geographic_scope </PATH>"], "scores": [-0.026646055281162262, -0.03638584539294243, -0.03760427236557007], "norm_scores": [0.33563700318336487, 0.3323838412761688, 0.33197909593582153]}}
{"id": "WebQTest-1760", "question": "who plays ziva david on ncis", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_guest_role.character", "tv.tv_actor.guest_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["tv.tv_guest_role.character", "tv.tv_guest_role.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays ziva david on ncis [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.021158546209335327, -0.030794844031333923, -0.032645344734191895], "norm_scores": [0.3356843590736389, 0.33246514201164246, 0.33185049891471863]}}
{"id": "WebQTest-1761", "question": "who ruled after king henry viii died", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.religion", "people.person.religion"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who ruled after king henry viii died [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.religion <SEP> people.person.religion </PATH>"], "scores": [-0.02086833119392395, -0.03749445080757141, -0.045348554849624634], "norm_scores": [0.33791452646255493, 0.3323427736759186, 0.3297427296638489]}}
{"id": "WebQTest-1763", "question": "who are the st louis cardinals coaches", "prediction": [["baseball.baseball_team.current_coaches"], ["sports.sports_team.coaches", "sports.sports_team_coach_tenure.coach"], ["baseball.baseball_team.current_coaches", "baseball.current_coaching_tenure.baseball_coach"]], "ground_paths": [["baseball.current_coaching_tenure.baseball_team", "baseball.baseball_coach.current_team_coaching"], ["baseball.baseball_team.current_coaches", "baseball.current_coaching_tenure.baseball_coach"], ["baseball.baseball_team.current_coaches", "baseball.baseball_coach.current_team_coaching"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the st louis cardinals coaches [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_team.current_coaches </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> baseball.baseball_team.current_coaches <SEP> baseball.current_coaching_tenure.baseball_coach </PATH>"], "scores": [-0.016204994171857834, -0.033339086920022964, -0.04066737741231918], "norm_scores": [0.3379696011543274, 0.3322280943393707, 0.32980233430862427]}}
{"id": "WebQTest-1764", "question": "where is daud ibrahim", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is daud ibrahim [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.016163242980837822, -0.022146180272102356, -0.04232800379395485], "norm_scores": [0.33690345287323, 0.3348937928676605, 0.3282027840614319]}}
{"id": "WebQTest-1765", "question": "what region is turkey considered", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.countries.continent"]], "ground_paths": [["base.locations.countries.continent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what region is turkey considered [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.012539932504296303, -0.02775728888809681, -0.04776265099644661], "norm_scores": [0.33894991874694824, 0.3338310420513153, 0.32721900939941406]}}
{"id": "WebQTest-1766", "question": "what language do russians speak", "prediction": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["people.ethnicity.geographic_distribution", "location.country.languages_spoken"], ["people.ethnicity.geographic_distribution", "location.country.official_language"], ["people.ethnicity.geographic_distribution", "language.human_language.countries_spoken_in"], ["people.ethnicity.included_in_group", "people.ethnicity.languages_spoken"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["people.ethnicity.geographic_distribution", "language.human_language.main_country"], ["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do russians speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.014762625098228455, -0.02000386267900467, -0.028709176927804947], "norm_scores": [0.335466593503952, 0.33371293544769287, 0.3308204710483551]}}
{"id": "WebQTest-1767", "question": "where was martin luther king jr buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["people.person.place_of_interment"]], "ground_paths": [["people.deceased_person.place_of_burial"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was martin luther king jr buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_interment </PATH>"], "scores": [-0.003212141338735819, -0.03162815794348717, -0.08659255504608154], "norm_scores": [0.3457832634449005, 0.33609578013420105, 0.3181209862232208]}}
{"id": "WebQTest-1768", "question": "what are the official languages of argentina", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the official languages of argentina [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.008173197507858276, -0.025198757648468018, -0.04731779173016548], "norm_scores": [0.3395896852016449, 0.3338569104671478, 0.3265534043312073]}}
{"id": "WebQTest-1769", "question": "where is the city wales", "prediction": [["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"]], "ground_paths": [["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the city wales [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-0.018482711166143417, -0.021179797127842903, -0.036405522376298904], "norm_scores": [0.3356218934059143, 0.33471789956092834, 0.32966017723083496]}}
{"id": "WebQTest-1770", "question": "what country does rafael nadal play for", "prediction": [["people.person.nationality"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country does rafael nadal play for [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.017497189342975616, -0.02130349725484848, -0.029823902994394302], "norm_scores": [0.3351263105869293, 0.3338531255722046, 0.33102062344551086]}}
{"id": "WebQTest-1771", "question": "what is the islamic belief", "prediction": [["religion.belief.belief_of"], ["religion.religion.beliefs"], ["belief.belief.belief_of"]], "ground_paths": [["religion.religion.beliefs"], ["religion.belief.belief_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the islamic belief [/INST]", "raw_output": {"paths": ["<PATH> religion.belief.belief_of </PATH>", "<PATH> religion.religion.beliefs </PATH>", "<PATH> belief.belief.belief_of </PATH>"], "scores": [-0.008321105502545834, -0.018837932497262955, -0.11354576051235199], "norm_scores": [0.3460613787174225, 0.34244099259376526, 0.3114975690841675]}}
{"id": "WebQTest-1772", "question": "who is the prime minister of spain 2011", "prediction": [["people.person.nationality"], ["base.onephylogeny.type_of_thing.things_of_this_type", "people.person.profession"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the prime minister of spain 2011 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type <SEP> people.person.profession </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.008992883376777172, -0.01920868270099163, -0.02965947799384594], "norm_scores": [0.3367704451084137, 0.3333475589752197, 0.3298819661140442]}}
{"id": "WebQTest-1773", "question": "who was the son of king leonidas", "prediction": [["people.person.gender"], ["people.person.children"], ["people.person.parents"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the son of king leonidas [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.009060525335371494, -0.030167801305651665, -0.041958972811698914], "norm_scores": [0.33935683965682983, 0.33226901292800903, 0.3283741772174835]}}
{"id": "WebQTest-1774", "question": "who was judi dench married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.children"]], "ground_paths": [["people.person.parents", "people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was judi dench married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.021096469834446907, -0.02717299945652485, -0.0465526357293129], "norm_scores": [0.336835652589798, 0.3347950279712677, 0.32836928963661194]}}
{"id": "WebQTest-1775", "question": "how many languages do they speak in spain", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how many languages do they speak in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.016562890261411667, -0.01755882054567337, -0.03278364986181259], "norm_scores": [0.3352425694465637, 0.3349088728427887, 0.3298485577106476]}}
{"id": "WebQTest-1776", "question": "where are the sahara desert located", "prediction": [["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.contains"]], "ground_paths": [["book.written_work.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the sahara desert located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0024251467548310757, -0.05374165251851082, -0.07110431045293808], "norm_scores": [0.3467882573604584, 0.3294411897659302, 0.3237706124782562]}}
{"id": "WebQTest-1777", "question": "what is rihanna mum called", "prediction": [["people.person.parents"], ["people.person.children"], ["music.artist.origin"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is rihanna mum called [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.012646186165511608, -0.015087941661477089, -0.12654605507850647], "norm_scores": [0.34603172540664673, 0.34518781304359436, 0.3087804615497589]}}
{"id": "WebQTest-1779", "question": "when did president theodore roosevelt take office", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.office_holder"], ["government.us_president.vice_president"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did president theodore roosevelt take office [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.027823038399219513, -0.034433506429195404, -0.03538396209478378], "norm_scores": [0.3349097669124603, 0.332703173160553, 0.3323870897293091]}}
{"id": "WebQTest-1780", "question": "who does michael oher play for in 2010", "prediction": [["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does michael oher play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>"], "scores": [-0.016830459237098694, -0.01890176720917225, -0.018962174654006958], "norm_scores": [0.3338004946708679, 0.3331097960472107, 0.333089679479599]}}
{"id": "WebQTest-1781", "question": "who or what influenced albert einstein", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who or what influenced albert einstein [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>"], "scores": [-0.01127021573483944, -0.012472216039896011, -0.10266778618097305], "norm_scores": [0.3434710204601288, 0.34305843710899353, 0.3134704828262329]}}
{"id": "WebQTest-1782", "question": "where does egypt export to", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"]], "ground_paths": [["location.location.partially_contains", "location.location.partially_contains"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["location.location.contains", "location.location.contains"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.partially_contains", "location.location.partially_containedby"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.statistical_region.places_exported_to", "location.imports_and_exports.exported_to"], ["base.schemastaging.disputed_location.claimed_by", "base.schemastaging.disputed_location_claimant.disputed_territories"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["location.location.events", "time.event.locations"], ["language.human_language.countries_spoken_in", "location.country.official_language"], ["location.location.events", "location.location.events"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.partially_contains", "location.location.containedby"], ["location.location.contains_major_portion_of", "base.locations.countries.continent"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does egypt export to [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>"], "scores": [-0.026283901184797287, -0.029086962342262268, -0.03224773332476616], "norm_scores": [0.33430784940719604, 0.3333721160888672, 0.33232003450393677]}}
{"id": "WebQTest-1784", "question": "where is the seat of the african union", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["location.location.containedby"], ["organization.organization_founder.organizations_founded", "location.location.containedby"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the seat of the african union [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.location.containedby </PATH>"], "scores": [-0.009826593101024628, -0.03549887612462044, -0.03887353464961052], "norm_scores": [0.33944037556648254, 0.33083710074424744, 0.32972249388694763]}}
{"id": "WebQTest-1785", "question": "what was kim richards in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["film.actor.film", "film.film.starring"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.film.starring"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was kim richards in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.01764352060854435, -0.03685920313000679, -0.03774912655353546], "norm_scores": [0.33771657943725586, 0.33128905296325684, 0.3309943974018097]}}
{"id": "WebQTest-1788", "question": "what artistic style did henri rousseau work in", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what artistic style did henri rousseau work in [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.0020458356011658907, -0.039624568074941635, -0.051486387848854065], "norm_scores": [0.3430672585964203, 0.33041444420814514, 0.32651829719543457]}}
{"id": "WebQTest-1789", "question": "what is john edwards indicted for", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.election_campaign.candidate", "government.election_campaign.offense"]], "ground_paths": [["base.crime.acquitted_person.acquittal", "base.crime.acquittal.crime_type"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is john edwards indicted for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.offense </PATH>"], "scores": [-0.022407829761505127, -0.025743713602423668, -0.04306745156645775], "norm_scores": [0.33599644899368286, 0.33487746119499207, 0.3291260898113251]}}
{"id": "WebQTest-1790", "question": "where was huey newton killed", "prediction": [["people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["people.deceased_person.place_of_death", "base.biblioness.bibs_location.city"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was huey newton killed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> base.biblioness.bibs_location.city </PATH>"], "scores": [-0.00035413005389273167, -0.07682879269123077, -0.10524999350309372], "norm_scores": [0.3537575602531433, 0.32771265506744385, 0.31852981448173523]}}
{"id": "WebQTest-1791", "question": "when was the printing press invented by gutenberg", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["book.book_edition.author_editor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the printing press invented by gutenberg [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.014884157106280327, -0.02685389295220375, -0.030095748603343964], "norm_scores": [0.33635997772216797, 0.33235785365104675, 0.3312821388244629]}}
{"id": "WebQTest-1792", "question": "where is midlothian scotland", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["location.location.containedby"], ["location.administrative_division.second_level_division_of"], ["base.aareas.schema.administrative_area.administrative_parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is midlothian scotland [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.0027458397671580315, -0.04850424826145172, -0.06260978430509567], "norm_scores": [0.3451649844646454, 0.3297266960144043, 0.3251083791255951]}}
{"id": "WebQTest-1793", "question": "where does marta play soccer", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does marta play soccer [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.026486266404390335, -0.03338311240077019, -0.038498926907777786], "norm_scores": [0.33543694019317627, 0.33313146233558655, 0.3314315676689148]}}
{"id": "WebQTest-1794", "question": "where is mitt romney 's family from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["people.person.parents"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mitt romney 's family from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.013414472341537476, -0.023611120879650116, -0.044798530638217926], "norm_scores": [0.3379567265510559, 0.33452823758125305, 0.32751500606536865]}}
{"id": "WebQTest-1795", "question": "what are john terry 's children called", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.parent"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are john terry 's children called [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.009094343520700932, -0.01865980587899685, -0.1414511352777481], "norm_scores": [0.34885644912719727, 0.34553539752960205, 0.30560818314552307]}}
{"id": "WebQTest-1796", "question": "what city was michael jackson born in", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["music.artist.origin"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city was michael jackson born in [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.011700993403792381, -0.020223023369908333, -0.03915200009942055], "norm_scores": [0.33733224868774414, 0.33446967601776123, 0.32819807529449463]}}
{"id": "WebQTest-1797", "question": "who fought in the battle of vicksburg", "prediction": [["time.event.locations"], ["base.culturalevent.event.entity_involved"], ["military.military_conflict.military_personnel_involved"]], "ground_paths": [["base.culturalevent.event.entity_involved"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who fought in the battle of vicksburg [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>"], "scores": [-0.01645076461136341, -0.030977321788668633, -0.04525771737098694], "norm_scores": [0.3381597399711609, 0.3332829177379608, 0.3285573422908783]}}
{"id": "WebQTest-1799", "question": "what kind of writing is nikolai gogol famous for", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.author.school_or_movement"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of writing is nikolai gogol famous for [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.author.school_or_movement </PATH>"], "scores": [-0.015630729496479034, -0.024067271500825882, -0.03008255548775196], "norm_scores": [0.335880309343338, 0.33305856585502625, 0.33106112480163574]}}
{"id": "WebQTest-1800", "question": "what year was first world series", "prediction": [["time.event.instance_of_recurring_event"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.championship"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was first world series [/INST]", "raw_output": {"paths": ["<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.011989690363407135, -0.01798170432448387, -0.05342132970690727], "norm_scores": [0.3385881185531616, 0.3365653455257416, 0.3248465061187744]}}
{"id": "WebQTest-1801", "question": "who plays simon birch", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["award.award_honor.honored_for", "award.award_winner.awards_won"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays simon birch [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.021350665017962456, -0.02800128050148487, -0.04315925016999245], "norm_scores": [0.3364965319633484, 0.3342660665512085, 0.3292374610900879]}}
{"id": "WebQTest-1802", "question": "where are the timezones in the usa", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["base.locations.countries.continent"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the timezones in the usa [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-2.461082476656884e-05, -0.12205365300178528, -0.1524498611688614], "norm_scores": [0.36446529626846313, 0.3225964605808258, 0.31293827295303345]}}
{"id": "WebQTest-1803", "question": "what does gm make", "prediction": [["automotive.company.make_s"], ["automotive.make.parent_company"], ["common.topic.subjects"]], "ground_paths": [["business.business_operation.industry"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does gm make [/INST]", "raw_output": {"paths": ["<PATH> automotive.company.make_s </PATH>", "<PATH> automotive.make.parent_company </PATH>", "<PATH> common.topic.subjects </PATH>"], "scores": [-0.012224311009049416, -0.018320975825190544, -0.07259433716535568], "norm_scores": [0.34067627787590027, 0.3386055827140808, 0.32071810960769653]}}
{"id": "WebQTest-1804", "question": "what do people speak in uk", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people speak in uk [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.013304000720381737, -0.0225441325455904, -0.03593030571937561], "norm_scores": [0.33687838912010193, 0.3337799310684204, 0.3293416500091553]}}
{"id": "WebQTest-1805", "question": "what do people in jamaica speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people in jamaica speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01115050446242094, -0.01934274472296238, -0.044969797134399414], "norm_scores": [0.3379991352558136, 0.33524149656295776, 0.326759397983551]}}
{"id": "WebQTest-1806", "question": "what position did lebron james play", "prediction": [["basketball.basketball_player.position_s"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.position"], ["sports.sports_position.sport", "sports.sports_team.sport"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.position"], ["basketball.basketball_player.position_s"], ["sports.sports_team_roster.player", "sports.sports_team_roster.position"], ["basketball.basketball_position.players"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what position did lebron james play [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.position_s </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.position </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>"], "scores": [-0.008236636407673359, -0.03376827761530876, -0.046478521078825], "norm_scores": [0.34045201539993286, 0.3318697512149811, 0.32767829298973083]}}
{"id": "WebQTest-1807", "question": "what was wayne gretzky 's first team", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was wayne gretzky 's first team [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.008223778568208218, -0.020349424332380295, -0.035048194229602814], "norm_scores": [0.3376690447330475, 0.3335992991924286, 0.3287316560745239]}}
{"id": "WebQTest-1808", "question": "who is the president of peru now", "prediction": [["people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the president of peru now [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.003997341264039278, -0.03672836348414421, -0.043373215943574905], "norm_scores": [0.34139150381088257, 0.330398291349411, 0.32821014523506165]}}
{"id": "WebQTest-1809", "question": "what kind of guitar did jerry garcia play", "prediction": [["music.group_member.instruments_played"], ["music.guitarist.guitars_played"], ["music.guitar.guitarists"]], "ground_paths": [["music.guitar.guitarists"], ["music.guitarist.guitars_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of guitar did jerry garcia play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.guitar.guitarists </PATH>"], "scores": [-0.012789341621100903, -0.02647755667567253, -0.03317447751760483], "norm_scores": [0.33712872862815857, 0.3325454592704773, 0.3303258717060089]}}
{"id": "WebQTest-1810", "question": "what type of art leonardo da vinci do", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_art_form.artists"]], "ground_paths": [["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art leonardo da vinci do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>"], "scores": [-0.00022256597003433853, -0.08910490572452545, -0.09965493530035019], "norm_scores": [0.354571670293808, 0.32441648840904236, 0.32101187109947205]}}
{"id": "WebQTest-1811", "question": "who are the senators of new jersey now", "prediction": [["location.administrative_division.first_level_division_of", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["location.location.containedby", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the senators of new jersey now [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.first_level_division_of <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.017903558909893036, -0.018074538558721542, -0.021565953269600868], "norm_scores": [0.333759069442749, 0.333702027797699, 0.3325389325618744]}}
{"id": "WebQTest-1812", "question": "where was rihanna raised", "prediction": [["location.location.people_born_here"], ["music.artist.origin"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was rihanna raised [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.01978946663439274, -0.020936496555805206, -0.02597039006650448], "norm_scores": [0.3341473340988159, 0.33376428484916687, 0.3320883810520172]}}
{"id": "WebQTest-1813", "question": "what team does messi play for 2011", "prediction": [["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"]], "ground_paths": [["sports.sports_award.award_winner", "sports.sports_award.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does messi play for 2011 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>"], "scores": [-0.025132255628705025, -0.02577921934425831, -0.03862155228853226], "norm_scores": [0.33490127325057983, 0.33468466997146606, 0.3304140269756317]}}
{"id": "WebQTest-1815", "question": "where does lil wayne go to school", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does lil wayne go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.01624513603746891, -0.016483895480632782, -0.0280819833278656], "norm_scores": [0.3346726596355438, 0.3345927894115448, 0.33073458075523376]}}
{"id": "WebQTest-1817", "question": "where did jrr tolkien go to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.employment_history", "business.employment_tenure.company"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["business.employment_tenure.person", "business.employment_tenure.company"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did jrr tolkien go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.011654444970190525, -0.019389847293496132, -0.033581703901290894], "norm_scores": [0.3366316556930542, 0.334037721157074, 0.32933059334754944]}}
{"id": "WebQTest-1818", "question": "what state is saint louis university in", "prediction": [["location.location.containedby"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"], ["education.educational_institution.parent_institution", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state is saint louis university in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>"], "scores": [-0.006062721833586693, -0.03385848551988602, -0.04033762216567993], "norm_scores": [0.3402642011642456, 0.3309365212917328, 0.3287992775440216]}}
{"id": "WebQTest-1819", "question": "what religion does canada follow", "prediction": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"], ["royalty.monarch.kingdom", "people.person.religion"]], "ground_paths": [["royalty.monarch.kingdom", "people.person.religion"], ["book.newspaper.circulation_areas", "book.periodical_subject.periodicals"], ["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religion does canada follow [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.religion </PATH>"], "scores": [-0.012794101610779762, -0.021945659071207047, -0.026762165129184723], "norm_scores": [0.335906445980072, 0.33284640312194824, 0.33124709129333496]}}
{"id": "WebQTest-1821", "question": "which countries have spanish as their main language", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries have spanish as their main language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.008827362209558487, -0.027700455859303474, -0.03213810548186302], "norm_scores": [0.3380362391471863, 0.33171626925468445, 0.3302474915981293]}}
{"id": "WebQTest-1822", "question": "who is in paul ryan 's family", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["people.person.parents", "people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.parents"], ["people.person.parents"], ["people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.spouse_s", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is in paul ryan 's family [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.020221123471856117, -0.025415856391191483, -0.03422127664089203], "norm_scores": [0.3354673385620117, 0.33372920751571655, 0.3308034837245941]}}
{"id": "WebQTest-1823", "question": "what government does iraq use", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what government does iraq use [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.005956505890935659, -0.02247695066034794, -0.1306009739637375], "norm_scores": [0.3488664925098419, 0.34315040707588196, 0.30798307061195374]}}
{"id": "WebQTest-1824", "question": "what type of currency do they use in spain", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of currency do they use in spain [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.003516658442094922, -0.03200417384505272, -0.1309870034456253], "norm_scores": [0.350602388381958, 0.34075552225112915, 0.30864211916923523]}}
{"id": "WebQTest-1825", "question": "what did lincoln steffens wrote", "prediction": [["book.author.works_written"], ["book.written_work.author"], ["book.book_edition.author_editor"]], "ground_paths": [["book.author.works_written"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did lincoln steffens wrote [/INST]", "raw_output": {"paths": ["<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.012576634995639324, -0.016190653666853905, -0.07282553613185883], "norm_scores": [0.3403765559196472, 0.3391486406326294, 0.3204748034477234]}}
{"id": "WebQTest-1826", "question": "what schools did anders celsius attend", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what schools did anders celsius attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.018569795414805412, -0.019253723323345184, -0.02278853952884674], "norm_scores": [0.33387792110443115, 0.3336496651172638, 0.3324723541736603]}}
{"id": "WebQTest-1827", "question": "what can do in new york", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["location.neighborhood.neighborhood_of"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what can do in new york [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01387952920049429, -0.016498154029250145, -0.03452520817518234], "norm_scores": [0.33591416478157043, 0.3350357115268707, 0.32905012369155884]}}
{"id": "WebQTest-1828", "question": "what currency does france accept", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["location.country.currency_used"], ["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does france accept [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.00837218388915062, -0.02145829051733017, -0.09626892954111099], "norm_scores": [0.3444885015487671, 0.340009868144989, 0.3155016601085663]}}
{"id": "WebQTest-1829", "question": "what type of religion did massachusetts have", "prediction": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"], ["religion.religion.states"]], "ground_paths": [["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of religion did massachusetts have [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> religion.religion.states </PATH>"], "scores": [-0.003441241104155779, -0.038535308092832565, -0.07964149862527847], "norm_scores": [0.34576407074928284, 0.3338402807712555, 0.3203956186771393]}}
{"id": "WebQTest-1830", "question": "what language do they speak in colombia south america", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they speak in colombia south america [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.008272191509604454, -0.018610801547765732, -0.05721140652894974], "norm_scores": [0.3399101793766022, 0.336414098739624, 0.3236757218837738]}}
{"id": "WebQTest-1833", "question": "what kind of government does brazil has", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does brazil has [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <PATH>"], "scores": [-0.003727557370439172, -0.02904021553695202, -0.17209355533123016], "norm_scores": [0.35460370779037476, 0.345740407705307, 0.2996559143066406]}}
{"id": "WebQTest-1835", "question": "what country did gregor mendel live in", "prediction": [["people.deceased_person.place_of_death"], ["people.person.nationality"], ["location.location.people_born_here"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did gregor mendel live in [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.016015304252505302, -0.016155697405338287, -0.04718120023608208], "norm_scores": [0.3367939591407776, 0.3367466926574707, 0.3264593482017517]}}
{"id": "WebQTest-1836", "question": "where laura ingalls wilder lived", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.deceased_person.place_of_death"]], "ground_paths": [["people.person.place_of_birth"], ["people.person.nationality", "location.administrative_division.first_level_division_of"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.parents", "people.deceased_person.place_of_death"], ["book.author.works_written", "fictional_universe.work_of_fiction.setting"], ["book.book_edition.author_editor", "book.written_work.subjects"], ["people.person.nationality", "location.location.containedby"], ["influence.influence_node.influenced", "location.location.people_born_here"], ["people.person.places_lived", "people.place_lived.location"], ["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where laura ingalls wilder lived [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>"], "scores": [-0.025113902986049652, -0.028653312474489212, -0.029911400750279427], "norm_scores": [0.3342602550983429, 0.3330792486667633, 0.3326604664325714]}}
{"id": "WebQTest-1838", "question": "what sports are popular in germany", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["time.event.locations"]], "ground_paths": [["cricket.cricket_team.country"], ["sports.sports_team.location"], ["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sports are popular in germany [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.01042789313942194, -0.01544762309640646, -0.13650988042354584], "norm_scores": [0.34764042496681213, 0.3458997309207916, 0.30645984411239624]}}
{"id": "WebQTest-1839", "question": "what 's my timezone in oklahoma", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what 's my timezone in oklahoma [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-9.493094694335014e-06, -0.18883615732192993, -0.19141650199890137], "norm_scores": [0.3768286108970642, 0.3119877278804779, 0.31118372082710266]}}
{"id": "WebQTest-1840", "question": "what do they speak in norway", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do they speak in norway [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.017171377316117287, -0.018705548718571663, -0.03282638639211655], "norm_scores": [0.3352403938770294, 0.334726482629776, 0.3300330936908722]}}
{"id": "WebQTest-1842", "question": "what currency does singapore use", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does singapore use [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>"], "scores": [-0.004023552406579256, -0.0318758524954319, -0.13789114356040955], "norm_scores": [0.3512175679206848, 0.34157031774520874, 0.30721208453178406]}}
{"id": "WebQTest-1843", "question": "what did the blackfoot indians speak", "prediction": [["people.ethnicity.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.ethnicity.languages_spoken", "language.human_language.language_family", "language.human_language.language_family"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did the blackfoot indians speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> language.human_language.language_family <SEP> language.human_language.language_family </PATH>"], "scores": [-0.001164087443612516, -0.0671447142958641, -0.07594579458236694], "norm_scores": [0.3491503894329071, 0.326856791973114, 0.3239927291870117]}}
{"id": "WebQTest-1844", "question": "who plays robin hood in prince of thieves", "prediction": [["film.film.country", "people.person.nationality"], ["film.film.starring", "film.actor.film"], ["film.film.starring", "film.performance.actor"]], "ground_paths": [["film.producer.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays robin hood in prince of thieves [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film.starring <SEP> film.actor.film </PATH>", "<PATH> film.film.starring <SEP> film.performance.actor </PATH>"], "scores": [-0.012404005974531174, -0.03698291629552841, -0.04969248175621033], "norm_scores": [0.3402380347251892, 0.3319772779941559, 0.3277847170829773]}}
{"id": "WebQTest-1845", "question": "what standard time is texas", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_Zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what standard time is texas [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_Zones </PATH>"], "scores": [-1.6850824977154844e-05, -0.13700832426548004, -0.185678631067276], "norm_scores": [0.37002333998680115, 0.32265210151672363, 0.30732452869415283]}}
{"id": "WebQTest-1847", "question": "where does kurdish people live", "prediction": [["people.ethnicity.geographic_distribution"], ["location.location.containedby"], ["people.ethnicity.languages_spoken"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does kurdish people live [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>"], "scores": [-0.0011915592476725578, -0.0610395222902298, -0.11689483374357224], "norm_scores": [0.3530266582965851, 0.3325185477733612, 0.3144547939300537]}}
{"id": "WebQTest-1848", "question": "what instrument does louis armstrong", "prediction": [["music.group_member.instruments_played"], ["music.artist.track_contributions", "music.track_contribution.role"], ["music.group_member.instruments_played", "music.group_member.instruments_played"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what instrument does louis armstrong [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>", "<PATH> music.group_member.instruments_played <SEP> music.group_member.instruments_played </PATH>"], "scores": [-0.0002591328229755163, -0.07951389998197556, -0.1239338219165802], "norm_scores": [0.35619235038757324, 0.3290521204471588, 0.31475552916526794]}}
{"id": "WebQTest-1849", "question": "who does alyson stoner play in camp rock", "prediction": [["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"]], "ground_paths": [["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"], ["film.film.starring", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does alyson stoner play in camp rock [/INST]", "raw_output": {"paths": ["<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.024942297488451004, -0.0250704288482666, -0.02544042281806469], "norm_scores": [0.3334029018878937, 0.3333601951599121, 0.3332368731498718]}}
{"id": "WebQTest-1850", "question": "what religion does madonna practice", "prediction": [["people.person.religion"], ["religion.religion.notable_figures"], ["religion.religion.practices"]], "ground_paths": [["people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religion does madonna practice [/INST]", "raw_output": {"paths": ["<PATH> people.person.religion </PATH>", "<PATH> religion.religion.notable_figures </PATH>", "<PATH> religion.religion.practices </PATH>"], "scores": [-0.002685138024389744, -0.040174584835767746, -0.10279425233602524], "norm_scores": [0.3486819267272949, 0.3358520567417145, 0.315466046333313]}}
{"id": "WebQTest-1851", "question": "what countries are located near egypt", "prediction": [["location.location.events", "time.event.locations"], ["location.location.partially_contains", "location.location.partially_contains"], ["time.event.locations", "time.event.locations"]], "ground_paths": [["location.location.contains_major_portion_of", "location.location.containedby"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["location.location.contains_major_portion_of", "base.locations.countries.continent"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.statistical_region.places_exported_to", "location.imports_and_exports.exported_to"], ["location.location.events", "time.event.locations"], ["language.human_language.countries_spoken_in", "location.country.official_language"], ["location.location.time_zones", "location.location.time_zones"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.partially_containedby", "location.location.contains"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["location.location.contains", "location.location.contains"], ["location.location.partially_contains", "location.location.partially_containedby"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["base.schemastaging.disputed_location.claimed_by", "base.schemastaging.disputed_location_claimant.disputed_territories"], ["film.film.featured_film_locations", "film.film.featured_film_locations"], ["location.location.events", "location.location.events"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.contains", "location.location.containedby"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["time.event.locations", "time.event.locations"], ["location.location.partially_contains", "location.location.containedby"], ["government.form_of_government.countries", "location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are located near egypt [/INST]", "raw_output": {"paths": ["<PATH> location.location.events <SEP> time.event.locations </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> time.event.locations <SEP> time.event.locations </PATH>"], "scores": [-0.04282044619321823, -0.05119277909398079, -0.05250832810997963], "norm_scores": [0.33534300327301025, 0.332547128200531, 0.3321099281311035]}}
{"id": "WebQTest-1853", "question": "what made richard hammond famous", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what made richard hammond famous [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.00175567832775414, -0.04842556267976761, -0.11877452582120895], "norm_scores": [0.3516210615634918, 0.3355879783630371, 0.31279098987579346]}}
{"id": "WebQTest-1854", "question": "who originally voiced lois griffin", "prediction": [["film.performance.character", "film.actor.film"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["people.person.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who originally voiced lois griffin [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.02761823683977127, -0.028107577934861183, -0.033759403973817825], "norm_scores": [0.3340695798397064, 0.3339061439037323, 0.3320242762565613]}}
{"id": "WebQTest-1855", "question": "what airport do you fly into in paris", "prediction": [["aviation.airport.serves"], ["location.location.nearby_airports"], ["location.location.containedby"]], "ground_paths": [["aviation.airport.serves"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what airport do you fly into in paris [/INST]", "raw_output": {"paths": ["<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.nearby_airports </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.006856859195977449, -0.024876413866877556, -0.05910602584481239], "norm_scores": [0.34115323424339294, 0.33506083488464355, 0.3237859308719635]}}
{"id": "WebQTest-1857", "question": "what rihanna new album called 2012", "prediction": [["music.album.artist"], ["music.artist.album"], ["music.recording.artist"]], "ground_paths": [["music.album.artist"], ["music.artist.album"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what rihanna new album called 2012 [/INST]", "raw_output": {"paths": ["<PATH> music.album.artist </PATH>", "<PATH> music.artist.album </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.00975888129323721, -0.021781248971819878, -0.056123681366443634], "norm_scores": [0.3398189842700958, 0.3357580006122589, 0.32442301511764526]}}
{"id": "WebQTest-1858", "question": "where does dave ramsey live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["location.location.people_born_here"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does dave ramsey live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.02531106024980545, -0.026491308584809303, -0.03091113269329071], "norm_scores": [0.3340865671634674, 0.3336925208568573, 0.3322209119796753]}}
{"id": "WebQTest-1860", "question": "what discovery did galileo make", "prediction": [["astronomy.astronomer.astronomical_objects_discovered"], ["law.invention.inventor"], ["symbols.namesake.named_after"]], "ground_paths": [["astronomy.astronomical_discovery.discoverer"], ["astronomy.astronomer.astronomical_objects_discovered"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what discovery did galileo make [/INST]", "raw_output": {"paths": ["<PATH> astronomy.astronomer.astronomical_objects_discovered </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.012559959664940834, -0.03573181480169296, -0.039774660021066666], "norm_scores": [0.3389546871185303, 0.3311907649040222, 0.3298545181751251]}}
{"id": "WebQTest-1861", "question": "what is the government system of saudi arabia", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["location.location.contains", "location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the government system of saudi arabia [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> location.location.contains <SEP> location.country.form_of_government </PATH>"], "scores": [-0.010883557610213757, -0.01260462123900652, -0.09619098156690598], "norm_scores": [0.34287551045417786, 0.3422859013080597, 0.31483855843544006]}}
{"id": "WebQTest-1864", "question": "who plays the role of tony dinozzo on ncis", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays the role of tony dinozzo on ncis [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.017999740317463875, -0.030547119677066803, -0.031165366992354393], "norm_scores": [0.3361963927745819, 0.3320043683052063, 0.3317992091178894]}}
{"id": "WebQTest-1865", "question": "what did jesus do for living", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did jesus do for living [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0010148053988814354, -0.07613170146942139, -0.07848434150218964], "norm_scores": [0.35049715638160706, 0.32513344287872314, 0.3243694305419922]}}
{"id": "WebQTest-1866", "question": "what is the state name of new york city", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["base.biblioness.bibs_location.state"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the state name of new york city [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.011213896796107292, -0.050650421530008316, -0.05082016438245773], "norm_scores": [0.3421729505062103, 0.3289414644241333, 0.32888564467430115]}}
{"id": "WebQTest-1868", "question": "who plays knox overstreet", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "ground_paths": [["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays knox overstreet [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.02705519087612629, -0.03364214673638344, -0.036909908056259155], "norm_scores": [0.3351624011993408, 0.33296194672584534, 0.33187568187713623]}}
{"id": "WebQTest-1870", "question": "what movies did david carradine play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["film.film.edited_by"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["people.person.languages", "tv.tv_program.languages"], ["film.director.film"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.producer.films_executive_produced"], ["people.person.languages", "film.film.language"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "film.film.country"], ["film.personal_film_appearance.person", "film.personal_film_appearance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did david carradine play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.016212603077292442, -0.034662459045648575, -0.036439649760723114], "norm_scores": [0.33764442801475525, 0.3314720690250397, 0.3308835029602051]}}
{"id": "WebQTest-1871", "question": "what is the stanley cup named after", "prediction": [["sports.sports_championship_event.championship"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.champion", "sports.sports_team_owner.teams_owned"]], "ground_paths": [["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the stanley cup named after [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_team_owner.teams_owned </PATH>"], "scores": [-0.014820849522948265, -0.01706562004983425, -0.04735483229160309], "norm_scores": [0.3371831476688385, 0.33642709255218506, 0.32638975977897644]}}
{"id": "WebQTest-1873", "question": "where is traditional chinese used", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is traditional chinese used [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.018094288185238838, -0.025496773421764374, -0.028144462034106255], "norm_scores": [0.33527514338493347, 0.332802414894104, 0.3319224417209625]}}
{"id": "WebQTest-1874", "question": "who does portugal trade with", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"]], "ground_paths": [["people.person.nationality", "location.location.people_born_here"], ["location.country.form_of_government", "government.form_of_government.countries"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.sport_country.multi_event_tournaments_participated_in"], ["location.imports_and_exports.imported_from", "location.imports_and_exports.exported_to"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.statistical_region.places_exported_to", "location.imports_and_exports.exported_to"], ["language.human_language.countries_spoken_in", "location.country.official_language"], ["location.location.time_zones", "location.location.time_zones"], ["book.written_work.subjects", "book.written_work.subjects"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["organization.organization.founders", "organization.organization.founders"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["common.topic.notable_types", "common.topic.notable_types"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["location.country.form_of_government", "location.country.form_of_government"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["military.military_combatant.belongs_to_combatant_group", "military.military_combatant.includes_allies"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["government.form_of_government.countries", "location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does portugal trade with [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> location.imports_and_exports.imported_from <SEP> location.statistical_region.places_imported_from </PATH>"], "scores": [-0.02922314591705799, -0.03093692660331726, -0.03779210150241852], "norm_scores": [0.3344755172729492, 0.3339028060436249, 0.3316216766834259]}}
{"id": "WebQTest-1875", "question": "what type of political system is brazil", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of political system is brazil [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <PATH>"], "scores": [-0.0047147925943136215, -0.025335803627967834, -0.17222009599208832], "norm_scores": [0.35393691062927246, 0.3467130959033966, 0.29934996366500854]}}
{"id": "WebQTest-1876", "question": "what language did the ancient babylonians speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language did the ancient babylonians speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.007834129966795444, -0.02990851178765297, -0.03569096326828003], "norm_scores": [0.33890318870544434, 0.33150407671928406, 0.3295927345752716]}}
{"id": "WebQTest-1877", "question": "what kind of painter was wassily kandinsky", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"], ["book.author.school_or_movement"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of painter was wassily kandinsky [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> book.author.school_or_movement </PATH>"], "scores": [-0.0033146103378385305, -0.0377345010638237, -0.05950137600302696], "norm_scores": [0.3434622287750244, 0.33184143900871277, 0.3246963322162628]}}
{"id": "WebQTest-1878", "question": "what form of government does the united states use", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does the united states use [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.form_of_government </PATH>"], "scores": [-0.007059067487716675, -0.01819998398423195, -0.14864203333854675], "norm_scores": [0.35002925992012024, 0.3461512625217438, 0.3038195073604584]}}
{"id": "WebQTest-1879", "question": "what money does cuba use", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country"]], "ground_paths": [["location.country.currency_used"], ["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money does cuba use [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>"], "scores": [-0.012255643494427204, -0.015083355829119682, -0.14394263923168182], "norm_scores": [0.3479723334312439, 0.3469897508621216, 0.30503788590431213]}}
{"id": "WebQTest-1880", "question": "who is my representative in illinois house", "prediction": [["government.government_position_held.governmental_body", "government.politician.government_positions_held"], ["government.government_position_held.governmental_body", "government.government_position_held.office_holder"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"]], "ground_paths": [["government.governmental_body.members", "government.government_position_held.office_holder"], ["government.government_position_held.governmental_body", "government.politician.government_positions_held"], ["government.governmental_body.members", "government.politician.government_positions_held"], ["government.government_position_held.governmental_body", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is my representative in illinois house [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.governmental_body <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.02250576578080654, -0.03022206574678421, -0.030965641140937805], "norm_scores": [0.33513307571411133, 0.3325570523738861, 0.3323098421096802]}}
{"id": "WebQTest-1881", "question": "what timezone is sweden gmt", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is sweden gmt [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.0029439886566251516, -0.032769687473773956, -0.1337653547525406], "norm_scores": [0.35112494230270386, 0.3408070206642151, 0.30806806683540344]}}
{"id": "WebQTest-1882", "question": "what was franklin d roosevelt 's job before president", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_office_or_title.category", "government.government_office_or_title.jurisdiction", "base.aareas.schema.administrative_area.administrative_parent", "government.governmental_jurisdiction.government_positions"], ["government.government_position_held.basic_title", "government.government_position_held.jurisdiction_of_office", "government.governmental_jurisdiction.government_positions"]], "ground_paths": [["government.government_position_held.appointed_by", "government.government_office_or_title.office_holders"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was franklin d roosevelt 's job before president [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_office_or_title.category <SEP> government.government_office_or_title.jurisdiction <SEP> base.aareas.schema.administrative_area.administrative_parent <SEP> government.governmental_jurisdiction.government_positions </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.jurisdiction_of_office <SEP> government.governmental_jurisdiction.government_positions </PATH>"], "scores": [-0.018975794315338135, -0.025344092398881912, -0.027737081050872803], "norm_scores": [0.3350163698196411, 0.3328896462917328, 0.3320940136909485]}}
{"id": "WebQTest-1883", "question": "where did robert downey jr go to jail", "prediction": [["people.person.nationality", "location.location.containedby"], ["base.prison.imprisonment.prisoner", "base.prison.prison.prisoners"], ["people.person.languages", "location.country.official_language"]], "ground_paths": [["celebrities.rehab.celebrity", "celebrities.rehab.rehab_facility"], ["base.prison.imprisonment.prisoner", "base.prison.imprisonment.prison"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did robert downey jr go to jail [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> base.prison.imprisonment.prisoner <SEP> base.prison.prison.prisoners </PATH>", "<PATH> people.person.languages <SEP> location.country.official_language </PATH>"], "scores": [-0.00694811437278986, -0.02724224142730236, -0.04850216954946518], "norm_scores": [0.34022772312164307, 0.3333926796913147, 0.32637959718704224]}}
{"id": "WebQTest-1884", "question": "what degrees did barack obama get", "prediction": [["education.education.student", "education.education.degree"], ["people.person.education", "education.education.degree"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.degree"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what degrees did barack obama get [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.011099130846560001, -0.019731195643544197, -0.027147311717271805], "norm_scores": [0.3360796868801117, 0.3331910967826843, 0.33072924613952637]}}
{"id": "WebQTest-1885", "question": "what does god shiva represent", "prediction": [["religion.religion.deities"], ["religion.deity.deity_of"], ["common.topic.notable_types"]], "ground_paths": [["religion.deity.deity_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does god shiva represent [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.deities </PATH>", "<PATH> religion.deity.deity_of </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.010800709947943687, -0.025630628690123558, -0.0531049519777298], "norm_scores": [0.33969032764434814, 0.3346899151802063, 0.32561972737312317]}}
{"id": "WebQTest-1886", "question": "what inventions were made by ben franklin", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"]], "ground_paths": [["law.invention.inventor"], ["law.inventor.inventions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inventions were made by ben franklin [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>"], "scores": [-0.007684445474296808, -0.020965276286005974, -0.07282755523920059], "norm_scores": [0.3420276343822479, 0.3375152349472046, 0.3204571008682251]}}
{"id": "WebQTest-1887", "question": "what is the name of the currency used in switzerland", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["common.topic.notable_types"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the currency used in switzerland [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.004797085653990507, -0.02573331631720066, -0.1622917503118515], "norm_scores": [0.352912575006485, 0.3456007242202759, 0.30148664116859436]}}
{"id": "WebQTest-1888", "question": "what year did the milwaukee brewers go to the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the milwaukee brewers go to the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.005433299578726292, -0.021874763071537018, -0.04776835814118385], "norm_scores": [0.3398769497871399, 0.3343345522880554, 0.3257885277271271]}}
{"id": "WebQTest-1889", "question": "who was vice president with bill clinton", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["government.us_vice_president.to_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vice president with bill clinton [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.018350331112742424, -0.021941397339105606, -0.03738953173160553], "norm_scores": [0.3358458876609802, 0.3346419930458069, 0.3295121192932129]}}
{"id": "WebQTest-1890", "question": "what songs did franz liszt compose", "prediction": [["music.composition.composer"], ["music.composition.lyricist"], ["music.lyricist.lyrics_written"]], "ground_paths": [["book.author.works_written"], ["music.composition.composer"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what songs did franz liszt compose [/INST]", "raw_output": {"paths": ["<PATH> music.composition.composer </PATH>", "<PATH> music.composition.lyricist </PATH>", "<PATH> music.lyricist.lyrics_written </PATH>"], "scores": [-0.0030892493668943644, -0.061421334743499756, -0.06149092689156532], "norm_scores": [0.34642741084098816, 0.3267976641654968, 0.3267749547958374]}}
{"id": "WebQTest-1891", "question": "where did the russian japanese war happen", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the russian japanese war happen [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.011467606760561466, -0.018256288021802902, -0.060840051621198654], "norm_scores": [0.33955153822898865, 0.33725425601005554, 0.3231941759586334]}}
{"id": "WebQTest-1892", "question": "what year did the la kings win the cup", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the la kings win the cup [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.008104734122753143, -0.01620480976998806, -0.11799579113721848], "norm_scores": [0.3462766408920288, 0.3434831202030182, 0.3102402687072754]}}
{"id": "WebQTest-1893", "question": "what kind of money does egypt have", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money does egypt have [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.0025820338632911444, -0.04179505258798599, -0.08487632870674133], "norm_scores": [0.34691542387008667, 0.3335750997066498, 0.3195094168186188]}}
{"id": "WebQTest-1894", "question": "what baseball team was jackie robinson on", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what baseball team was jackie robinson on [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.014545437879860401, -0.014783386141061783, -0.031903840601444244], "norm_scores": [0.33528316020965576, 0.3352034091949463, 0.32951340079307556]}}
{"id": "WebQTest-1895", "question": "what body of water does st lawrence river flow into", "prediction": [["geography.river.mouth"], ["geography.body_of_water.mouth"], ["location.location.containedby", "geography.lake.basin_countries"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what body of water does st lawrence river flow into [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> geography.body_of_water.mouth </PATH>", "<PATH> location.location.containedby <SEP> geography.lake.basin_countries </PATH>"], "scores": [-0.0027727638371288776, -0.05156951770186424, -0.07096870988607407], "norm_scores": [0.34644606709480286, 0.32994645833969116, 0.3236074447631836]}}
{"id": "WebQTest-1896", "question": "what form of government does the united states have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does the united states have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.form_of_government </PATH>"], "scores": [-0.007239605765789747, -0.01785149797797203, -0.14710132777690887], "norm_scores": [0.3497821092605591, 0.3460898697376251, 0.3041279911994934]}}
{"id": "WebQTest-1897", "question": "what are the names of ariel 's six sisters", "prediction": [["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.children"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.parents"], ["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"]], "ground_paths": [["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.parents"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.children"], ["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"], ["fictional_universe.sibling_relationship_of_fictional_characters.siblings", "fictional_universe.fictional_character.siblings"], ["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.sibling_relationship_of_fictional_characters.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"], ["fictional_universe.fictional_character.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"], ["fictional_universe.fictional_character.siblings", "fictional_universe.fictional_character.siblings"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the names of ariel 's six sisters [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.parents <SEP> fictional_universe.fictional_character.children </PATH>", "<PATH> fictional_universe.fictional_character.parents <SEP> fictional_universe.fictional_character.parents </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> fictional_universe.fictional_character.gender </PATH>"], "scores": [-0.01648341491818428, -0.021548230201005936, -0.02791575901210308], "norm_scores": [0.3351677656173706, 0.3334744870662689, 0.33135780692100525]}}
{"id": "WebQTest-1898", "question": "what voices does seth macfarlane play on family guy", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_universe.characters"], ["tv.regular_tv_appearance.series", "tv.tv_character.appeared_in_tv_program"]], "ground_paths": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_universe.characters"], ["tv.tv_program.regular_cast", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.series", "tv.tv_character.appeared_in_tv_program"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.character"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"], ["tv.tv_program_creator.programs_created", "fictional_universe.fictional_character.character_created_by"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what voices does seth macfarlane play on family guy [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes <SEP> fictional_universe.fictional_character.appears_in_these_fictional_universes </PATH>", "<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes <SEP> fictional_universe.fictional_universe.characters </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_character.appeared_in_tv_program </PATH>"], "scores": [-0.01914534717798233, -0.021713437512516975, -0.024366559460759163], "norm_scores": [0.3341991901397705, 0.333342045545578, 0.3324587941169739]}}
{"id": "WebQTest-1899", "question": "who was william mckinley married to", "prediction": [["base.famouspets.pet_ownership.owner", "base.famouspets.pet_owner.pets_owned"], ["people.person.parents", "people.person.parents"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_ownership.owner"]], "ground_paths": [["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_owner.pets_owned"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_owner.pets_owned"], ["people.person.children", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.profession", "common.topic.notable_types"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was william mckinley married to [/INST]", "raw_output": {"paths": ["<PATH> base.famouspets.pet_ownership.owner <SEP> base.famouspets.pet_owner.pets_owned </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> base.famouspets.pet_ownership.owner <SEP> base.famouspets.pet_ownership.owner </PATH>"], "scores": [-0.037464845925569534, -0.038301292806863785, -0.038449425250291824], "norm_scores": [0.33353570103645325, 0.3332568407058716, 0.33320745825767517]}}
{"id": "WebQTest-1900", "question": "who played darth vader in the first movie", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "ground_paths": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played darth vader in the first movie [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>"], "scores": [-0.019458148628473282, -0.019858932122588158, -0.02426631562411785], "norm_scores": [0.33391183614730835, 0.3337780237197876, 0.33231016993522644]}}
{"id": "WebQTest-1901", "question": "where did kevin james go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kevin james go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.013154799118638039, -0.017677824944257736, -0.0270080603659153], "norm_scores": [0.3353758156299591, 0.3338623344898224, 0.3307618200778961]}}
{"id": "WebQTest-1902", "question": "what year was hitler was born", "prediction": [["people.person.place_of_birth"], ["military.military_person.participated_in_conflicts"], ["base.activism.activist.area_of_activism", "time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was hitler was born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> time.event.locations </PATH>"], "scores": [-0.011473510414361954, -0.037159476429224014, -0.04375340789556503], "norm_scores": [0.3398035764694214, 0.33118653297424316, 0.32900992035865784]}}
{"id": "WebQTest-1904", "question": "what 's there to see in atlanta", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what 's there to see in atlanta [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.015861043706536293, -0.017376169562339783, -0.023448243737220764], "norm_scores": [0.33434444665908813, 0.33383825421333313, 0.3318173289299011]}}
{"id": "WebQTest-1905", "question": "where was auschwitz concentration camp located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["time.event.locations"]], "ground_paths": [["location.location.containedby"], ["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was auschwitz concentration camp located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.00200164713896811, -0.0648873820900917, -0.08836665749549866], "norm_scores": [0.3501020073890686, 0.32876357436180115, 0.3211343586444855]}}
{"id": "WebQTest-1907", "question": "what kind of currency does dominican republic use", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of currency does dominican republic use [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-8.139074634527788e-05, -0.10546284914016724, -0.1316617727279663], "norm_scores": [0.3601410388946533, 0.3241201341152191, 0.31573882699012756]}}
{"id": "WebQTest-1909", "question": "what is spoken in czech republic", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is spoken in czech republic [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.012421700172126293, -0.01815865933895111, -0.04404870793223381], "norm_scores": [0.33747899532318115, 0.33554843068122864, 0.3269725739955902]}}
{"id": "WebQTest-1910", "question": "where was shakespeare born at", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was shakespeare born at [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.012587831355631351, -0.014162600040435791, -0.08585473150014877], "norm_scores": [0.34155580401420593, 0.3410183787345886, 0.31742584705352783]}}
{"id": "WebQTest-1911", "question": "where did scott fitzgerald go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did scott fitzgerald go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.013706074096262455, -0.01503127720206976, -0.026505442336201668], "norm_scores": [0.3349009156227112, 0.3344573974609375, 0.3306416869163513]}}
{"id": "WebQTest-1912", "question": "what did joan crawford died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did joan crawford died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>"], "scores": [-0.00256431195884943, -0.035398103296756744, -0.12084442377090454], "norm_scores": [0.3501220941543579, 0.3388129472732544, 0.3110649883747101]}}
{"id": "WebQTest-1913", "question": "where did rob kardashian go to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.place_of_birth", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did rob kardashian go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.01401863805949688, -0.018473109230399132, -0.027416452765464783], "norm_scores": [0.33531761169433594, 0.33382728695869446, 0.3308550715446472]}}
{"id": "WebQTest-1914", "question": "what year was allen iverson mvp", "prediction": [["award.award_honor.award_winner", "award.award_honor.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"], ["award.award_winner.awards", "award.award_honor.season"]], "ground_paths": [["sports.sports_award.award_winner", "sports.sports_award.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["sports.sports_award_winner.awards", "sports.sports_league_season.awards"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was allen iverson mvp [/INST]", "raw_output": {"paths": ["<PATH> award.award_honor.award_winner <SEP> award.award_honor.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> award.award_winner.awards <SEP> award.award_honor.season </PATH>"], "scores": [-0.028165997937321663, -0.03032342903316021, -0.042378511279821396], "norm_scores": [0.3351506292819977, 0.3344283401966095, 0.33042100071907043]}}
{"id": "WebQTest-1915", "question": "where did harry s truman go to school", "prediction": [["book.written_work.subjects", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["book.written_work.subjects", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did harry s truman go to school [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.01089490670710802, -0.026714017614722252, -0.03318921476602554], "norm_scores": [0.3375803530216217, 0.332282155752182, 0.3301374912261963]}}
{"id": "WebQTest-1916", "question": "what county is utica ny in", "prediction": [["location.location.containedby"], ["location.us_county.hud_county_place"], ["location.hud_county_place.county"]], "ground_paths": [["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is utica ny in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.022223809733986855, -0.023866208270192146, -0.02400885708630085], "norm_scores": [0.33371424674987793, 0.33316662907600403, 0.33311909437179565]}}
{"id": "WebQTest-1917", "question": "what did charles babbage discover", "prediction": [["law.invention.inventor"], ["symbols.namesake.named_after"], ["base.argumentmaps.innovator.original_ideas"]], "ground_paths": [["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did charles babbage discover [/INST]", "raw_output": {"paths": ["<PATH> law.invention.inventor </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>"], "scores": [-0.0030456488020718098, -0.05365561693906784, -0.06075558066368103], "norm_scores": [0.3454740643501282, 0.32842472195625305, 0.3261011838912964]}}
{"id": "WebQTest-1919", "question": "who plays king julian madagascar", "prediction": [["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["film.performance.film", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays king julian madagascar [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.029590869322419167, -0.03501227870583534, -0.04045386239886284], "norm_scores": [0.3351443409919739, 0.3333323001861572, 0.3315233886241913]}}
{"id": "WebQTest-1920", "question": "who is rick scott", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_position_held.office_holder"], ["government.politician.government_positions_held"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"], ["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is rick scott [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.politician.government_positions_held </PATH>"], "scores": [-0.0017609200440347195, -0.057655736804008484, -0.07857214659452438], "norm_scores": [0.3482253849506378, 0.3292953670024872, 0.3224792182445526]}}
{"id": "WebQTest-1921", "question": "where did the crimean war take place", "prediction": [["time.event.locations"], ["location.location.events"], ["book.written_work.subjects"]], "ground_paths": [["time.event.locations"], ["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the crimean war take place [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.009482456371188164, -0.019985167309641838, -0.09310033917427063], "norm_scores": [0.3437211811542511, 0.3401300609111786, 0.3161487579345703]}}
{"id": "WebQTest-1922", "question": "what offices did theodore roosevelt hold", "prediction": [["business.employment_tenure.person", "business.employment_tenure.title"], ["government.politician.government_positions_held", "government.government_position_held.basic_title"], ["common.topic.notable_types", "type.type.expected_by"]], "ground_paths": [["government.government_position_held.office_holder", "government.government_position_held.basic_title"], ["government.politician.government_positions_held", "government.government_position_held.basic_title"], ["government.government_position_held.appointed_by", "government.government_position_held.basic_title"], ["business.employment_tenure.person", "business.employment_tenure.title"], ["government.government_position_held.office_holder", "government.government_office_category.officeholders"], ["common.topic.notable_types", "type.type.expected_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what offices did theodore roosevelt hold [/INST]", "raw_output": {"paths": ["<PATH> business.employment_tenure.person <SEP> business.employment_tenure.title </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.basic_title </PATH>", "<PATH> common.topic.notable_types <SEP> type.type.expected_by </PATH>"], "scores": [-0.01643158495426178, -0.018063802272081375, -0.027384350076317787], "norm_scores": [0.33473074436187744, 0.33418482542037964, 0.3310844898223877]}}
{"id": "WebQTest-1923", "question": "what movies has miley cyrus been in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["music.artist.genre", "music.album.genre"]], "ground_paths": [["film.person_or_entity_appearing_in_film.films", "film.film.personal_appearances"], ["people.person.children", "film.film.produced_by"], ["film.actor.film", "film.film.starring"], ["music.artist.genre", "music.album.genre"], ["people.person.children", "film.producer.film"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["music.lyricist.lyrics_written", "film.film.featured_song"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["film.film_featured_song.performed_by", "film.film.featured_song"], ["film.performance.actor", "film.performance.film"], ["people.person.children", "film.film.executive_produced_by"], ["music.recording_contribution.contributor", "music.recording_contribution.album"], ["music.artist.origin", "film.film.featured_film_locations"], ["people.person.languages", "film.film.language"], ["film.performance.actor", "film.film.starring"], ["common.topic.notable_types", "common.topic.notable_types"], ["music.artist.concert_tours", "music.album.supporting_tours"], ["people.person.languages", "tv.tv_program.languages"], ["music.artist.genre", "media_common.netflix_title.netflix_genres"], ["film.film.music"], ["film.actor.film", "film.performance.film"], ["music.concert_tour.artist", "music.album.supporting_tours"], ["music.artist.album", "media_common.adaptation.adapted_from"], ["people.person.nationality", "film.film.country"], ["people.person.parents", "film.film.produced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies has miley cyrus been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> music.artist.genre <SEP> music.album.genre </PATH>"], "scores": [-0.027650287374854088, -0.029082180932164192, -0.037367403507232666], "norm_scores": [0.33457133173942566, 0.3340926170349121, 0.33133599162101746]}}
{"id": "WebQTest-1925", "question": "who did james franco play in milk", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did james franco play in milk [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.01910894364118576, -0.02710593119263649, -0.03308108448982239], "norm_scores": [0.33577781915664673, 0.3331032991409302, 0.3311188817024231]}}
{"id": "WebQTest-1926", "question": "when did the libyan conflict start", "prediction": [["time.event.locations"], ["time.event.includes_event"], ["time.event.included_in_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the libyan conflict start [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> time.event.included_in_event </PATH>"], "scores": [-0.010631868615746498, -0.02599354460835457, -0.03709034621715546], "norm_scores": [0.33799266815185547, 0.3328402042388916, 0.3291671574115753]}}
{"id": "WebQTest-1928", "question": "what wnba team is diana taurasi on", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what wnba team is diana taurasi on [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.019053585827350616, -0.025442132726311684, -0.02884138561785221], "norm_scores": [0.33513280749320984, 0.3329986035823822, 0.33186858892440796]}}
{"id": "WebQTest-1929", "question": "what do see in paris", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["base.arthist.helynevek.intezmeny"], ["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do see in paris [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.012505090795457363, -0.016795698553323746, -0.040632352232933044], "norm_scores": [0.3369291126728058, 0.33548659086227417, 0.32758426666259766]}}
{"id": "WebQTest-1932", "question": "who made the laws in canada", "prediction": [["government.governmental_jurisdiction.government_bodies"], ["government.governmental_jurisdiction.government"], ["government.governmental_body.jurisdiction"]], "ground_paths": [["government.governmental_jurisdiction.government_bodies"], ["government.governmental_body.jurisdiction"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who made the laws in canada [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.government_bodies </PATH>", "<PATH> government.governmental_jurisdiction.government </PATH>", "<PATH> government.governmental_body.jurisdiction </PATH>"], "scores": [-0.015328929759562016, -0.03073584847152233, -0.04796237498521805], "norm_scores": [0.3386840522289276, 0.3335059881210327, 0.32781001925468445]}}
{"id": "WebQTest-1933", "question": "what year did adam morrison get drafted", "prediction": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did adam morrison get drafted [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>"], "scores": [-0.019621122628450394, -0.021982252597808838, -0.02325083501636982], "norm_scores": [0.33399927616119385, 0.33321160078048706, 0.3327891528606415]}}
{"id": "WebQTest-1934", "question": "where michael jackson from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["music.artist.origin"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where michael jackson from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.005926619749516249, -0.035209957510232925, -0.04761867597699165], "norm_scores": [0.34126123785972595, 0.3314128816127777, 0.32732588052749634]}}
{"id": "WebQTest-1935", "question": "what college did maya moore go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did maya moore go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.012918638996779919, -0.020996904000639915, -0.02479972317814827], "norm_scores": [0.335554301738739, 0.3328545391559601, 0.3315911293029785]}}
{"id": "WebQTest-1936", "question": "when was the last time the dodgers won a championship", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.mascot.team"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the dodgers won a championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.mascot.team </PATH>"], "scores": [-0.009754792787134647, -0.013607577420771122, -0.10014093667268753], "norm_scores": [0.3436741530895233, 0.34235262870788574, 0.3139732778072357]}}
{"id": "WebQTest-1937", "question": "who is captain james cook facts", "prediction": [["people.person.profession"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is captain james cook facts [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.0017611468210816383, -0.06420287489891052, -0.07656848430633545], "norm_scores": [0.34874919056892395, 0.3276386260986328, 0.32361212372779846]}}
{"id": "WebQTest-1938", "question": "where did robin williams get married", "prediction": [["people.person.children", "people.person.place_of_birth"], ["people.marriage.spouse", "people.marriage.location_of_ceremony"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.spouse_s", "people.marriage.location_of_ceremony"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did robin williams get married [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.location_of_ceremony </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.0234772227704525, -0.026153596118092537, -0.028428129851818085], "norm_scores": [0.33418118953704834, 0.3332880139350891, 0.33253076672554016]}}
{"id": "WebQTest-1939", "question": "what form of government does canada", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does canada [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> form_of_government.countries </PATH>"], "scores": [-0.008332443423569202, -0.017329489812254906, -0.16848108172416687], "norm_scores": [0.35173362493515015, 0.34858328104019165, 0.2996830642223358]}}
{"id": "WebQTest-1940", "question": "who do islamic people follow", "prediction": [["religion.religion.deities"], ["religion.deity.deity_of"], ["people.person.religion"]], "ground_paths": [["religion.deity.deity_of"], ["religion.religion.deities"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who do islamic people follow [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.deities </PATH>", "<PATH> religion.deity.deity_of </PATH>", "<PATH> people.person.religion </PATH>"], "scores": [-0.005648768972605467, -0.03156144171953201, -0.0728221982717514], "norm_scores": [0.34370720386505127, 0.3349152207374573, 0.32137757539749146]}}
{"id": "WebQTest-1941", "question": "what kind of government does peru", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does peru [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.009735597297549248, -0.01538099441677332, -0.14269748330116272], "norm_scores": [0.3484479486942291, 0.3464863896369934, 0.30506566166877747]}}
{"id": "WebQTest-1942", "question": "who was in the israeli palestinian conflict", "prediction": [["time.event.people_involved"], ["people.person.nationality"], ["people.person.ethnicity"]], "ground_paths": [["time.event.people_involved"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was in the israeli palestinian conflict [/INST]", "raw_output": {"paths": ["<PATH> time.event.people_involved </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.ethnicity </PATH>"], "scores": [-0.0005684333154931664, -0.07273975759744644, -0.11251723021268845], "norm_scores": [0.35404977202415466, 0.3293978273868561, 0.31655240058898926]}}
{"id": "WebQTest-1943", "question": "who founded the afl", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["organization.organization.spun_off_from"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who founded the afl [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.spun_off_from </PATH>"], "scores": [-0.0014242377365007997, -0.05193820223212242, -0.10632425546646118], "norm_scores": [0.35073500871658325, 0.3334580659866333, 0.31580689549446106]}}
{"id": "WebQTest-1944", "question": "where did diego velazquez die", "prediction": [["people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["people.place_of_death.people_deceased_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did diego velazquez die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>"], "scores": [-0.0002467423037160188, -0.10948222130537033, -0.11219989508390427], "norm_scores": [0.3583451509475708, 0.3212633430957794, 0.320391446352005]}}
{"id": "WebQTest-1946", "question": "what is the country code for mexico", "prediction": [["location.country.internet_tld"], ["location.location.containedby"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.internet_tld"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the country code for mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.internet_tld </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.016951462253928185, -0.029843779280781746, -0.033777520060539246], "norm_scores": [0.3366430699825287, 0.3323308229446411, 0.3310260772705078]}}
{"id": "WebQTest-1947", "question": "what currency do they use in switzerland", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency do they use in switzerland [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.004480992443859577, -0.029196474701166153, -0.1655995100736618], "norm_scores": [0.3537595570087433, 0.34512338042259216, 0.30111703276634216]}}
{"id": "WebQTest-1950", "question": "where is president kennedy buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["people.person.place_of_interment"]], "ground_paths": [["people.deceased_person.place_of_burial"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is president kennedy buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_interment </PATH>"], "scores": [-0.004863192792981863, -0.029096312820911407, -0.07561682164669037], "norm_scores": [0.34390854835510254, 0.3356747627258301, 0.320416659116745]}}
{"id": "WebQTest-1952", "question": "where was the prophet mohammed from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the prophet mohammed from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.00441633490845561, -0.028847426176071167, -0.09463425725698471], "norm_scores": [0.3460690379142761, 0.33771663904190063, 0.31621435284614563]}}
{"id": "WebQTest-1954", "question": "what language do ukrainian people speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do ukrainian people speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.008998725563287735, -0.018182625994086266, -0.07277139276266098], "norm_scores": [0.3414045572280884, 0.3382834792137146, 0.3203119933605194]}}
{"id": "WebQTest-1955", "question": "who are the judges appointed in the supreme court", "prediction": [["law.court.jurisdiction", "people.person.nationality"], ["book.written_work.subjects", "book.author.works_written"], ["book.written_work.subjects", "book.written_work.author"]], "ground_paths": [["law.court.jurisdiction", "people.person.nationality"], ["book.written_work.subjects", "book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the judges appointed in the supreme court [/INST]", "raw_output": {"paths": ["<PATH> law.court.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> book.written_work.subjects <SEP> book.author.works_written </PATH>", "<PATH> book.written_work.subjects <SEP> book.written_work.author </PATH>"], "scores": [-0.013430501334369183, -0.021827874705195427, -0.02963709458708763], "norm_scores": [0.3360710144042969, 0.333260715007782, 0.3306683301925659]}}
{"id": "WebQTest-1956", "question": "which country in europe has the largest land area", "prediction": [["location.location.containedby"], ["base.locations.continents.countries_within"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country in europe has the largest land area [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.01402901578694582, -0.027620352804660797, -0.028762375935912132], "norm_scores": [0.33648788928985596, 0.3319455087184906, 0.33156663179397583]}}
{"id": "WebQTest-1957", "question": "when did kaley cuoco join charmed", "prediction": [["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.seasons"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did kaley cuoco join charmed [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.regular_tv_appearance.seasons </PATH>"], "scores": [-0.025809263810515404, -0.02660365030169487, -0.028643185272812843], "norm_scores": [0.33373647928237915, 0.33347147703170776, 0.3327920436859131]}}
{"id": "WebQTest-1958", "question": "where is chris rock from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is chris rock from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.008204999379813671, -0.02161705307662487, -0.08029196411371231], "norm_scores": [0.3428030014038086, 0.338236004114151, 0.3189610540866852]}}
{"id": "WebQTest-1960", "question": "when was the last time the toronto maple leafs win the stanley cup", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the toronto maple leafs win the stanley cup [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.009305376559495926, -0.011998147703707218, -0.07306171953678131], "norm_scores": [0.34065231680870056, 0.3397362232208252, 0.31961143016815186]}}
{"id": "WebQTest-1961", "question": "what kind of language do egyptian speak", "prediction": [["people.ethnicity.people", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.ethnicity.people", "people.person.nationality", "language.human_language.main_country"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.main_country"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language do egyptian speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> language.human_language.main_country </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.main_country </PATH>"], "scores": [-0.03437313809990883, -0.036529719829559326, -0.04053434729576111], "norm_scores": [0.33425772190093994, 0.33353763818740845, 0.3322046101093292]}}
{"id": "WebQTest-1962", "question": "what language do brazilians use", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do brazilians use [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.0013996968045830727, -0.061556972563266754, -0.0721910148859024], "norm_scores": [0.34803515672683716, 0.3277156352996826, 0.32424914836883545]}}
{"id": "WebQTest-1963", "question": "what position did vince lombardi play", "prediction": [["american_football.football_player.position_s"], ["sports.sports_team_coach.sports_coached", "sports.sports_team_coach.sports_coached"], ["sports.sports_team_coach.sports_coached", "sports.sports_team_member.position_s"]], "ground_paths": [["american_football.football_player.position_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what position did vince lombardi play [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.position_s </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> sports.sports_team_coach.sports_coached </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> sports.sports_team_member.position_s </PATH>"], "scores": [-0.010125339031219482, -0.029443861916661263, -0.04542328044772148], "norm_scores": [0.3394220471382141, 0.3329278230667114, 0.32765012979507446]}}
{"id": "WebQTest-1964", "question": "who played stanley kowalski in a streetcar named desire movie", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.character", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "ground_paths": [["theater.play.country_of_origin", "people.person.nationality"], ["film.film.starring", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["film.performance.character", "film.actor.film"], ["common.image.appears_in_topic_gallery", "common.topic.image"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played stanley kowalski in a streetcar named desire movie [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.0278471726924181, -0.03362935408949852, -0.037155915051698685], "norm_scores": [0.33501186966896057, 0.33308035135269165, 0.3319077789783478]}}
{"id": "WebQTest-1965", "question": "who did the voice for lola bunny", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.performance.character", "film.actor.film"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did the voice for lola bunny [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.01547929085791111, -0.02765713632106781, -0.030857959762215614], "norm_scores": [0.33640187978744507, 0.3323300778865814, 0.3312680423259735]}}
{"id": "WebQTest-1966", "question": "what book did charles darwin wrote in 1859", "prediction": [["book.written_work.author"], ["book.written_work.subjects"], ["common.topic.notable_types"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], [], ["exhibitions.exhibition_subject.exhibitions_created_about_this_subject"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what book did charles darwin wrote in 1859 [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0015342995757237077, -0.06534302979707718, -0.06746727973222733], "norm_scores": [0.3479013442993164, 0.3263956308364868, 0.3257029950618744]}}
{"id": "WebQTest-1967", "question": "what language do serbs speak", "prediction": [["people.ethnicity.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.main_country"], ["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do serbs speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.008528552018105984, -0.031065771356225014, -0.04817703738808632], "norm_scores": [0.340270072221756, 0.332687109708786, 0.3270428478717804]}}
{"id": "WebQTest-1968", "question": "what was the cause of death for laci peterson", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the cause of death for laci peterson [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0024704996030777693, -0.03420713171362877, -0.10150647908449173], "norm_scores": [0.3478899896144867, 0.3370225131511688, 0.3150874674320221]}}
{"id": "WebQTest-1969", "question": "what team is kris humphries play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team.roster"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is kris humphries play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.020839953795075417, -0.020912418141961098, -0.0217710230499506], "norm_scores": [0.3334448039531708, 0.33342066407203674, 0.3331345021724701]}}
{"id": "WebQTest-1971", "question": "what movies gerard butler has been in", "prediction": [["film.performance.actor", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["film.actor.film", "film.film.starring"], ["film.film.produced_by"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.producer.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["music.recording.artist"], ["film.actor.film", "film.performance.film"], ["film.producer.films_executive_produced"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.film.produced_by", "film.film.sequel"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies gerard butler has been in [/INST]", "raw_output": {"paths": ["<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.02980477549135685, -0.03128787502646446, -0.03255414590239525], "norm_scores": [0.3338037431240082, 0.33330902457237244, 0.3328872323036194]}}
{"id": "WebQTest-1972", "question": "where did john franklin live", "prediction": [["people.deceased_person.place_of_death"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did john franklin live [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.023731302469968796, -0.03424496576189995, -0.03733246773481369], "norm_scores": [0.3360178768634796, 0.3325035870075226, 0.3314785957336426]}}
{"id": "WebQTest-1975", "question": "what was the last movie sean connery was in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["film.film.produced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the last movie sean connery was in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.01581178791821003, -0.03129718080163002, -0.03458113595843315], "norm_scores": [0.3371499180793762, 0.3319692313671112, 0.3308808207511902]}}
{"id": "WebQTest-1976", "question": "what team did drogba play for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["people.person.place_of_birth", "sports.sports_team_location.teams"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did drogba play for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.02647479809820652, -0.030831124633550644, -0.03816784918308258], "norm_scores": [0.3351174592971802, 0.3336607813835144, 0.33122172951698303]}}
{"id": "WebQTest-1977", "question": "where did crabtree go to college", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did crabtree go to college [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.01699552871286869, -0.01750023476779461, -0.02825789339840412], "norm_scores": [0.3346388339996338, 0.3344700038433075, 0.33089113235473633]}}
{"id": "WebQTest-1978", "question": "where does freddie ljungberg come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does freddie ljungberg come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.004750998690724373, -0.038641124963760376, -0.03882668539881706], "norm_scores": [0.3409273624420166, 0.3295668959617615, 0.3295057415962219]}}
{"id": "WebQTest-1979", "question": "who is number 22 on miami dolphins", "prediction": [["american_football.football_player.position_s", "people.person.nationality", "people.person.nationality"], ["sports.sports_team_roster.team", "sports.sports_team_roster.player"], ["american_football.football_player.position_s"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is number 22 on miami dolphins [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.position_s <SEP> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> sports.sports_team_roster.team <SEP> sports.sports_team_roster.player </PATH>", "<PATH> american_football.football_player.position_s </PATH>"], "scores": [-0.020534653216600418, -0.026083510369062424, -0.02721673808991909], "norm_scores": [0.33469367027282715, 0.3328416347503662, 0.33246466517448425]}}
{"id": "WebQTest-1981", "question": "what state did john adams live in", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"], ["government.government_position_held.office_holder", "government.government_position_held.district_represented"]], "ground_paths": [["symbols.name_source.namesakes", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["symbols.name_source.namesakes", "base.usnationalparks.us_national_park.state"], ["tv.tv_program.country_of_origin", "base.locations.countries.states_provinces_within"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.deceased_person.place_of_burial", "location.location.containedby"], ["government.politician.government_positions_held", "government.government_position_held.district_represented"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state did john adams live in [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.district_represented </PATH>"], "scores": [-0.02493371069431305, -0.03380351886153221, -0.04250204563140869], "norm_scores": [0.33627524971961975, 0.3333057463169098, 0.33041903376579285]}}
{"id": "WebQTest-1982", "question": "what type of music did vivaldi compose", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did vivaldi compose [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.002703584497794509, -0.039913684129714966, -0.1086752712726593], "norm_scores": [0.3492932617664337, 0.33653488755226135, 0.31417185068130493]}}
{"id": "WebQTest-1985", "question": "what team does mike fisher play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does mike fisher play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.014791547320783138, -0.017140736803412437, -0.023598086088895798], "norm_scores": [0.33457285165786743, 0.33378779888153076, 0.3316393494606018]}}
{"id": "WebQTest-1986", "question": "what films has daniel craig been in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["people.person.languages", "tv.tv_program.languages"], ["film.actor.film", "film.film.starring"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.film.executive_produced_by"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["people.person.languages", "film.film.language"], ["film.performance.actor", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what films has daniel craig been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.020506007596850395, -0.034260958433151245, -0.03647473454475403], "norm_scores": [0.3366439640522003, 0.33204516768455505, 0.33131086826324463]}}
{"id": "WebQTest-1987", "question": "what materials did eduardo paolozzi use in his work", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_artist.art_forms", "visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists", "visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.artworks", "visual_art.artwork.media"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what materials did eduardo paolozzi use in his work [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_artist.art_forms <SEP> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists <SEP> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.0024820868857204914, -0.033999912440776825, -0.04604833200573921], "norm_scores": [0.3417234718799591, 0.3311210572719574, 0.3271555006504059]}}
{"id": "WebQTest-1988", "question": "what did joe biden study in college", "prediction": [["education.education.student", "education.education.major_field_of_study"], ["people.person.education", "education.education.major_field_of_study"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["common.topic.subjects", "book.written_work.school_or_movement"], ["education.education.student", "education.education.major_field_of_study"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did joe biden study in college [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.015527970157563686, -0.016810759902000427, -0.02832985483109951], "norm_scores": [0.334896445274353, 0.3344671130180359, 0.33063647150993347]}}
{"id": "WebQTest-1990", "question": "who plays timon 's voice in the lion king", "prediction": [["film.film.starring", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["film.performance.film", "film.performance.actor"]], "ground_paths": [["film.film.starring", "film.actor.film"], ["film.film.country", "people.person.nationality"], ["film.film.language", "people.person.languages"], ["music.recording.releases", "music.artist.track"], ["music.recording.releases", "music.featured_artist.recordings"], ["film.film.featured_song", "music.recording.artist"], ["film.performance.film", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays timon 's voice in the lion king [/INST]", "raw_output": {"paths": ["<PATH> film.film.starring <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>"], "scores": [-0.03083002008497715, -0.03665325045585632, -0.03873899206519127], "norm_scores": [0.33486077189445496, 0.3329164683818817, 0.3322228193283081]}}
{"id": "WebQTest-1991", "question": "what part of the country is ohio in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.states_and_provences.country"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part of the country is ohio in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.states_and_provences.country </PATH>"], "scores": [-0.012053688988089561, -0.01836901716887951, -0.07750745862722397], "norm_scores": [0.3412565290927887, 0.3391081690788269, 0.3196353018283844]}}
{"id": "WebQTest-1992", "question": "which island is kailua on", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.island.island_group"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which island is kailua on [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.island.island_group </PATH>"], "scores": [-0.008793249726295471, -0.032399244606494904, -0.04497764632105827], "norm_scores": [0.3400050103664398, 0.3320728540420532, 0.3279220759868622]}}
{"id": "WebQTest-1993", "question": "what language do speak in argentina", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do speak in argentina [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.007176651153713465, -0.028138551861047745, -0.050013959407806396], "norm_scores": [0.34044596552848816, 0.3333838880062103, 0.3261701464653015]}}
{"id": "WebQTest-1994", "question": "what did whoopi goldberg won a grammy for", "prediction": [["award.award_honor.award_winner", "award.award_honor.award"], ["award.award_winner.awards_won", "award.award_honor.award"], ["award.award_nomination.award_nominee", "award.award_nomination.award"]], "ground_paths": [["award.award_honor.award_winner", "award.award_category.winners"], ["award.award_nomination.award_nominee", "award.award_category.nominees"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did whoopi goldberg won a grammy for [/INST]", "raw_output": {"paths": ["<PATH> award.award_honor.award_winner <SEP> award.award_honor.award </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.award </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award </PATH>"], "scores": [-0.016628552228212357, -0.021749379113316536, -0.025208892300724983], "norm_scores": [0.3348570764064789, 0.33314672112464905, 0.33199620246887207]}}
{"id": "WebQTest-1996", "question": "who played maggie in himym", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"]], "ground_paths": [["fictional_universe.fictional_character.place_of_birth", "people.person.nationality"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played maggie in himym [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.01751176081597805, -0.03589196503162384, -0.03641087934374809], "norm_scores": [0.3374882638454437, 0.33134180307388306, 0.3311699330806732]}}
{"id": "WebQTest-1997", "question": "who does owen schmitt play for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does owen schmitt play for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.019669562578201294, -0.02170530892908573, -0.028077561408281326], "norm_scores": [0.33449363708496094, 0.3338133990764618, 0.33169299364089966]}}
{"id": "WebQTest-1998", "question": "what party was thomas jefferson affiliated with", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["organization.organization_founder.organizations_founded"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"]], "ground_paths": [["organization.organization_founder.organizations_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what party was thomas jefferson affiliated with [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>"], "scores": [-0.018711833283305168, -0.019037457183003426, -0.026437267661094666], "norm_scores": [0.3342269957065582, 0.33411818742752075, 0.3316548764705658]}}
{"id": "WebQTest-1999", "question": "what religions are popular in france", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["royalty.monarch.kingdom", "people.person.religion"]], "ground_paths": [["periodicals.newspaper_circulation_area.newspapers", "book.periodical.subjects"], ["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religions are popular in france [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.religion </PATH>"], "scores": [-0.009581566788256168, -0.01301895547658205, -0.07321126759052277], "norm_scores": [0.3407247066497803, 0.3395555317401886, 0.3197198510169983]}}
{"id": "WebQTest-2000", "question": "who played berger in sex and the city", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"]], "ground_paths": [["film.film.country", "people.person.nationality"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played berger in sex and the city [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>"], "scores": [-0.023340273648500443, -0.023414246737957, -0.053018905222415924], "norm_scores": [0.33662286400794983, 0.3365979790687561, 0.3267791271209717]}}
{"id": "WebQTest-2001", "question": "who won the 2000 fa cup final", "prediction": [["sports.sports_award_type.winners", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_team.championships"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"]], "ground_paths": [["time.event.instance_of_recurring_event", "sports.sports_team.championships"], ["time.event.instance_of_recurring_event", "sports.sports_championship_event.champion"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"], ["sports.sports_award.award", "sports.sports_award.award_winner"], ["sports.sports_award_type.winners", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won the 2000 fa cup final [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_type.winners <SEP> sports.sports_award.award_winner </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.01992471143603325, -0.02245565876364708, -0.024585232138633728], "norm_scores": [0.33413273096084595, 0.33328813314437866, 0.332579106092453]}}
{"id": "WebQTest-2002", "question": "what are the names of michael jackson 's kids", "prediction": [["people.person.parents"], ["people.person.children"], ["music.composition.composer", "people.person.parents"]], "ground_paths": [["people.person.children"], ["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the names of michael jackson 's kids [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> music.composition.composer <SEP> people.person.parents </PATH>"], "scores": [-0.008133978582918644, -0.019361969083547592, -0.09425072371959686], "norm_scores": [0.3440774977207184, 0.34023579955101013, 0.3156866729259491]}}
{"id": "WebQTest-2003", "question": "what movies have robert pattinson in them", "prediction": [["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["film.actor.film", "film.film.starring"], ["music.artist.track", "music.recording.releases"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.producer.films_executive_produced"], ["music.recording.artist", "music.recording.releases"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies have robert pattinson in them [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.028783831745386124, -0.029020201414823532, -0.031109103932976723], "norm_scores": [0.3336178958415985, 0.33353906869888306, 0.3328430652618408]}}
{"id": "WebQTest-2004", "question": "what the time zone in japan", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["language.human_language.countries_spoken_in", "location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the time zone in japan [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.location.time_zones </PATH>"], "scores": [-0.0004624682478606701, -0.06394914537668228, -0.10549363493919373], "norm_scores": [0.3522636294364929, 0.3305947184562683, 0.31714168190956116]}}
{"id": "WebQTest-2005", "question": "who plays bob kelso in scrubs", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays bob kelso in scrubs [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.01578976958990097, -0.02746153622865677, -0.03341834247112274], "norm_scores": [0.3365958333015442, 0.33269003033638, 0.3307141363620758]}}
{"id": "WebQTest-2006", "question": "what race is vanessa carlton", "prediction": [["people.person.ethnicity"], ["people.ethnicity.people"], ["people.person.nationality"]], "ground_paths": [["people.person.ethnicity"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what race is vanessa carlton [/INST]", "raw_output": {"paths": ["<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.006486429832875729, -0.041827406734228134, -0.052489060908555984], "norm_scores": [0.34242871403694153, 0.3305383026599884, 0.3270329535007477]}}
{"id": "WebQTest-2007", "question": "what do people in czech republic speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people in czech republic speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.012806499376893044, -0.013957219198346138, -0.06699168682098389], "norm_scores": [0.3394310176372528, 0.3390406668186188, 0.3215283155441284]}}
{"id": "WebQTest-2008", "question": "which countries speak arabic language", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries speak arabic language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.016856549307703972, -0.01731279119849205, -0.03305837884545326], "norm_scores": [0.3351798951625824, 0.33502697944641113, 0.32979312539100647]}}
{"id": "WebQTest-2009", "question": "where does tim cook work", "prediction": [["people.person.employment_history", "business.employer.employees"], ["people.person.nationality", "location.location.containedby"], ["people.person.employment_history", "business.employment_tenure.company"]], "ground_paths": [["organization.leadership.person", "organization.organization.leadership"], ["people.person.employment_history", "business.employment_tenure.company"], ["business.employment_tenure.person", "business.employment_tenure.company"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does tim cook work [/INST]", "raw_output": {"paths": ["<PATH> people.person.employment_history <SEP> business.employer.employees </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.01965111866593361, -0.0348472073674202, -0.04132940620183945], "norm_scores": [0.3374418318271637, 0.33235281705856323, 0.33020538091659546]}}
{"id": "WebQTest-2010", "question": "what degrees does romney have", "prediction": [["people.person.education", "education.education.degree"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.degree"]], "ground_paths": [["people.person.education", "education.education.degree"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what degrees does romney have [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.degree </PATH>"], "scores": [-0.01836051046848297, -0.018563169986009598, -0.019344203174114227], "norm_scores": [0.3334651589393616, 0.3333975672721863, 0.33313730359077454]}}
{"id": "WebQTest-2011", "question": "where is bob marley buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["people.person.place_of_interment"]], "ground_paths": [["people.deceased_person.place_of_burial"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is bob marley buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_interment </PATH>"], "scores": [-0.003642888506874442, -0.03192903846502304, -0.08396726846694946], "norm_scores": [0.3454318642616272, 0.33579781651496887, 0.3187703788280487]}}
{"id": "WebQTest-2012", "question": "what is london uk time zone", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is london uk time zone [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-4.36548980360385e-05, -0.10327810794115067, -0.18901771306991577], "norm_scores": [0.36633747816085815, 0.33040544390678406, 0.3032570779323578]}}
{"id": "WebQTest-2013", "question": "where to stay in chicago tourist", "prediction": [["location.location.containedby"], ["travel.travel_destination.accommodation"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where to stay in chicago tourist [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.accommodation </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.020909326151013374, -0.02227705903351307, -0.025274615734815598], "norm_scores": [0.33397039771080017, 0.33351391553878784, 0.332515686750412]}}
{"id": "WebQTest-2014", "question": "what type of breast cancer did sheryl crow have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.parent_disease"]], "ground_paths": [["medicine.notable_person_with_medical_condition.condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of breast cancer did sheryl crow have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> medicine.disease.parent_disease </PATH>"], "scores": [-0.0027224665973335505, -0.03453211858868599, -0.052381664514541626], "norm_scores": [0.3424370586872101, 0.3317157030105591, 0.32584723830223083]}}
{"id": "WebQTest-2015", "question": "what type of government does argentina have today", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does argentina have today [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>"], "scores": [-0.008954017423093319, -0.015457249246537685, -0.1364384889602661], "norm_scores": [0.34796831011772156, 0.34571272134780884, 0.3063189685344696]}}
{"id": "WebQTest-2016", "question": "where does fabio capello come from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does fabio capello come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.00561082037165761, -0.03417203202843666, -0.04837752878665924], "norm_scores": [0.34129953384399414, 0.3316894769668579, 0.32701098918914795]}}
{"id": "WebQTest-2017", "question": "what party was abe lincoln part of", "prediction": [["organization.organization_founder.organizations_founded"], ["people.person.nationality", "organization.organization.geographic_scope"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["people.person.nationality", "organization.organization.geographic_scope"], ["influence.influence_node.influenced_by", "organization.organization.founders"], ["government.politician.party", "government.political_party_tenure.party"], ["government.political_party_tenure.politician", "government.political_party.politicians_in_this_party"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what party was abe lincoln part of [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>"], "scores": [-0.013721313327550888, -0.02049654722213745, -0.025878483429551125], "norm_scores": [0.33543944358825684, 0.3331744372844696, 0.33138614892959595]}}
{"id": "WebQTest-2018", "question": "what did jack london do for a living", "prediction": [["people.person.profession"], ["fictional_universe.fictional_character.occupation"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did jack london do for a living [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0012649786658585072, -0.05585893243551254, -0.06744834035634995], "norm_scores": [0.3468815088272095, 0.328451544046402, 0.32466694712638855]}}
{"id": "WebQTest-2019", "question": "who did morgan freeman marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.children", "people.person.parents"]], "ground_paths": [["people.person.parents", "people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did morgan freeman marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>"], "scores": [-0.01889641024172306, -0.035967867821455, -0.04277938976883888], "norm_scores": [0.3378979563713074, 0.3321785032749176, 0.329923540353775]}}
{"id": "WebQTest-2020", "question": "what are the mountains in northern italy called", "prediction": [["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.partially_contains"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the mountains in northern italy called [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.013010561466217041, -0.028709828853607178, -0.03382376208901405], "norm_scores": [0.337401807308197, 0.33214622735977173, 0.33045196533203125]}}
{"id": "WebQTest-2021", "question": "what influenced john steinbeck to start writing", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what influenced john steinbeck to start writing [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>"], "scores": [-0.009609745815396309, -0.013463282026350498, -0.1039954200387001], "norm_scores": [0.3441055119037628, 0.34278202056884766, 0.3131125271320343]}}
{"id": "WebQTest-2022", "question": "where is dwayne johnson ethnicity", "prediction": [["people.person.ethnicity"], ["people.ethnicity.people"], ["people.person.nationality"]], "ground_paths": [["people.person.ethnicity"], ["people.ethnicity.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is dwayne johnson ethnicity [/INST]", "raw_output": {"paths": ["<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.016284149140119553, -0.01633547432720661, -0.04604551941156387], "norm_scores": [0.33662939071655273, 0.33661213517189026, 0.326758474111557]}}
{"id": "WebQTest-2023", "question": "where are the nato headquarters located", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.founders", "base.biblioness.bibs_location.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the nato headquarters located [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.007357782684266567, -0.04740811139345169, -0.05359668657183647], "norm_scores": [0.34298786520957947, 0.32952257990837097, 0.32748955488204956]}}
{"id": "WebQTest-2024", "question": "in which state was the battle of antietam fought", "prediction": [["location.location.events"], ["time.event.locations"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: in which state was the battle of antietam fought [/INST]", "raw_output": {"paths": ["<PATH> location.location.events </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.0013137324713170528, -0.05108354985713959, -0.11563742905855179], "norm_scores": [0.3516894578933716, 0.3346143662929535, 0.3136961758136749]}}
{"id": "WebQTest-2025", "question": "which ocean does the zambezi river flow into", "prediction": [["geography.river.mouth"], ["location.location.containedby"], ["location.location.partially_containedby"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which ocean does the zambezi river flow into [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.002412097994238138, -0.05095614865422249, -0.06755933165550232], "norm_scores": [0.34607523679733276, 0.3296765983104706, 0.32424813508987427]}}
{"id": "WebQTest-2027", "question": "what team did david beckham play for before la galaxy", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player.loans", "soccer.football_player_loan.lending_team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did david beckham play for before la galaxy [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.02436532825231552, -0.030978180468082428, -0.03198676556348801], "norm_scores": [0.33491677045822144, 0.33270931243896484, 0.3323739171028137]}}
{"id": "WebQTest-2028", "question": "who is the current leader of france 2010", "prediction": [["people.person.nationality"], ["base.onephylogeny.type_of_thing.things_of_this_type", "people.person.profession"], ["common.topic.image", "common.topic.image"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the current leader of france 2010 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type <SEP> people.person.profession </PATH>", "<PATH> common.topic.image <SEP> common.topic.image </PATH>"], "scores": [-0.00044726920896209776, -0.07224605977535248, -0.10705037415027618], "norm_scores": [0.35340675711631775, 0.3289220929145813, 0.3176710903644562]}}
{"id": "WebQTest-2029", "question": "where was the palace of knossos located", "prediction": [["location.location.containedby"], ["architecture.building.building_complex"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the palace of knossos located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> architecture.building.building_complex </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0009287461289204657, -0.07592141628265381, -0.09961333125829697], "norm_scores": [0.3528856933116913, 0.32738980650901794, 0.3197244703769684]}}
{"id": "WebQTest-2030", "question": "where is roswell area 51", "prediction": [["aviation.airport.serves"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is roswell area 51 [/INST]", "raw_output": {"paths": ["<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00438185827806592, -0.031650360673666, -0.08067956566810608], "norm_scores": [0.34487035870552063, 0.3355933427810669, 0.3195362985134125]}}
{"id": "WebQTest-2031", "question": "when was father chris riley born", "prediction": [["people.person.place_of_birth"], ["people.person.children"], ["people.person.parents"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was father chris riley born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.0014341291971504688, -0.06680914014577866, -0.08939168602228165], "norm_scores": [0.3505677282810211, 0.3283824324607849, 0.32104983925819397]}}
